{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Start Here","text":"<p>This is the Cleura documentation web site. The content you find here constitutes authoritative documentation for Cleura products and services, including Cleura\u00a0Cloud and the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"#open-source","title":"Open source","text":"<p>All content on this site is licensed under the CC BY-SA license.</p> <p>We host our site content on GitHub, and if you would like to help by contributing a patch, here\u2019s\u00a0how you do that!</p>"},{"location":"#reporting-issues","title":"Reporting issues","text":"<p>If you notice an error on this site or would like to report that something is missing from the documentation, please report an issue.</p>"},{"location":"#searching-this-site","title":"Searching this site","text":"<p>This site comes with a complete, privacy-preserving, searchable full-text index.</p> <p>To search for a word or phrase, use the Search box in the top bar. On a mobile device, tap the  icon in the top-right corner.</p>"},{"location":"#light-and-dark-mode","title":"Light and dark mode","text":"<p>You can switch between light and dark mode by tapping the / icons in the top bar.</p>"},{"location":"#site-analytics-and-cookies","title":"Site analytics and cookies","text":"<p>This site does not set cookies, which is why we also don\u2019t need to annoy you with a cookie consent banner. We use cookie-free site analytics from Plausible.</p>"},{"location":"background/","title":"Background","text":"<p>This section contains discussions regarding various features and modes of operation Cleura\u00a0Cloud provides. Those discussions are not concerned with how to do specific tasks, but rather with what features to take advantage of or how to go about achieving a set goal.</p>"},{"location":"background/disaster-recovery/","title":"Disaster recovery","text":"<p>When you create a new server in Cleura\u00a0Cloud you will notice an option named Disaster recovery, which is enabled by default.</p> <p></p> <p>Even if you choose to disable it for a particular server, keep in mind that you have the option to enable it at a later time.</p> <p></p> <p>In the following, we explain what this option does, how it works in the background, and why you should consider enabling it.</p>"},{"location":"background/disaster-recovery/#what-it-is","title":"What it is","text":"<p>The Disaster Recovery (DR) feature is available via the Cleura\u00a0Cloud Management\u00a0Panel and applies to servers and volumes that use our Ceph backend. That would be all servers but the ones of the <code>s</code> flavor.</p>"},{"location":"background/disaster-recovery/#how-it-works","title":"How it works","text":"<p>As soon as you enable DR for a server or a single volume, you start getting snapshots for the corresponding RADOS Block Device (RBD) image. Those snapshots are created automatically once per day, and you always have the snapshots of the last 10 days.</p> <p>Please keep in mind that all RBD snapshots are created in the same Ceph cluster and are not replicated remotely. If, for any reason, you delete the original volume, then all snapshots will also be deleted, and the snapshot creation schedule will be canceled immediately.</p>"},{"location":"background/disaster-recovery/#why-enable-it","title":"Why enable it","text":"<p>Provided snapshots are available, you can restore a server or a single volume to any of those snapshots. For instance, you may discover that due to faulty application logic or simply a bug, you are now experiencing data corruption. Then, one of your options would be to go back in time by restoring one of the available snapshots and keep going from there.</p>"},{"location":"background/disaster-recovery/#restoration-time","title":"Restoration time","text":"<p>You should know that the DR feature creates point-in-time snapshots on the storage level. The time required to restore a server to a particular snapshot depends on its size. During restoration, the server is shut off. After the restore, you need to power the server back on manually. Although this whole process takes time analogous to volume size, as we pointed out, we should also note that it only takes seconds to complete on average.</p>"},{"location":"background/gui-vs-api/","title":"Cleura Cloud Management Panel vs. OpenStack API","text":"<p>You can do many tasks in Cleura\u00a0Cloud via the Cleura\u00a0Cloud Management\u00a0Panel or via the OpenStack API, e.g., with the help of the OpenStack CLI tool. For some tasks, though, you will need the OpenStack API.</p> <p>But when does it make sense to prefer a particular way of working? What are the reasons, if any, for choosing one method over the other?</p>"},{"location":"background/gui-vs-api/#the-case-for-the-cleura-cloud-management-panel","title":"The case for the Cleura\u00a0Cloud Management\u00a0Panel","text":"<p>Ease of use is probably the main reason for choosing the Cleura\u00a0Cloud Management\u00a0Panel over the OpenStack API. Provided you have an account in Cleura\u00a0Cloud, you can simply log in and then follow step-by-step guides to create entities such as networks, servers, or even Kubernetes clusters.</p> <p>You can just as easily perform administrative tasks like</p> <ul> <li>creating security groups,</li> <li>setting up region-to-region VPN connections,</li> <li>deleting networks,</li> <li>modifying billing data, or</li> <li>managing invoices.</li> </ul> <p>All in all, there are many instances when the Cleura\u00a0Cloud Management\u00a0Panel is all you want and, at the same time, is more than enough for what you want.</p>"},{"location":"background/gui-vs-api/#the-case-for-the-openstack-api","title":"The case for the OpenStack API","text":"<p>There will be times when working from the terminal of your laptop is preferable to using the Cleura\u00a0Cloud Management\u00a0Panel. In cases like this, you will use the OpenStack API via a tool like the OpenStack CLI. Although <code>openstack</code> usually requires some reading before doing a specific task, it doesn\u2019t take much time to get used to its logic for constructing commands to achieve what you want. Plus, there is hardly a thing you can\u2019t do with <code>openstack</code> or a CLI tool that talks to the OpenStack API.</p> <p>We should also point out that specific tasks can be performed only via the OpenStack API, for there is no counterpart tool in the Cleura\u00a0Cloud Management\u00a0Panel toolbox.</p> <p>For instance, whenever you need to move servers between regions, then <code>openstack</code> is your only option. Another instance where you work with the OpenStack API and various CLI tools, is when you have to interact with the S3 API or the Swift API.</p> <p>Even though the command syntax of <code>openstack</code> may sometimes look overly complicated, the potential for scripting can speed up many operations considerably. In addition to that, the OpenStack API is employed by configuration management systems and automation platforms like Ansible, and infrastructure as code systems like Terraform and Pulumi.</p>"},{"location":"background/marketplace/","title":"Cleura\u00a0Cloud Marketplace","text":"<p>The Cleura Cloud Marketplace is an online self-service portal that allows you to deploy pre-configured applications. It is accessible via a menu option in the Cleura\u00a0Cloud Management\u00a0Panel, or directly at cleura.cloud/marketplace.</p> <p>You may choose any Marketplace application for quick and easy deployment in Cleura\u00a0Cloud\u2019s infrastructure.</p> <p>Before deploying a Marketplace application, you can check the minimum resource requirements. Please keep in mind that you always get a cost estimate in advance. The cost depends on the specific resource selections (e.g., number of CPUs, amount of RAM) and on the type of license that covers the application under consideration.</p> <p>To deploy or remove any Marketplace application, refer to the step-by-step guides.</p>"},{"location":"background/marketplace/#open-source-and-proprietary-software","title":"Open source and proprietary software","text":"<p>The Cleura\u00a0Cloud Marketplace includes open source and proprietary applications.</p> <p>There are cases of open source applications that also have proprietary editions with additional features. There are also open source applications available with a service contract, which may include, for instance, support.</p>"},{"location":"background/marketplace/#quality-control","title":"Quality control","text":"<p>All applications are periodically reviewed for relevance, quality, and compliance with Cleura\u00a0Cloud Marketplace standards.</p> <p>Cleura\u00a0Cloud does not guarantee that the Cleura\u00a0Cloud Marketplace will contain the latest versions of available applications. However, there may be multiple versions of a specific application to choose from.</p> <p>Please keep in mind that Marketplace applications you have already deployed are not automatically updated by Cleura\u00a0Cloud. Instead, you have the option to maintain and keep them up-to-date.</p>"},{"location":"background/marketplace/#support","title":"Support","text":"<p>Cleura\u00a0Cloud will address any queries related to the Cleura\u00a0Cloud Marketplace service itself, including service-related concerns and content issues in the service catalog.</p> <p>You may submit a support ticket anytime via our Service\u00a0Center.</p> <p>For support issues related to any application in the Cleura\u00a0Cloud Marketplace, you should follow any guidelines offered by the developer of that particular application.</p>"},{"location":"background/object-storage/","title":"Object storage in Cleura\u00a0Cloud","text":"<p>Object storage in Cleura\u00a0Cloud is implemented via the Ceph Object Gateway, also known as <code>radosgw</code> (commonly pronounced \u201crados gateway\u201d).</p> <p>This facility exposes access to objects via two RESTful APIs: the Amazon S3 API, and the OpenStack Swift API. It is, however, important to understand that radosgw shares no code with Amazon S3, nor with OpenStack Swift.</p> <p>As such, object storage in Cleura\u00a0Cloud is not expected to behave exactly as Amazon S3 or OpenStack Swift, although it tracks their behavior very closely. The Ceph upstream documentation lists API deviations from the respective reference implementations, for both the S3 and the Swift API. Our reference section lists additional limitations specific to Cleura\u00a0Cloud.</p>"},{"location":"background/object-storage/#object-storage-integration-with-openstack-authentication","title":"Object storage integration with OpenStack authentication","text":"<p>In Cleura\u00a0Cloud, the OpenStack <code>object\u2011store</code> service endpoints point not to native OpenStack Swift proxy servers, but to radosgw endpoints. This is to say that radosgw acts as a \u201cdrop-in\u201d replacement for OpenStack Swift.</p> <p>As such, it is fully integrated with OpenStack\u2019s authentication facility, OpenStack Keystone. This means that once you have configured the OpenStack CLI correctly, you can interact with the <code>object\u2011store</code> endpoint as with any other OpenStack service, using the Swift API.</p> <p>The S3 API, however, does not natively \u201cknow\u201d about OpenStack authentication. The radosgw endpoints in Cleura\u00a0Cloud still authenticate via Keystone, albeit taking a little detour: you must first create AWS-style credentials and configure your S3 client with them, which it can then use to make properly authenticated S3 API calls.</p>"},{"location":"background/object-storage/#multi-tenancy","title":"Multi-tenancy","text":"<p>OpenStack Swift has the concept of multi-tenancy built-in. Thus, if you use the Swift API with credentials for an OpenStack user that authenticates against a specific Cleura\u00a0Cloud region and uses a specific project, it can only interact with objects stored in containers belonging to that project by default.</p> <p>In Cleura\u00a0Cloud, you also get multi-tenancy for S3 objects, even though this concept is not intrinsic to S3 itself. Here, again, a slight conceptual detour applies: when you create AWS-style credentials, then those are linked to a specific OpenStack user, region, and project. When you subsequently use those credentials for S3 interactions, any buckets and objects thus created belong to the project to which the credentials are linked.</p>"},{"location":"background/object-storage/#object-identity-in-cleura-cloud","title":"Object identity in Cleura\u00a0Cloud","text":"<p>Whether you access a specific object via the Swift API or the S3 API, it is the same object. This also means that the object\u2019s container (accessed via the Swift API) is identical to its containing bucket (accessed via the S3 API).</p> <p>Thus, if you</p> <ul> <li>enable the OpenStack CLI with a certain set of user credentials;</li> <li>use those credentials to create a private container named <code>test</code> and an object named <code>my\u2011object</code>, using the Swift API;</li> <li>use the same OpenStack credentials to create AWS-style credentials;</li> <li>then use those AWS-style credentials to list your buckets and objects, using the S3 API,</li> </ul> <p>you will find an S3 bucket named <code>test</code>, containing an object named <code>my\u2011object</code>.</p>"},{"location":"background/object-storage/#permission-and-feature-conflicts","title":"Permission and feature conflicts","text":"<p>The fact that objects are identical when accessed via Swift and S3 also entails that bucket and object permissions, set via one API, also apply to object access using the other API.</p> <p>For example, if you make a container public via the Swift API, it also becomes a public bucket that is accessible via an S3 API path. You cannot simultaneously retain mandatory private (authenticated) access to the corresponding bucket via the S3 API.</p> <p>Object storage in Cleura\u00a0Cloud also does not allow you to make competing feature settings on containers/buckets, based on the API used to access them. For example, it is not possible to create a Swift container that enables versioning, while disabling bucket versioning on the corresponding S3 bucket.</p> <p>Sometimes, this creates unavoidable conflicts if a specific feature is only available in one of the supported APIs. For example, if you set a public read policy on an S3 bucket, the corresponding Swift container will still show an empty Read ACL, making the Swift container look like it is private, even though its objects are accessible through simple public URLs. This is because Swift has no concept of fine-grained bucket policies, as they exist in S3.</p>"},{"location":"background/object-storage/#object-versioning","title":"Object versioning","text":"<p>In regular default buckets, each object has just one representation, and all operations (like removal or upload) operate on this one entity. With versioned buckets, modifications like uploading an object with the same name only modify a view of the bucket. In this mode of operation, each object receives a unique version ID, and modifications retain prior versions. This allows you to restore an earlier version of an object.</p> Operation Default bucket Versioned bucket Uploading a new object New object is stored New object is stored Overwrite of existing object New object overwrites old one New object version becomes current Delete object Object is deleted Delete marker is placed, subsequent object access returns \u201cNot found\u201d <p>Once you enable versioning on a bucket, you cannot turn it off. You can, however, suspend it. When versioning is suspended, new objects are created with a version ID of <code>null</code>, overwriting any existing <code>null</code> version. All previously versioned objects remain unchanged and accessible.</p>"},{"location":"background/object-storage/#delete-markers","title":"Delete markers","text":"<p>When you delete an object in a versioned bucket, the object isn\u2019t actually removed. Instead, a special placeholder called a delete marker acts as the current version of the object. This marker has a unique version ID, causing the object service to return a 404 \u201cNot found\u201d error whenever you attempt to retrieve the object.</p> <p>However, trying to access any older version of the object works as expected. Delete markers themselves can also be deleted. When you delete a delete marker, the previous version becomes current again, effectively \u201cundeleting\u201d the object. To permanently remove an object version, you must delete it by specifying its version ID.</p>"},{"location":"background/object-storage/#object-lock","title":"Object lock","text":"<p>Object lock is a feature in S3 that essentially allows you to protect objects from being deleted or overwritten. There are two methods for managing object retention: Retention periods and Legal hold.</p>"},{"location":"background/object-storage/#retention-periods","title":"Retention periods","text":"<p>Retention periods specify the length of time that an object remains locked and protected from being overwritten or deleted. When you set a retention period, the object is safeguarded for the specified time. You can set the retention period in either days or years, with a minimum of one day and no maximum limit.</p> <p>In addition to retention periods, you can choose the retention mode that applies to your objects (either Governance or Compliance). In Cleura\u00a0Cloud, the only supported retention mode is Compliance.</p> <p>Compliance mode is recommended when storing compliant data, as it prevents objects from being overwritten or deleted by any user. If you configure an object with this mode, you cannot shorten or change its retention period, ensuring that the data remains secure and compliant with regulatory requirements.</p>"},{"location":"background/object-storage/#legal-hold","title":"Legal hold","text":"<p>Legal hold is a feature designed for situations when you are uncertain about the length of time you need to retain an object. It can be enabled/disabled for any object in a locked bucket, regardless of the lock configuration, object retention, or age.</p> <p>This feature provides the same level of protection as a retention period but with no fixed expiration date. Instead, a legal hold will remain in effect until you remove it explicitly.</p>"},{"location":"background/project-deletion/","title":"Why you should not attempt to delete projects","text":"<p>You may have noticed that the Cleura\u00a0Cloud Management\u00a0Panel does not provide a mechanism for completely wiping out projects, also known as tenants. In other words, you cannot simply delete a project and all of its resources in one go. Also, you cannot delete a project even after you have meticulously deleted all of its resources.</p> <p>Following is a list of facts explaining our policy against wiping out or deleting projects.</p>"},{"location":"background/project-deletion/#purging-a-projects-resources-is-not-possible","title":"Purging a project\u2019s resources is not possible","text":"<p>There is simply no support for deleting all resources belonging to a project in one fell swoop, not even at the OpenStack API level. That is why you can also not wipe out projects even from the command line.</p>"},{"location":"background/project-deletion/#a-way-of-deleting-a-project","title":"A way of deleting a project","text":"<p>Once all of its associated resources have been removed, deleting a project is technically feasible. To delete a project with no associated resources, a user with domain admin privileges may utilize the OpenStack CLI. The <code>openstack</code> client understands the command <code>project</code>, and that command understands the <code>delete</code> subcommand. But to successfully run <code>openstack project delete &lt;project&gt;</code>, domain admin privileges are required, and those are not available for Cleura\u00a0Cloud users.</p>"},{"location":"background/project-deletion/#disabling-a-project-is-possible","title":"Disabling a project is possible","text":"<p>If you wish to disable a project, you can do so via the Cleura\u00a0Cloud REST\u00a0API. Make sure you have access to it, and then consult the documentation to actually disable a project.</p>"},{"location":"background/kubernetes/","title":"Kubernetes in Cleura Cloud","text":"<p>Cleura\u00a0Cloud has two management facilities for Kubernetes clusters:</p> <ul> <li>Gardener in Cleura\u00a0Cloud,</li> <li>OpenStack Magnum.</li> </ul> <p>In most scenarios, Gardener is the preferred option, since it supports more recent Kubernetes versions and offers you a greater degree of \u201chands-off\u201d management.</p> <p>For more details on the relative merits of both options, refer to the sections below.</p>"},{"location":"background/kubernetes/#general-characteristics","title":"General characteristics","text":"Gardener Magnum Kubernetes Cloud Provider OpenStack OpenStack Base operating system for nodes Garden Linux Fedora CoreOS Latest installable Kubernetes minor release 1.31 1.27"},{"location":"background/kubernetes/#api-and-cli-support","title":"API and CLI support","text":"Gardener Magnum Manageable via Cleura Cloud REST API Manageable via OpenStack REST API Manageable via OpenStack CLI"},{"location":"background/kubernetes/#updates-and-upgrades","title":"Updates and upgrades","text":"Gardener Magnum Automatic update to new Kubernetes patch release Rolling upgrade to new Kubernetes minor release Automatic upgrade to new Kubernetes minor release Rolling upgrade to new base operating system release Automatic upgrade to new base operating system release"},{"location":"background/kubernetes/#functional-features","title":"Functional features","text":"Gardener Magnum Built-in private registry for container images Hibernation Manual vertical scaling (bigger/smaller worker nodes) <sup>1</sup> Vertical autoscaling Manual horizontal scaling (more/fewer worker nodes) Horizontal autoscaling <sup>2</sup> Kubernetes dashboard <sup>3</sup>"},{"location":"background/kubernetes/#charges-and-billing","title":"Charges and billing","text":"Gardener Magnum Monthly subscription fee Cleura\u00a0Cloud charges for Kubernetes control plane nodes Cleura\u00a0Cloud charges for Kubernetes worker nodes <ol> <li> <p>Vertical scaling is only supported via defining additional worker node groups.\u00a0\u21a9</p> </li> <li> <p>You must deploy Magnum Cluster Autoscaler to use horizontal autoscaling.\u00a0\u21a9</p> </li> <li> <p>You must separately deploy the Kubernetes Dashboard.\u00a0\u21a9</p> </li> </ol>"},{"location":"background/kubernetes/gardener/autoscaling/","title":"Autoscaling","text":"<p>Autoscaling is the ability of your Gardener shoot cluster to add and remove nodes as needed, based on the workload you deploy to the Kubernetes cluster.</p> <p>Autoscaling in Cleura\u00a0Cloud is always horizontal rather than vertical, meaning Gardener adds or removes nodes of the pre-defined machine type (flavor), rather than changing the flavor of existing nodes in-place.</p>"},{"location":"background/kubernetes/gardener/autoscaling/#default-settings","title":"Default Settings","text":"<p>By default, Gardener launches 3 initial worker nodes, and limits your cluster to a maximum of 5 nodes.</p> <p>You can modify these settings (Autoscaler Min and Autoscaler Max) when you create a new shoot cluster, or at any time thereafter.</p>"},{"location":"background/kubernetes/gardener/autoscaling/#how-autoscaling-works","title":"How autoscaling works","text":"<p>Autoscaling is designed to be a \u201chands-off\u201d cluster feature.</p> <p>Once your cluster decides its resources are insufficient to manage the current workload, it adds nodes to handle it. This is called scale-out.</p> <p>When the workload becomes less, it removes the extra nodes again. This is called scale-in.</p> <p>Scale-out is triggered if any Pod on the cluster fails to be scheduled to an existing node. This may be because the node is already overloaded (meaning its <code>DiskPressure</code>, <code>MemoryPressure</code> or <code>PIDPressure</code> node condition is active), or because a newly launched Pod is configured with a request that none of the existing nodes can meet. Thus, scaling out is nearly immediate: as soon as a Pod scheduling failure occurs for one of these reasons, the cluster launches a new worker node \u2014 unless it is already running with the maximum number of nodes, as defined by Autoscaler Max.</p> <p>Scale-in, in contrast, happens when a node\u2019s utilization drops below 50% for a period of 30 minutes. Therefore, scaling in occurs in a delayed fashion. This is by design: if the dip in resource utilization is only temporary, sufficient worker node capacity is already available when it rebounds.</p>"},{"location":"background/kubernetes/gardener/autoscaling/#autoscaling-limitations","title":"Autoscaling limitations","text":"<p>Autoscaling may sometimes not occur when you expect it to, including the following situations:</p> <ul> <li> <p>If a Pod spec contains a request that is impossible to meet even with scale-out, no autoscaling occurs.   For example, if a Pod were to request 1TiB of memory, and the configured worker node flavor has less than that, then scale-out would not help:   to run such a Pod, you would instead have to add a new worker group (with a larger flavor) to the cluster.</p> </li> <li> <p>If a Deployment contains Pods with anti-affinity rules that restrict multiple replicas from running on the same node, and its number of running replicas is already equal to the current number of worker nodes in the group, then no scale-in is possible.   For example, suppose a Deployment is running with 4 replicas of an anti-affinity Pod in a cluster with default Autoscaler Min and Autoscaler Max values.   When at some point that cluster has 4 worker nodes, the fourth node will remain even if its utilization is permanently below 50%.</p> </li> </ul>"},{"location":"background/kubernetes/gardener/autoupgrades/","title":"Automatic upgrades","text":"<p>By default, Kubernetes clusters created with Gardener in Cleura\u00a0Cloud are upgraded automatically.</p> <p>More specifically, during the specified maintenance window, Gardener checks for a new Kubernetes version, and a new version of the base image that runs on the control plane and worker nodes. If there is a new version of any of those, the cluster starts upgrading automatically.</p> <p>During the upgrade, the API server may briefly be unavailable.</p>"},{"location":"background/kubernetes/gardener/autoupgrades/#machine-image-upgrades","title":"Machine image upgrades","text":"<p>Both minor and major Garden Linux release upgrades are allowed and performed automatically. Also, version jumps are allowed over minor release upgrades.</p> <p>Whenever there is a new machine image available, this is indicated in the Cleura\u00a0Cloud Management\u00a0Panel:</p> <p></p> <p>Before the auto-upgrade kicks in, the version of the new image is displayed in the expanded view of the cluster, in the Worker Groups tab.</p> <p></p>"},{"location":"background/kubernetes/gardener/autoupgrades/#kubernetes-upgrades","title":"Kubernetes upgrades","text":"<p>Due to the Kubernetes versioning scheme, which does not follow the semantic\u00a0versioning logic, only patch-level release upgrades and jumps are performed automatically. For instance, an upgrade from 1.24.9 to 1.24.10 is allowed, and so is an upgrade from 1.24.9 to 1.24.11.</p> <p>Minor release upgrades, such as from 1.24 to 1.25, do not happen automatically. You must always initiate them from the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"background/kubernetes/gardener/autoupgrades/#hibernated-clusters","title":"Hibernated clusters","text":"<p>Whenever a Gardener cluster is hibernated, all of its worker nodes are removed.</p> <p>If automatic upgrades are enabled on a hibernated cluster, then waking the cluster will initiate an immediate automated upgrade. That is to say that, rather than recreating them with the old versions and waiting for the next maintenance window, Gardener recreates the worker nodes using current (updated) Garden Linux and Kubernetes versions.</p>"},{"location":"background/kubernetes/gardener/garden-linux/","title":"Garden Linux","text":"<p>In a Gardener cluster, worker nodes run Garden Linux, which is a Linux distribution based on Debian GNU/Linux.</p>"},{"location":"background/kubernetes/gardener/garden-linux/#version-scheme","title":"Version scheme","text":"<p>Garden Linux uses an unusual version scheme: releases use a major version number derived from the \u201cgardenlinux epoch\u201d, and should ostensibly reflect the number of days since April 1, 2020 (although in reality, the major release numbers are off by a number of days).</p> <p>For example, the major release that landed on November 11, 2022 was release 934. The next major release, on November 23, 2023, had the major release number 1312.</p> <p>Minor releases are numbered with incrementing integer minor version numbers. They tend to include mostly bug fixes, as opposed to functionality updates or new features. For example, 934.6 was a Garden Linux release that fixed several security issues in the 934 release series.</p>"},{"location":"background/kubernetes/gardener/garden-linux/#linux-kernel","title":"Linux kernel","text":"<p>Garden Linux runs on the latest longterm release Linux kernel version available at the time of a major release. For example, Garden Linux 934 includes the Linux 5.15 kernel, whereas Garden Linux 1312 includes Linux 6.6.</p>"},{"location":"background/kubernetes/gardener/garden-linux/#container-runtime","title":"Container runtime","text":"<p>Garden Linux defaults to using the Podman container runtime.</p>"},{"location":"background/kubernetes/gardener/hibernation/","title":"Hibernation","text":"<p>In Cleura\u00a0Cloud, you have the option of putting a Gardener-based Kubernetes cluster in hibernation. This is something you might want to do whenever you know you won\u2019t be needing the cluster for some time. Putting it in hibernation makes sense because from that time on and before you wake it up, you pay less for hosting.</p> <p>You may wonder why you still pay something after putting a cluster in hibernation, instead of paying nothing at all. To answer this question, we have to explain how hibernation works.</p>"},{"location":"background/kubernetes/gardener/hibernation/#how-hibernation-works","title":"How hibernation works","text":"<p>Learning about the logic of hibernation in Gardener, we immediately get a fresh perspective on the whole concept. When we hear about hibernation, we usually think of resources that are merely stopped or frozen, and their volatile information made persistent. This is not how Gardener hibernation works \u2014 and here\u2019s why.</p> <p>When you put a Gardener cluster in hibernation, what really happens is that all worker nodes are removed. So, as long as the cluster is in hibernation, you will not be paying for CPU, RAM, and boot volume storage utilization incurred by the worker nodes. But there might still be resources created by the cluster, for which you will keep getting charged even when the cluster is in hibernation. More specifically, you will keep paying for any persistent volumes, floating IPs, and load balancers associated with the cluster.</p> <p>In Kubernetes, a Persistent Volume (or PV), is a piece of storage in the cluster that has been provisioned either dynamically (using Storage Classes) or by an administrator. When a user makes a request for storage, then we have a PersistentVolumeClaim (or PVC). Floating IPs, on the other hand, are instantiated to be assigned to Kubernetes Services that need a public IP. Those services have an external load balancer (i.e., a service of <code>Type: LoadBalancer</code>). When you hibernate a Gardener cluster, objects of any of those types are retained.</p>"},{"location":"background/kubernetes/gardener/hibernation/#waking-up-clusters","title":"Waking up clusters","text":"<p>When you wake a hibernated cluster, the previously removed worker nodes reappear. Since all information about cluster resources (Pods, Deployments, ReplicaSets etc.) are still available on the control plane, these cluster resources also automatically reappear on the recreated worker nodes.</p> <p>However, to run your Pods, Gardener will have to re-fetch any images they previously ran on. That is because the image cache of newly created worker nodes starts empty. Consequently, please keep in mind that a cluster whose resources were perfectly humming along before the hibernation, might suddenly see Pods failing with <code>ImagePullError</code> if any image they depend on has been deleted in an upstream registry.</p> <p>Last but not least, the act of waking a cluster up may temporarily fail, because while the cluster was hibernating, the tenant came close to its volume, RAM, CPU, etc. quota, and attempting to re-instantiate the worker nodes and re-activate the cluster would breach the quota limit.</p>"},{"location":"contrib/","title":"Contribution Guide","text":"<p>See something on this site that is inaccurate, missing, or that could simply be improved? There are multiple ways for you to help make this site better, and we welcome all of them.</p> <p>You can make modifications and contributions using Git, and we apply certain checks to ensure consistent documentation quality.</p>"},{"location":"contrib/#technical-writing-resources","title":"Technical Writing Resources","text":"<p>Whether you are an experienced technical writer, a regular (or not-so-regular) open source contributor, or you\u2019re making your very first writing contribution, there are a ton of helpful resources on technical writing. Here are a few:</p> <ul> <li>Digital Ocean\u2019s Technical Writing   Guidelines</li> <li>Red Hat\u2019s Writing Style   Guide</li> <li>Gareth Dwyer\u2019s Technical   Writing repository</li> <li>Bolaji Ayodeji\u2019s Awesome\u00a0Technical\u00a0Writing collection</li> </ul>"},{"location":"contrib/#markdown","title":"Markdown","text":"<p>The documentation on this site uses Markdown. Markdown is a documentation format that is rich enough to be useful for good technical documentation, and yet simpler and easier to learn than other formats like reStructuredText or DocBook XML.</p> <p>If you\u2019re unfamiliar with Markdown, you can read up on its basics in this classic article by John Gruber if you\u2019re interested, but chances are that you\u2019ll also find all the information you\u2019ll need in this cheat sheet by Adam Pritchard, or the Start Writing guide from GitHub.</p> <p>Or you simply look at the source of one of the pages on this site (try the Edit button on this one!) and figure it out as you go along \u2014 it\u2019s really pretty straightforward.</p>"},{"location":"contrib/#license","title":"License","text":"<p>With the sole exception of trademarks like \u201cCleura\u201d and the Cleura logo, the content on this site is available under the Creative Commons Attribution-ShareAlike 4.0 International license, as you can see from the  icon at the bottom of each page. Please keep in mind that you are making your contribution under those terms.</p>"},{"location":"contrib/issues/","title":"Reporting issues","text":"<p>If you notice an issue (a typo, a grammar glitch, missing or factually incorrect information), here\u2019s how you can let us know:</p> <p>Go to our issue tracker and click New issue.</p> <p></p> <p>Select the issue type that most closely matches the type of issue you are reporting.</p> <p></p> <p>Be sure to fill all required fields (the ones marked with a red asterisk), and fill in the optional ones as necessary.</p> <p></p> <p>Don\u2019t forget to click Submit new issue when you\u2019re done.</p> <p>And if you want to help us fix the issue you\u2019ve just reported, please take a look at our guidelines for modifying content on this site!</p>"},{"location":"contrib/modifications/","title":"Modifying content on this site","text":"<p>You have two options for editing content: directly in your browser using GitHub, or from your local work environment using a Git-based workflow.</p>"},{"location":"contrib/modifications/#notes-on-content-additions","title":"Notes on content additions","text":"<p>Sections: Generally, content additions should fit somewhere within the existing top-level and second-level sections (like Background, or Kubernetes). Try not to introduce a new top-level or second-level section.</p> <p>Text: Write your Markdown text in such a way that each sentence in a paragraph has its own line, and is followed immediately by a newline character with no spaces in between. This facilitates the review of your change, and it also helps you notice long run-on sentences as you write.</p> <p>Headings: Keep headings concise, under 80 characters.</p> <p>Screenshots: If you are contributing a change that contains screenshots from Cleura\u00a0Cloud Management\u00a0Panel, they should have a resolution of 1920\u00d71080 pixels (1080p). If your screen has a larger resolution, use Firefox Responsive Design Mode or Chrome/Chromium Device Mode to configure your browser with 1080p.</p> <p>Screenshots should be added to a directory named <code>assets</code>, located in the same directory as the Markdown file you are adding or editing.</p> <p>CLI screen dumps: If you are contributing a change that contains a screen dump from the <code>openstack</code> command-line client, please limit its width to 100 characters. You can do this by setting the following environment variable in your terminal, before you start working on your change.</p> <pre><code>export CLIFF_MAX_TERM_WIDTH=100\n</code></pre> <p>Renamed sources: If you rename an existing Markdown source, the path of the rendered page\u2019s URI will change. In this case, be sure to define a redirect from the old URI to the new one, by adding an entry to the <code>plugins.redirect.redirect_maps</code> dictionary in the <code>mkdocs.yml</code> configuration file.</p>"},{"location":"contrib/modifications/#modifying-content-from-your-browser","title":"Modifying content from your browser","text":"<p>Every page on this site has an Edit button (). If you click it, it\u2019ll take you straight to the corresponding source page in GitHub. Then, you can follow GitHub\u2019s documentation on how to propose changes to another user\u2019s repository.</p>"},{"location":"contrib/modifications/#modifying-content-using-git","title":"Modifying content using Git","text":"<p>The Git repository for this site lives on GitHub. You can fork that repository, make the proposed changes in your fork, and then send us a standard GitHub pull request.</p> <p>For this purpose, use <code>git</code> in combination with either GitHub\u2019s web interface, or the <code>gh</code> command-line interface (CLI).</p> <p>First, create a fork of the documentation repository:</p> <code>git</code> client and web browser<code>gh</code> client <p>Open our GitHub repo and click the Fork button. When you create your new fork, it\u2019s fine to leave the Copy the <code>main</code> branch only option enabled.</p> <p>Then, proceed to create a new local checkout of your fork: <pre><code>git clone git@github.com:&lt;yourusername&gt;/&lt;your-repo-fork&gt; cleura\u00a0cloud-docs\ncd cleura\u00a0cloud-docs\n</code></pre></p> <pre><code>gh repo fork --clone https://github.com/citynetwork/docs -- cleura\u00a0cloud-docs\ncd cleura\u00a0cloud-docs\n</code></pre> <p>Next, create a local topic branch and make your modifications:</p> <pre><code>git checkout -b &lt;your-topic-branch-name&gt;\n# edit your files\ngit add &lt;files-to-add&gt;\ngit commit\n</code></pre> <p>Please see our notes on commit messages.</p> <p>Finally, create a pull request (PR) from your changes:</p> <code>git</code> client and web browser<code>gh</code> client <p>Run the following <code>git</code> command (assuming <code>origin</code> is the remote that points to your fork): <pre><code>git push origin &lt;your-topic-branch-name&gt;\n</code></pre> Then, open your browser to the URL suggested by the <code>git push</code> command, and proceed to create a pull request.</p> <pre><code>gh pr create --fill\n</code></pre>"},{"location":"contrib/modifications/#monitoring-changes-as-you-edit","title":"Monitoring changes as you edit","text":"<p>To see your changes as you work on them, you can use tox. Having created a topic branch with your modifications, run:</p> <pre><code>cd cleura\u00a0cloud-docs\ngit checkout &lt;your-topic-branch-name&gt;\ntox -e serve\n</code></pre> <p>A local copy of the documentation will then run on your local machine and be accessible from http://localhost:8000 in your browser.</p> <p>When planning to make several changes in rapid succession, you may want to speed up site rendering after each change. You may do so by disabling a plugin that checks all links (including external links) for accessibility:</p> <pre><code>cd cleura\u00a0cloud-docs\nexport DOCS_ENABLE_HTMLPROOFER=false\ntox -e serve\n</code></pre>"},{"location":"contrib/modifications/#commit-messages","title":"Commit messages","text":"<p>When you submit a change, you will need to provide a commit message, which is very nearly as important as the change itself. Excellent guides on what constitutes a good commit message are available from Tim Pope and Colleen Murphy.</p> <p>In addition, we have adopted the Conventional Commits style for commit message subjects. Please make sure that your commit message starts with one of the following prefixes:</p> <ul> <li><code>feat:</code> denotes a content addition, such as adding documentation for some Cleura\u00a0Cloud functionality that was not included in the documentation before.</li> <li><code>fix:</code> denotes a content correction, such as fixing a documentation bug.</li> <li><code>build:</code> denotes a change to the build process, such as an improvement to a CI check.</li> <li><code>chore:</code> denotes a minor change that is neither a feature, nor a fix, nor a build improvement, such as when you edit the <code>.mailmap</code> file.</li> <li><code>docs:</code> denotes a change to the documention for this site, such as an update to the <code>README.md</code> file.</li> </ul>"},{"location":"contrib/quality/","title":"Quality checks","text":"<p>There are a few checks that we apply to the configuration of this site. These checks run automatically via GitHub Actions workflows when you send your PR:</p> <ul> <li>We check the commit message with   gitlint, and enforce the   Conventional   Commits commit   message style.</li> <li>We check whether the documentation still builds correctly, with your   change applied.</li> <li>We apply pymarkdownlnt   checks for Markdown consistency and to encourage some good   documentation practices.</li> <li>We check to make sure that no internal or external links in the   documentation are dead. This is one example where the checks might   fail through no fault of yours \u2014 some external link may have   disappeared between the most recent change and your contribution, by   pure coincidence. When that happens, we\u2019ll fix it together.</li> <li>We check some YAML conventions with   yamllint. However,   most contributions would probably only touch Markdown files and not   YAML, so you\u2019re unlikely to trip over this.</li> </ul> <p>If you\u2019re working in your local Git repository and your work environment has tox installed, you can also run the checks locally:</p> <pre><code>tox\n</code></pre> <p>You can also configure your local checkout to run quality checks on each commit. To do that, run:</p> <pre><code>git config core.hooksPath .githooks\n</code></pre>"},{"location":"howto/","title":"About our How-To guides","text":"<p>In this section you\u2019ll find details about how you can accomplish specific tasks in Cleura\u00a0Cloud and the services we support.</p> <p>There are several categories of How-To guides, and they tend to be focused on a specific cloud technology.</p> <ul> <li> <p>Getting Started How-Tos help you create an account in Cleura\u00a0Cloud, and start using our services.</p> </li> <li> <p>Kubernetes How-Tos cover how you can create and manage your Kubernetes deployments using Cleura\u00a0Cloud Management\u00a0Panel.</p> </li> <li> <p>Object storage How-Tos deal with the S3 and Swift object storage APIs, and how you can use them for object storage in Cleura\u00a0Cloud.</p> </li> <li> <p>OpenStack CLI/API How-Tos cover tasks that you can accomplish with the OpenStack command line interfaces and application programming interfaces.   They generally do not depend on any adjacent services or tools, just your Cleura\u00a0Cloud OpenStack credentials, the <code>openstack</code> client, and/or the native OpenStack APIs.</p> </li> <li> <p>Cleura\u00a0Cloud Marketplace How-Tos cover deploying applications or services available in the Cleura\u00a0Cloud Marketplace.</p> </li> </ul>"},{"location":"howto/account-billing/change-account-data/","title":"Changing your account data","text":"<p>You may at any time change the contact person, address, company name, and purchase order number associated with your Cleura\u00a0Cloud account.</p> <p>To get started, navigate to the Cleura\u00a0Cloud Management\u00a0Panel and log into your account. Expand the vertical pane on the left, click on Settings, and then on Manage Account.</p> <p>In the central pane, named Account Settings, click on the Customer Info tab. There, you can change and manage your customer information.</p> <p></p> <p>Finalize your changes by clicking the green Update button at the bottom.</p>"},{"location":"howto/account-billing/change-account-data/#adding-or-removing-email-addresses","title":"Adding or removing email addresses","text":"<p>In the Account Settings pane, click the tab labeled Contact. This will show your currently configured email addresses. If there is only one email address, it will have all available roles enabled.</p> <p></p>"},{"location":"howto/account-billing/change-account-data/#adding-a-new-email-address","title":"Adding a new email address","text":"<p>Click on the green button labeled Create new Email (see above).</p> <p></p> <p>A new vertical pane will slide over from the right-hand side of the Cleura\u00a0Cloud Management\u00a0Panel. Enter your new email address and click the green Create button.</p> <p>The system will then send an automated email containing a verification code. The verification email looks like the example below, with the subject line Please confirm your new address - Cleura Account.</p> <p></p> <p>Please enter this code to verify your email address. Then click the green button labeled Verify.</p> <p></p> <p>Choose a role association for the new email address, and click the Back button.</p> <p></p> <p>You have now added one more email address and selected a role.</p> <p></p>"},{"location":"howto/account-billing/change-account-data/#removing-an-email-address","title":"Removing an email address","text":"<p>Click on the red Trash icon  next to the email address you want to remove.</p> <p></p> <p>A pop-up window appears. Click on the red button labeled Yes, Delete.</p> <p></p> <p>You have now removed one email address.</p> <p></p>"},{"location":"howto/account-billing/change-account-data/#assigning-an-email-address-to-a-new-role","title":"Assigning an email address to a new role","text":"<p>Click on the Modify button  next to the email address you want to assign to a different role.</p> <p></p> <p>Change the role by clicking one or all of Billing, Tech, or DPO.</p> <p></p> <p>At this point, the roles associated with the email address have been changed:</p> <p></p>"},{"location":"howto/account-billing/change-account-data/#changing-an-accounts-organization-number","title":"Changing an account\u2019s organization number","text":"<p>Please note that you cannot readily change your organization number (for business accounts) or your personal number (for individual accounts) in the Cleura\u00a0Cloud Management\u00a0Panel.</p> <p>In case you do wish to change any of those, you will have to submit a Transfer Form via our Service\u00a0Center.</p>"},{"location":"howto/account-billing/change-credit-card/","title":"Managing your credit card information","text":"<p>You may add or change credit card information via the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"howto/account-billing/change-credit-card/#prerequisites","title":"Prerequisites","text":"<p>You must be logged in to your Cleura\u00a0Cloud account to manage credit card information.</p>"},{"location":"howto/account-billing/change-credit-card/#adding-a-credit-card","title":"Adding a credit card","text":"<p>In the Cleura\u00a0Cloud Management\u00a0Panel, make sure the vertical pane on the left is visible. Click on Settings, and then on Manage Invoice Settings. In the central pane, named Invoice Settings, notice the three tabs at the top. Click on the one labeled Credit Cards, and then on the green Add New Card button below.</p> <p></p> <p>You are redirected to a PayEx Sverige AB page, where you enter your credit card information. When you are done, click the black Confirm button.</p> <p></p> <p>To confirm the new card, Cleura\u00a0Cloud charges you zero (0) units of your card\u2019s currency. Depending on your bank, you might have to use whatever mechanism is provided to authorize that zero charge.</p> <p></p> <p>Once the new credit card is added, it will be visible in the Credit Cards tab.</p> <p></p>"},{"location":"howto/account-billing/change-credit-card/#changing-a-credit-card","title":"Changing a credit card","text":"<p>If you want to use another credit card, click the green Add New Card button below the existing card.</p> <p></p> <p>Configure the new card as you did when adding the existing card. When you are done adding the new one, you will realize that the old one is no longer listed under Credit Cards. In addition to the old card not being visible, please keep in mind that it is also removed from the system.</p>"},{"location":"howto/account-billing/change-credit-card/#removing-a-credit-card","title":"Removing a credit card","text":"<p>To remove a credit card you have already added to your account, you need to send an email to support@cleura.com. This email should come either from the technical contact address or the billing contact address of your account.</p>"},{"location":"howto/account-billing/delete-account/","title":"Deleting your account","text":"<p>In case you decide to close your Cleura\u00a0Cloud account, you may do so via the Cleura\u00a0Cloud Management\u00a0Panel. From the vertical pane on the left, select Settings and then Manage Account.</p> <p>In the central Account Settings pane, click the red Delete Account tab on the right.</p> <p></p> <p>At the bottom of the tab, click the red button named Delete Account.</p> <p></p> <p>Choose a reason for leaving and type in your current password. Click the red button named Delete Account one more time. That will trigger a request for account deletion.</p> <p></p>"},{"location":"howto/account-billing/delete-account/#using-the-termination-of-subscription-form","title":"Using the Termination of Subscription Form","text":"<p>You may be unable to follow the standard account deletion process using the Cleura\u00a0Cloud Management\u00a0Panel, because you have lost your administrative credentials or you cannot access your account owner\u2019s email address.</p> <p>In this case, you can instead terminate your account by filling out the Termination of Subscription Form. Be sure to add your own signature (for personal accounts), or that of an authorized signatory (for company accounts).</p> <p>Submit the form via our Service\u00a0Center. If you have lost your Service\u00a0Center credentials, you may also send it by email to support@cleura.com.</p>"},{"location":"howto/account-billing/delete-account/#what-happens-after-you-delete-your-account","title":"What happens after you delete your account","text":"<p>Three (3) days after receiving the request for deletion, any running virtual machines will be shut off. Then, after ten (10) days, all virtual machines and any other resources connected to your account will be deleted, including backups.</p> <p>After the last invoice has been paid, we will delete your account. This entire process may take up to thirty (30) days after the last invoice is paid.</p> <p>If you change your mind during the first three (3) days, please get in touch with our Service\u00a0Center, and we will not delete your assets or your account.</p>"},{"location":"howto/account-billing/e-invoice/","title":"E-invoicing","text":"<p>Electronic invoicing (e-invoicing) is available to select accounts. More specifically, you have the option of e-invoicing if all three of the following conditions are met:</p> <ul> <li>The country in the customer contract is set to Sweden (that is the country in the account owner data, not the country in the billing data),</li> <li>you have provided a company name, and</li> <li>you have provided a Swedish organisationsnummer (link in Swedish).</li> </ul> <p>If you are eligible for e-invoicing and wish to activate it, navigate to the Cleura\u00a0Cloud Management\u00a0Panel start page and log into your Cleura\u00a0Cloud account.</p> <p>Make sure the vertical pane at the left-hand side of the Cleura\u00a0Cloud Management\u00a0Panel is expanded. Click on Settings, then on Manage Invoice Settings. In the central pane of the Cleura\u00a0Cloud Management\u00a0Panel, titled Invoice Settings, be sure to have the Settings tab open, and then click on the green Request form button.</p> <p></p> <p>A pop-up window titled Request e-invoice will appear. Click on the Choose Provider drop-down menu to expand it.</p> <p></p> <p>Choose one of the available providers for your invoicing application.</p> <p></p> <p>Finally, click the green Send request button.</p> <p></p> <p>Please note that if your provider is not on the list, we cannot provide e-invoicing.</p>"},{"location":"howto/account-billing/forgot-your-password/","title":"Resetting your password or reclaiming your username","text":"<p>Whether you have to reset a forgotten password or remember the usernames associated with your Cleura\u00a0Cloud account, you can do so via the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"howto/account-billing/forgot-your-password/#reset-lost-password","title":"Reset lost password","text":"<p>Begin by navigating to the Cleura\u00a0Cloud Management\u00a0Panel. At the bottom left-hand side of the page, click on the \u201cForgot your password?\u201d button.</p> <p></p> <p>A pop-up window titled \u201cReclaim lost password!\u201d, appears. Type in your username, then click on the \u201cReclaim password!\u201d button.</p> <p></p> <p>The content of the pop-up window changes, informing you that an email has been sent to you with instructions on how to reset your password.</p> <p>Click the \u201cOk!\u201d button, then go ahead and check your inbox (or spam/junk folder) for an email with the subject Cleura Account password reset request.</p> <p></p> <p>The email will look like the one shown below. Just click the link to create a new password.</p> <p></p> <p>After clicking the link you are landed on a new page, where you are asked to set a new password.</p> <p></p> <p>Come up with a strong password and type it in twice, for security reasons. Click the \u201cSet new password\u201d button when you are done. You will be informed that the password has been reset, so click the \u201cGo back to login page\u201d button to login.</p> <p></p>"},{"location":"howto/account-billing/forgot-your-password/#reclaim-your-username","title":"Reclaim your username","text":"<p>Navigate to the Cleura\u00a0Cloud Management\u00a0Panel, and at the bottom left-hand side of the page click the \u201cForgot your password?\u201d button.</p> <p></p> <p>A pop-up window titled \u201cReclaim lost password!\u201d appears. Ignore the \u201cReclaim password!\u201d button, and follow the link \u201cI don\u2019t know my username\u201d instead.</p> <p></p> <p>The pop-up window title changes to \u201cReclaim lost username\u201d. Type in your account email address, then click the \u201cReclaim username\u201d button.</p> <p></p> <p>Now, the same pop-up window informs you that, provided the email address you just typed exists in the system, you will shortly receive a new email with your username.</p> <p>Click the \u201cOk!\u201d button, then go ahead and check your inbox (or spam/junk folder) for an email with the subject List of usernames for Cleura Cloud.</p> <p></p> <p>Open the email and find all the usernames associated with your Cleura\u00a0Cloud account.</p> <p></p>"},{"location":"howto/account-billing/manage-invoices/","title":"Managing invoices","text":"<p>You may quickly review your invoices regarding the monthly usage of Cleura\u00a0Cloud resources, selectively pay outstanding invoices, and save any number of them in PDF format.</p> <p>Using your favorite web browser, navigate to the Cleura\u00a0Cloud Management\u00a0Panel start page and log into your Cleura\u00a0Cloud account.</p>"},{"location":"howto/account-billing/manage-invoices/#listing-invoices","title":"Listing invoices","text":"<p>Make sure the vertical pane on the left-hand side of the dashboard is expanded. Click on Invoices, right below Users. In the main pane of the dashboard, you will see a list of paid and outstanding invoices.</p> <p></p> <p>Any paid invoice has a green  icon at the left of its row. At the Outstanding Amount column, for each paid invoice there is a value of <code>0.00</code>.</p> <p></p>"},{"location":"howto/account-billing/manage-invoices/#displaying-invoice-details","title":"Displaying invoice details","text":"<p>Notice the orange  icon at the right of any invoice row. By clicking on it, you get a pop-up with specific options regarding the corresponding invoice. For example, you can view the invoice or download it onto your computer in PDF format.</p> <p></p>"},{"location":"howto/account-billing/manage-invoices/#paying-outstanding-invoices","title":"Paying outstanding invoices","text":"<p>There is also the Pay this invoice option. Choose it for an outstanding invoice, and you will get a pop-up with a detailed rundown of the dues, together with available payment methods.</p> <p></p>"},{"location":"howto/account-billing/manage-invoices/#activate-automatic-payments","title":"Activate automatic payments","text":"<p>You may activate automatic payments for pending invoices.</p> <p>From the left-hand side vertical pane, click on Settings and then on Manage Invoice Settings. In the central pane, make sure you are in the Settings tab. Use the radio button on the right of Automatic Payment to enable automatic payments, and then click the green Update button. If the automatic payments are already enabled, you may disable them by using the same radio button.</p> <p></p>"},{"location":"howto/account-billing/rest-invoice-data/","title":"Retrieving invoice data with the Cleura Cloud REST API","text":"<p>You may download some or all of your Cleura\u00a0Cloud invoice data using the Cleura\u00a0Cloud REST\u00a0API.</p>"},{"location":"howto/account-billing/rest-invoice-data/#prerequisites","title":"Prerequisites","text":"<p>Before retrieving any of your invoice data, you need to create a token for the current session.</p>"},{"location":"howto/account-billing/rest-invoice-data/#saving-invoice-data","title":"Saving invoice data","text":"<p>To save JSON data about all available invoices locally, submit a Cleura\u00a0Cloud REST\u00a0API request like this:</p> <pre><code>curl -H \"X-AUTH-LOGIN: your_username\" \\\n     -H \"X-AUTH-TOKEN: your_token\" \\\n     -o invoices.json \\\n     https://rest.cleura.cloud/account/v1/invoices\n</code></pre> <p>A command like the above will download the data of up to 20 invoices and save them as JSON objects into the file <code>invoices.json</code>. You may change the number of invoices with the <code>limit</code> parameter:</p> <pre><code>curl -H \"X-AUTH-LOGIN: your_username\" \\\n     -H \"X-AUTH-TOKEN: your_token\" \\\n     -o invoices.json \\\n     https://rest.cleura.cloud/account/v1/invoices?limit=2\n</code></pre> <p>That will limit the number of invoices to 2. Please note that you may download data from up to 100 invoices.</p>"},{"location":"howto/account-billing/rest-invoice-data/#sifting-through-invoice-data","title":"Sifting through invoice data","text":"<p>Once you have your invoice data, you may sift through it with a tool like <code>jq</code>. For example, here is a quick way to check if all downloaded invoices are already paid:</p> <pre><code>jq '.[].status' invoices.json\n\"paid\"\n\"paid\"\n</code></pre> <p>To get a list with all invoice IDs, type the following:</p> <pre><code>jq '.[].id' invoices.json\n\"164...\"\n\"163...\"\n</code></pre> <p>You may now get more information regarding a specific invoice. Find out, for instance, when is (or was) the due date for the invoice with ID <code>164...</code>:</p> <pre><code>jq '.[] | select(.id==\"164...\") .dueDate' invoices.json\n\"2023-01-23\"\n</code></pre>"},{"location":"howto/getting-started/accessing-cc-rest-api/","title":"Accessing the Cleura Cloud REST API","text":"<p>Cleura\u00a0Cloud provides a REST API, which you can take advantage of with a tool like <code>curl</code>. But before you do, you must create a token for the current session. For that, you only need to have an account in Cleura\u00a0Cloud.</p>"},{"location":"howto/getting-started/accessing-cc-rest-api/#creating-a-token","title":"Creating a token","text":"<p>Cleura\u00a0Cloud REST\u00a0API calls use token authentication. The process of obtaining a valid token works slightly differently, based on whether your account has two-factor authentication (2FA) enabled or not.</p> Without 2FAWith 2FA <p>If you do not have 2FA enabled for your account, to create a token, have your username (<code>your_cc_username</code>) and password (<code>your_cc_password</code>) to the Cleura\u00a0Cloud handy and type:</p> <pre><code>curl -d '{\"auth\": {\"login\": \"your_cc_username\", \"password\": \"your_cc_password\"}}' \\\n    https://rest.cleura.cloud/auth/v1/tokens\n</code></pre> <p>Provided you typed your username and password correctly and that there were no connection issues to the remote endpoint, you will get a JSON object with a string (<code>token</code>) that holds your session token:</p> <pre><code>{\n  \"result\": \"login_ok\",\n  \"token\": \"vahkie7EiDaij7chegaitee2zohsh1oh\"\n}\n</code></pre> <p>If your Cleura\u00a0Cloud account has 2FA enabled, you must configure it with SMS as a second-factor option. Accounts with only WebAuthn as their second factor cannot be used for Cleura\u00a0Cloud REST\u00a0API operations.</p> <p>First, initiate a token request giving your Cleura\u00a0Cloud username and password, and setting the <code>twofa_method</code> option to <code>sms</code>:</p> <pre><code>curl -d '{\"auth\": {\"login\": \"your_cc_username\", \"password\": \"your_cc_password\", \"twofa_method\": \"sms\"}}' \\\n    https://rest.cleura.cloud/auth/v1/tokens\n</code></pre> <p>Instead of a token, you will get a verification code (look at <code>verification</code>):</p> <pre><code>{\n  \"result\": \"twofactor_required\",\n  \"type\": \"sms\",\n  \"verification\": \"ahb4en3cho\"\n}\n</code></pre> <p>This verification code is required for requesting a 2FA code:</p> <pre><code>curl -d '{\"request2fa\": {\"login\": \"your_cc_username\", \"verification\": \"ahb4en3cho\"}}' \\\n    https://rest.cleura.cloud/auth/v1/tokens/request2facode\n</code></pre> <p>You will now receive your 6-digit second-factor code via an SMS message. This enables you to request your REST API token like this:</p> <pre><code>curl -d '{\"verify2fa\": {\"login\": \"your_cc_username\", \"verification\": \"ahb4en3cho\", \"code\": 123456}}' \\\n    https://rest.cleura.cloud/auth/v1/tokens/verify2fa\n</code></pre> <p>Make sure not to put the code in quotes, for it is of type integer and that fact should be reflected during the assignment to <code>code</code>. You will get a JSON object with your token:</p> <pre><code>{\n  \"result\": \"login_ok\",\n  \"token\": \"fiushood9oraTiNa4ban3eemeezoeDae\"\n}\n</code></pre> <p>Now that you have obtained a valid token, you can proceed with making Cleura\u00a0Cloud REST\u00a0API calls.</p>"},{"location":"howto/getting-started/accessing-cc-rest-api/#testing-the-token","title":"Testing the token","text":"<p>One way to make sure the token is valid is by getting a list of all supported regions. All you have to do is use <code>curl</code> to provide your username (<code>your_cc_username</code>) and token and connect to the <code>https://rest.cleura.cloud/accesscontrol/v1/openstack/domains</code> endpoint:</p> <pre><code>curl -H \"X-AUTH-LOGIN: your_cc_username\" \\\n    -H \"X-AUTH-TOKEN: vahkie7EiDaij7chegaitee2zohsh1oh\" \\\n    https://rest.cleura.cloud/accesscontrol/v1/openstack/domains\n</code></pre> <p>All supported regions should be returned as objects in a JSON array:</p> <pre><code>[\n  {\n    \"area\": {\n      \"id\": \"6\",\n      \"name\": \"Sweden \\/ Stockholm\",\n      \"tag\": \"SE_STO2\",\n      \"regions\": [\n        {\n          \"zone_id\": \"2\",\n          \"name\": \"Stockholm \\/ Sweden\",\n          \"status\": \"active\",\n          \"region\": \"Sto2\"\n        }\n      ]\n    },\n    \"id\": \"79b0cef45b504586ad0bf057dc4cb8b8\",\n    \"status\": \"provisioned\",\n    \"name\": \"CCP_Domain_43597\",\n    \"enabled\": true\n  },\n  {\n    \"area\": {\n      \"id\": \"7\",\n      \"name\": \"Germany \\/ Frankfurt\",\n      \"tag\": \"DE\",\n      \"regions\": [\n        {\n          \"zone_id\": \"6\",\n          \"name\": \"Frankfurt \\/ Germany\",\n          \"status\": \"active\",\n          \"region\": \"Fra1\"\n        }\n      ]\n    },\n    \"id\": \"08db589d04f442e19c272f2cd1a02906\",\n    \"status\": \"provisioned\",\n    \"name\": \"CCP_Domain_43597\",\n    \"enabled\": true\n  },\n  {\n    \"area\": {\n      \"id\": \"1\",\n      \"name\": \"Europe\",\n      \"tag\": \"EU\",\n      \"regions\": [\n        {\n          \"zone_id\": \"1\",\n          \"name\": \"Karlskrona \\/ Sweden\",\n          \"status\": \"active\",\n          \"region\": \"Kna1\"\n        }\n      ]\n    },\n    \"id\": \"a018123218f5428ba96d7fb212d90cf2\",\n    \"status\": \"provisioned\",\n    \"name\": \"CCP_Domain_43597\",\n    \"enabled\": true\n  }\n]\n</code></pre>"},{"location":"howto/getting-started/create-account/","title":"Creating a new account","text":"<p>For access to Cleura\u00a0Cloud, you first have to create a new account. For that, navigate to https://cleura.cloud/register.</p> <p></p> <p>Select the new account type (Company or Private), type in a valid email address, and choose your country. Please be sure to read the Terms of Service and our Data Processing Agreement.</p> <p>Agree to these documents (select Yes), check the I\u2019m not a robot box, and then click on the Create button.</p> <p>This will redirect you to the Cleura\u00a0Cloud Management\u00a0Panel, and since you are logging in from a new account for the first time, you now have to take three steps.</p> <p></p> <ul> <li>Step 1 - Confirm your email.   Check your inbox or your spam/junk folder for an email from <code>no-reply@cleura.com</code> with the subject Thank you for your registration - Cleura Cloud.   Open that email, and click on the link in the message body.</li> </ul> <p></p> <ul> <li>Step 2 - Account information.   After clicking on the confirmation link you move on to step two, where you enter the information that uniquely identifies the brand-new account.   Type in a username for the account user, and define a strong password.   (We recommend using a password manager.)   Please note that all fields are mandatory.   In case you have a voucher code, click on I have a voucher code and type it in below.   When you are done, click on the Save button.</li> </ul> <p></p> <ul> <li>Step 3 - Account verification.   To make your account fully operational, you have to take one more verification step.   You do that either by entering valid credit card information, or by placing a phone call.   Verifying by credit card does not incur charges.   If you prefer to verify by phone (available only for Company accounts), you may do so during business hours (08:00 \u2013 17:00 CET/CEST UTC+1/UTC+2).   During the verification call, you will be asked for the username of the new account.</li> </ul> <p></p> <p>After the account verification is complete, you can start using Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"howto/getting-started/enable-openstack-cli/","title":"Enabling the OpenStack CLI","text":"<p>The OpenStack Command Line Interface (CLI) tool, also known as OpenStack Client (OSC) or simply <code>openstack</code>, conveniently provides access to various OpenStack APIs. Using the OpenStack CLI tool, you can remotely create and manage the lifecycle of objects related, for example, to Compute, Networking, or Storage.</p> <p>Before installing <code>openstack</code> to your local laptop or workstation, you first need to have an OpenStack user in your Cleura\u00a0Cloud account. Next, you create and download a special RC file onto your computer, modify it to reflect your OpenStack user\u2019s credentials, and source it. Only then will you be able to use any installed <code>openstack</code> client.</p>"},{"location":"howto/getting-started/enable-openstack-cli/#creating-an-openstack-user","title":"Creating an OpenStack user","text":"<p>From your favorite web browser, navigate to the Cleura\u00a0Cloud Management\u00a0Panel start page, and login into your Cleura\u00a0Cloud account.</p> <p>Please make sure the left-hand side pane on the Cleura\u00a0Cloud Management\u00a0Panel is fully visible, click the Users category to expand it, and click on Openstack Users.</p> <p></p> <p>Then, at the top right-hand side of the Cleura\u00a0Cloud Management\u00a0Panel, click once more the Add new Openstack user option. A new pane will slide into view, titled Create Openstack User.</p> <p></p> <p>Type in a username and a password for the new OpenStack user. To ensure you typed the password correctly, you must re-type it below. This password should be adequately strong, and thus a password manager may come in handy.</p> <p></p> <p>Scroll down a bit, so the Regions section is in full view. Expand one or more of the available regions you want your new user to have access to. For each one of the expanded regions, select one or more Projects. For each project, activate one or more Roles. (Hint: For an overview of the rights that roles provide, hover the mouse pointer over the exclamation mark icon by the Roles.)</p> <p></p> <p>Optionally, type in a description for the new OpenStack user. Then, create the user by clicking the green Create button below the Description box.</p> <p></p> <p>The new OpenStack user will be ready in just a few seconds. At any time, you can view all available OpenStack users by going to the left-hand side pane on the Cleura\u00a0Cloud Management\u00a0Panel and selecting Users &gt; Openstack Users.</p> <p></p>"},{"location":"howto/getting-started/enable-openstack-cli/#downloading-an-rc-file","title":"Downloading an RC file","text":"<p>On the Cleura\u00a0Cloud Management\u00a0Panel, expand the left-hand side vertical pane, click Users, and then Openstack Users. You will see, listed in the main pane, all available users. Click the three-dot round icon on the right of the user their RC file you want. From the drop-down menu that appears, select Download RC-File.</p> <p></p> <p>Before downloading the RC file onto your local computer, you must select one of the available projects to relate it to. Do so and then click the blue Download button.</p> <p></p> <p>A Save as dialog window appears. Select a convenient location and save your RC file.</p>"},{"location":"howto/getting-started/enable-openstack-cli/#modifying-and-sourcing-the-rc-file","title":"Modifying and sourcing the RC file","text":"<p>The general naming for RC files goes like this:</p> <pre><code>your_username--region_name--project_name--rc\n</code></pre> <p>So, assuming your username is <code>olafsdottir</code>, and the RC file has been created for the <code>fra1</code> region and the <code>katla</code> project, your RC file name should be this:</p> <pre><code>olafsdottir--fra1--katla--rc\n</code></pre> <p>Take a look at the contents of this file \u2014 they should be like this:</p> <pre><code>export OS_USERNAME=olafsdottir\nexport OS_PASSWORD=&lt;your password goes here&gt;\nexport OS_AUTH_URL=https://fra1.citycloud.com:5000\nexport OS_USER_DOMAIN_NAME=...\nexport OS_PROJECT_DOMAIN_NAME=...\nexport OS_REGION_NAME=Fra1\nexport OS_PROJECT_NAME=\"katla\"\nexport OS_TENANT_NAME=\"katla\"\nexport OS_AUTH_VERSION=3\nexport OS_IDENTITY_API_VERSION=3\n</code></pre> <p>Before you source the RC file, and thus initialize all relevant environment variables, make sure to edit the file and put your OpenStack user password in place of <code>&lt;your password goes here&gt;</code>. Also, change the permissions of the file, so it is readable and writable by your local user only:</p> <pre><code>chmod 600 olafsdottir--fra1--katla--rc\n</code></pre> <p>Then, go ahead and source it:</p> <pre><code>source olafsdottir--fra1--katla--rc\n</code></pre>"},{"location":"howto/getting-started/enable-openstack-cli/#installing-the-openstack-cli","title":"Installing the OpenStack CLI","text":"<p>If you do not have the OpenStack CLI tool readily available, use your operating system\u2019s package manager or <code>pip</code> to install it. Some examples follow.</p> Debian/UbuntuMac OS X with HomebrewPython package <pre><code>apt update &amp;&amp; apt install python3-openstackclient\n</code></pre> <pre><code>brew install openstackclient\n</code></pre> <pre><code>pip install python-openstackclient\n</code></pre>"},{"location":"howto/getting-started/enable-openstack-cli/#testing-access","title":"Testing access","text":"<p>Provided you have already sourced your RC file, you can now use the <code>openstack</code> command line tool to access various OpenStack APIs on the Cleura\u00a0Cloud.</p> <p>To make sure your local installation of <code>openstack</code> works as expected, type:</p> <pre><code>openstack token issue\n</code></pre> <p>If <code>openstack</code> can indeed connect to the Cleura\u00a0Cloud OpenStack APIs, then you will get information, in tabular format, regarding the issuance of a new token.</p> <p>To get general help regarding <code>openstack</code>, type:</p> <pre><code>openstack --help\n</code></pre> <p>When you need help on a specific command, type something like <code>openstack help command</code>.</p>"},{"location":"howto/getting-started/enable-openstack-cli/#auto-adjusting-the-cli-output-to-your-terminal-size","title":"Auto-adjusting the CLI output to your terminal size","text":"<p>Many of the subcommands available in the <code>openstack</code> CLI produce tabular about by default. To ensure that this output always fits neatly into your terminal window, you may add the following line either to OpenStack RC file(s), or to your shell initialization file (like <code>.profile</code> or <code>.bashrc</code>):</p> <pre><code>export CLIFF_FIT_WIDTH=1\n</code></pre> <p>Then, be sure to either re-source the file you modified, and/or restart your shell.</p>"},{"location":"howto/kubernetes/","title":"Kubernetes in Cleura Cloud","text":"<p>In Cleura\u00a0Cloud, you have several options for deploying and managing Kubernetes clusters.</p> <p>Cleura\u00a0Cloud Management\u00a0Panel includes management interfaces for Gardener and OpenStack Magnum. To manage Magnum clusters, you can also use the OpenStack command-line interface.</p> <p>To assess which facility is more suitable for your specific deployment scenario and use case, refer to this summary.</p>"},{"location":"howto/kubernetes/gardener/","title":"Gardener","text":"<p>Gardener in Cleura\u00a0Cloud is a Kubernetes-native system that provides automated management and operation of Kubernetes clusters as a service. It allows you to create clusters and automatically handle their lifecycle operations, including configurable maintenance windows, hibernation schedules, and automatic updates to Kubernetes control plane and worker nodes.</p> <p>You can read more about Gardener and its capabilities on its documentation website.</p>"},{"location":"howto/kubernetes/gardener/#activating-the-gardener-service","title":"Activating the Gardener service","text":"<p>To use Gardener in Cleura\u00a0Cloud, you first need to activate the service. You can conveniently do this via the Cleura\u00a0Cloud Management\u00a0Panel.</p> <p>To activate Gardener, select Containers \u2192 Gardener in the side panel. Then, click the Activate Gardener Service button:</p> <p></p> <p>You only need to do this once.</p>"},{"location":"howto/kubernetes/gardener/#deploying-kubernetes-clusters-with-gardener","title":"Deploying Kubernetes clusters with Gardener","text":"<p>To learn how to use Gardener in the Cleura\u00a0Cloud Management\u00a0Panel, refer to Creating Kubernetes clusters with Gardener.</p>"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/","title":"Creating a Kubernetes cluster","text":"<p>To create a Kubernetes cluster, you may use the Cleura\u00a0Cloud Management\u00a0Panel and the Gardener service in particular. This guide shows you how to do that.</p>"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>If this is your first time using Gardener in Cleura\u00a0Cloud, you need to activate the service from the Cleura\u00a0Cloud Management\u00a0Panel.</li> <li>To access the Kubernetes cluster from your computer, you must install\u00a0<code>kubectl</code> on your local host.</li> </ul>"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/#creating-a-kubernetes-cluster-in-cleura-cloud-management-panel","title":"Creating a Kubernetes cluster in Cleura\u00a0Cloud Management\u00a0Panel","text":"<p>To get started, navigate to https://cleura.cloud and, in the left-hand side panel, choose Containers\u00a0\u2192\u00a0Gardener. A central pane named Gardener / Shoot Clusters appears. There, you can create and manage your Gardener-based clusters. In Gardener terminology, a Kubernetes cluster is referred to as a shoot (as in, new plant growth).</p> <p>At the top right-hand side of the central pane, click on Create\u00a0Kubernetes\u00a0cluster.</p> <p></p> <p>A new pane named Create Gardener Shoot Cluster slides over from the right-hand side of the browser. Type in a name for the new shoot cluster, and select a region. Also, select the version of Kubernetes the new cluster will be running. In the example below, we have chosen version 1.30.5, which, at the time of writing, was the latest supported in Cleura\u00a0Cloud. Please note the Load balancer provider and Availability Zone parameters. Both are already set and for the time being, you cannot modify any of them.</p> <p></p> <p>Next, you may accept the proposed network address (in CIDR notation) for the worker nodes, or type in the one you prefer.</p> <p></p> <p>Alternatively, click on the drop-down menu at the right-hand side of Network for worker nodes. From the available menu items, choose a pre-existing network. Be sure to select one that has an assigned subnet and is connected to a router.</p> <p></p> <p>Then, define a new subnet for the worker nodes by typing in a CIDR network address. If the new subnet overlaps with one of the subnets of the network you just selected, you will see an error message in red.</p> <p></p> <p>Instead, you should define a subnet that does not overlap with any of the subnets of your selected network.</p> <p></p> <p>Now, scroll down a bit until you bring the Worker Groups section into full view. Make sure there is at least one defined. (By default, you start with one Worker Group.) Pay attention to the values you may set for the following parameters, and keep in mind that they apply to the particular Worker Group only. In case you\u2019ll create another Worker Group, you\u2019ll have another set of parameters for it.</p> <ul> <li>Flavor: The flavor your worker nodes will use; this determines the number of CPU cores and the amount of RAM allocated to them.</li> <li>Volume Size: The amount of local storage allocated to each worker node.</li> <li>Autoscaler Min: The minimum number of worker nodes to run at any time.</li> <li>Autoscaler Max: The maximum number of worker nodes the cluster automatically scales to, in the event that the current number of nodes cannot handle the deployed workload.</li> <li>Max Surge: The maximum number of additional nodes to deploy in an autoscaling event.</li> </ul> <p></p> <p>For a test cluster, feel free to leave each parameter at its default value. When you scroll a bit further down, you see that for each Worker Group you may add Labels, Annotations, or Taints.</p> <p></p> <p>Each shoot cluster in Cleura\u00a0Cloud has a maintenance window. Additionally, the operating system of the worker nodes can be automatically updated, and so can Kubernetes itself. Do not change anything regarding the maintenance window or the updates, for everything is already set for you. Instead, at the bottom of the pane, click the green Create button.</p> <p></p> <p>In the list of clusters, you will see your new Gardener shoot bootstrapping. The animated icon on the left-hand side of the cluster row marks the progress. Creating the cluster may take several minutes.</p> <p></p> <p>Once your Gardener cluster has successfully launched, a green check mark will appear at the left of the cluster row.</p> <p></p>"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/#a-note-on-quotas","title":"A note on quotas","text":"<p>Your Gardener worker nodes are subject to quotas applicable to your Cleura\u00a0Cloud project. It would be best if you kept that in mind when selecting the worker node flavor, setting the volume size and the Autoscaler Max values, or when creating more than one Worker Groups, so you will not be at risk of violating any quota.</p> <p>For example, if your project is configured with the default quotas, and you select the <code>b.4c16gb</code> flavor for your worker nodes, your cluster would be able to run with a maximum of 3 worker nodes (since their total memory footprint would be 3\u00d716=48\u00a0GiB, just short of the default 50\u00a0GiB limit). A 4th node would push your total memory allocation to 64\u00a0GiB, violating your quota.</p> <p>If necessary, be sure to request a quota increase via our Service\u00a0Center.</p>"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/#viewing-details-and-monitoring","title":"Viewing details and monitoring","text":"<p>After the new shoot cluster finishes bootstrapping, you may click on its row to bring all relevant details into view. For instance, click the Details tab to get networking information about the worker nodes.</p> <p></p> <p>Next, go to the Monitoring tab. There, you will notice two orange buttons: one for launching Prometheus, and one for launching Plutono.</p> <p></p> <p>As an example, we have clicked the button for Plutono. Prior to launching the dashboard, a pop-up window appears. This shows the default username (<code>admin</code>), and the masked generated password. You get this pop-up every time you click on either of the orange buttons. In any case, to copy the password into the clipboard, click the related button shown below. Then, click the orange button labeled Open Dashboard.</p> <p></p> <p>After typing in the default username and pasting the password, you get access to the Prometheus or Plutono dashboard for your shoot cluster. In the example below, we have the Plutono dashboard displaying graphical information regarding the cluster nodes.</p> <p></p>"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/#interacting-with-your-cluster","title":"Interacting with your cluster","text":"<p>Once your new shoot cluster is operational, you can start interacting with it.</p>"},{"location":"howto/kubernetes/gardener/hibernate-shoot-cluster/","title":"Hibernating a Kubernetes cluster","text":"<p>There will be times when you won\u2019t be using your Gardener-based cluster much, if at all. To save on costs, you can put the whole cluster in hibernation. If you do, then from that time on and until you wake up the cluster again, you will be paying less for it.</p>"},{"location":"howto/kubernetes/gardener/hibernate-shoot-cluster/#prerequisites","title":"Prerequisites","text":"<p>We assume you have already used Gardener in Cleura\u00a0Cloud to spin up a Kubernetes cluster, which is now humming away. If you\u2019ve never done this before, please feel free to follow this guide.</p>"},{"location":"howto/kubernetes/gardener/hibernate-shoot-cluster/#hibernating-a-cluster","title":"Hibernating a cluster","text":"<p>Fire up your favorite web browser and navigate to https://cleura.cloud. Make sure the vertical pane at the left-hand side of the page is in full view, then choose Containers\u00a0\u2192\u00a0Gardener. You will see your Gardener cluster in the main pane. Click on its row for a detailed view of the various cluster characteristics. Bring up the Status tab and, in the Constraints section, check whether hibernation is possible. For the shoot cluster in the example below, it is indeed possible.</p> <p></p> <p>To hibernate the cluster, click the orange  icon at the right-hand side of the cluster row. From the pop-up menu that appears, select Hibernate Cluster.</p> <p></p> <p>A big pop-up window labeled About to hibernate a gardener shoot appears, asking you to confirm the action. Click the red button labeled Yes, Hibernate.</p> <p></p> <p>The cluster will begin hibernating. The animated icon at the left-hand side of the cluster row marks the progress.</p> <p></p> <p>After a minute or two, the cluster will be in a hibernated state. This fact will be indicated by the red  icon, again at the left-hand side of the cluster row.</p> <p></p> <p>From this point on, and as long as the cluster is in hibernation, any attempt to access it, e.g., via <code>kubectl</code>, will fail.</p>"},{"location":"howto/kubernetes/gardener/hibernate-shoot-cluster/#waking-up-a-cluster-in-hibernation","title":"Waking up a cluster in hibernation","text":"<p>To wake up a cluster in hibernation, click the orange  icon at the right-hand side of its row. From the pop-up menu that appears, select Wake up Cluster.</p> <p></p> <p>A big pop-up window labeled About to wakeup a gardener shoot appears, asking you to confirm that you want to wake up the cluster. Go ahead and click the red Yes, Wakeup button.</p> <p></p> <p>The cluster will start waking up. Again, the animated icon at the left-hand side of the cluster row marks the progress.</p> <p></p> <p>After a couple of minutes, the cluster will be fully awake. The fact will be indicated by the green  icon at the left-hand side of the cluster row.</p> <p></p> <p>From then on, the cluster will be fully operational and accessible again.</p>"},{"location":"howto/kubernetes/gardener/kubectl/","title":"Managing a Kubernetes cluster","text":"<p>Once you have launched a new cluster, you can interact with it using <code>kubectl</code> with a kubeconfig file.</p>"},{"location":"howto/kubernetes/gardener/kubectl/#prerequisites","title":"Prerequisites","text":"<p>On your local computer, you should make sure you have <code>kubectl</code>, the Kubernetes command line tool. As soon as you do, you may issue commands against your clusters. To install <code>kubectl</code>, follow the official documentation.</p>"},{"location":"howto/kubernetes/gardener/kubectl/#extracting-the-kubeconfig-file","title":"Extracting the kubeconfig file","text":"<p>You can access your shoot cluster using a certificate-based kubeconfig file. Such a kubeconfig is valid for a predefined amount of time, during which it can be used for accessing the corresponding shoot cluster with <code>cluster-admin</code> privileges. The credentials associated with this type of kubeconfig are client certificates with time-limited validity.</p> <p>To get a kubeconfig file for a shoot cluster, in the Cleura\u00a0Cloud Management\u00a0Panel click on the cluster row to bring its properties into full view and go to the KubeConfig tab.</p> <p>Notice that you may set the duration of the kubeconfig file validity.</p> <p></p> <p>The default validity period is one hour, but you may use the drop-down menu to select a different duration.</p> <p></p> <p>When satisfied with the duration, click the green button labeled Generate config.</p> <p></p> <p>Right below, you will see the contents of your dynamically generated kubeconfig. To get the file, click the blue button labeled Download KubeConfig.</p> <p></p> <p>In the default download folder of your local user account, you will get a configuration file named like so:</p> <pre><code>kubeconfig--&lt;cluster_name&gt;--&lt;region_name&gt;--&lt;project_id&gt;.yaml\n</code></pre> <p>Create a directory named <code>.kube</code> in your local user\u2019s home, then move the YAML file you downloaded into it. Rename the YAML to <code>config</code>, ending up with <code>~/.kube/config</code>. By default, <code>kubectl</code> searches for its configuration in <code>~/.kube/config</code>. If necessary, you can modify this behavior by setting the <code>KUBECONFIG</code> environment variable. For example, if you want to retain the original filename exactly as downloaded, you might type something like the following:</p> <pre><code>export KUBECONFIG=~/.kube/kubeconfig--&lt;cluster_name&gt;--&lt;region_name&gt;--&lt;project_id&gt;.yaml\n</code></pre> <p>If you manage multiple Kubernetes clusters, you will probably prefer this approach.</p>"},{"location":"howto/kubernetes/gardener/kubectl/#verifying-your-kubeconfig","title":"Verifying your kubeconfig","text":"<p>Check if your <code>kubectl</code> uses the proper configuration by running:</p> <pre><code>kubectl config view\n</code></pre> <p>You should see something like this:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://api.myshoot.pqrxyz.k8s.cleura.cloud\n  name: garden-pqrxyz--myshoot-external\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://api.myshoot.pqrxyz.internal.k8s.cleura.cloud\n  name: garden-pqrxyz--myshoot-internal\ncontexts:\n- context:\n    cluster: garden-pqrxyz--myshoot-external\n    user: garden-pqrxyz--myshoot-external\n  name: garden-pqrxyz--myshoot-external\n- context:\n    cluster: garden-pqrxyz--myshoot-internal\n    user: garden-pqrxyz--myshoot-external\n  name: garden-pqrxyz--myshoot-internal\ncurrent-context: garden-pqrxyz--myshoot-external\nkind: Config\npreferences: {}\nusers:\n- name: garden-pqrxyz--myshoot-external\n  user:\n    client-certificate-data: DATA+OMITTED\n    client-key-data: DATA+OMITTED\n</code></pre> <p>Keep in mind that the cluster API endpoints (the <code>server</code> entries in your kubeconfig) are dynamically managed DNS addresses. Gardener in Cleura\u00a0Cloud automatically created the DNS records upon shoot cluster creation.</p> <p>The DNS records will subsequently disappear when you delete the cluster. They also disappear when you hibernate the shoot cluster, and reappear when you wake it from hibernation.</p>"},{"location":"howto/kubernetes/gardener/kubectl/#accessing-your-cluster-with-kubectl","title":"Accessing your cluster with <code>kubectl</code>","text":"<p>Check your available nodes by running:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Assuming you used the default options when creating the cluster, you should now see the three Gardener worker nodes that are initially available:</p> <pre><code>NAME                                           STATUS   ROLES    AGE   VERSION\nshoot--pqrxyz--myshoot-cwncz1-z1-7656c-cx8zn   Ready    &lt;none&gt;   23h   v1.30.5\nshoot--pqrxyz--myshoot-cwncz1-z1-7656c-nvgmc   Ready    &lt;none&gt;   23h   v1.30.5\nshoot--pqrxyz--myshoot-cwncz1-z1-7656c-rrkmz   Ready    &lt;none&gt;   23h   v1.30.5\n</code></pre> <p>Please note that in contrast to an OpenStack\u00a0Magnum-managed Kubernetes cluster, where the output of <code>kubectl\u00a0get\u00a0nodes</code> includes control plane and worker nodes, in a Gardener cluster the same command only lists the worker nodes.</p>"},{"location":"howto/kubernetes/gardener/kubectl/#deploying-an-application","title":"Deploying an application","text":"<p>Create a sample deployment with a Hello World application:</p> <pre><code>$ kubectl create deployment hello-node --image=registry.k8s.io/echoserver:1.4\ndeployment.apps/hello-node created\n\n$ kubectl expose deployment hello-node --type=LoadBalancer --port=8080\nservice/hello-node exposed\n</code></pre> <p>To access the created app, list the available services:</p> <pre><code>kubectl get services\n</code></pre> <p>After a minute or so, you should get the load balancer service with its external IP and port number:</p> <pre><code>NAME         TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)          AGE\nhello-node   LoadBalancer   100.70.213.140   198.51.100.42  8080:32421/TCP   2m50s\nkubernetes   ClusterIP      100.64.0.1       &lt;none&gt;         443/TCP          23h\n</code></pre> <p>Open a browser and navigate to <code>http://198.51.100.42:8080</code> (substituting the correct <code>EXTERNAL-IP</code> listed for your service). You should see the page of the Hello World app.</p> <p></p>"},{"location":"howto/kubernetes/gardener/kubectl/#rotating-kubeconfig","title":"Rotating kubeconfig","text":"<p>You may rotate an existing kubeconfig for a fresh one. Since certificate-based kubeconfig files have time-bound validity, issuing fresh kubeconfig files of this type is standard practice.</p> <p>In addition to creating new certificate-based kubeconfig files, it is highly recommended to rotate the CA bundle used for creating them in the first place. Here\u2019s how to go about it, using the Cleura\u00a0Cloud Management\u00a0Panel.</p> <p>Locate the shoot cluster you are interested in. Click on its row to bring all relevant details into view, and go to the KubeConfig tab. Click the red button labeled Rotate CA Bundle.</p> <p></p> <p>A new window pops up, informing you in detail about the 3-step process that is about to take place. Please notice that it may take up to twenty minutes to complete. When ready, click the red Start CA rotation button.</p> <p></p> <p>During the first step, a new CA bundle is generated. This may take up to ten minutes to complete. During that time, the existing kubeconfig file still works.</p> <p></p> <p>When the new CA bundle is ready, you remove the existing one by clicking the red Remove old CA bundle button. As soon as you click it, the existing kubeconfig stops working.</p> <p></p> <p>The old CA bundle removal process may take up to ten minutes. Notice the gray animated icon, indicating the process is ongoing.</p> <p></p> <p>It is now time to create a new certificate-based kubeconfig, which will be based on the new CA bundle. This time, click the Back button, or the CA bundle creation process will start over.</p> <p></p> <p>Choose a duration for the new kubeconfig, then click the green Generate config button.</p> <p></p> <p>To get your new certificate-based kubeconfig file, click the blue Download KubeConfig button.</p> <p></p>"},{"location":"howto/kubernetes/gardener/rolling-upgrades/","title":"Conducting rolling upgrades","text":"<p>By default, Kubernetes clusters created with Gardener in Cleura\u00a0Cloud are upgraded automatically. Those upgrades take place during a specified maintenance window, and you may learn more about what they involve and how they work. Besides the automatic upgrades, you may manually apply any upgrades available for your cluster.</p> <p>In the following two examples, we first show how to manually upgrade the machine image for cluster nodes. Then, we show how to upgrade the Kubernetes version the cluster is running.</p>"},{"location":"howto/kubernetes/gardener/rolling-upgrades/#upgrading-machine-images","title":"Upgrading machine images","text":"<p>In the left-hand vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, select Containers \u2192 Gardener. In the central pane are your Gardener clusters, each on its row.</p> <p>If a Kubernetes upgrade is available for a shoot cluster, you will notice a blue  icon on the right of the current Kubernetes version. In our example, the current Kubernetes version is 1.29.9. Additionally, if there is a new machine image for the cluster nodes, you will also notice a blue  icon on the right of the number of Worker Groups. In our example, the cluster has 1 Worker Group.</p> <p></p> <p>To get more information about the current machine image, click on the cluster row to bring its details into full view, then select the Worker Groups tab. There, you will see the version of the current machine image. To see the newer version of the image available, hover the mouse over any of the two  icons. The upgrade process starts by clicking on any one of those.</p> <p></p> <p>The upgrade process starts immediately and takes some time to complete. The animated icon at the left-hand side of the cluster row marks the progress.</p> <p></p> <p>As soon as the upgrade is complete, the new image version is displayed in the Image row of the Worker Groups tab. Since we have not upgraded Kubernetes yet, there is still a blue  icon on the right of the current Kubernetes version.</p> <p></p>"},{"location":"howto/kubernetes/gardener/rolling-upgrades/#upgrading-kubernetes","title":"Upgrading Kubernetes","text":"<p>To get the Kubernetes upgrade process going, click the orange  icon on the right-hand side of the cluster row. In the pop-up menu that appears, there is an option named Kubernetes Patch Available. That one is not always there, and when it is, it acts as yet another indicator of an available Kubernetes upgrade. Please select the option by clicking on it.</p> <p></p> <p>A new pop-up window named Upgrade Kubernetes Version for: &lt;cluster_name&gt; appears. From the Version drop-down menu, you may select the new Kubernetes version for your shoot cluster. When ready, click the blue Upgrade button.</p> <p></p> <p>The upgrade doesn\u2019t start right away. Instead, you are informed that the cluster is indeed ready for it, but there are some notes you might want to read up on before the actual process begins.</p> <p></p> <p>Click the Upgrade button \u2014 which is now green \u2014 to start the upgrade process. The animated icon at the left-hand side of the cluster row indicates the progress of the whole upgrade process.</p> <p></p> <p>After some minutes, the upgrade will be over. The new Kubernetes version will be visible on the right of the shoot cluster name.</p> <p></p>"},{"location":"howto/kubernetes/magnum/","title":"Magnum","text":"<p>Magnum lets you create clusters via the OpenStack APIs. To do that, you base your configuration on a Cluster Template. The template defines parameters, such as worker flavors, which describe how the cluster will be constructed. In each region, we offer predefined public Cluster Templates with ready-to-use configurations. To learn more about Cluster Templates, check out the Magnum documentation.</p> <p>Once you have chosen your Cluster Template, you move on to create a cluster based on that template. When you create the cluster, you can define the number of nodes, ask for multiple masters behind a load balancer, etc.</p>"},{"location":"howto/marketplace/","title":"Cleura Cloud Marketplace","text":"<p>In Cleura Cloud, there is a Marketplace where you can find readily available applications and services like Harbor, Grafana, Prometheus, and others.</p> <p></p> <p>You may deploy any of these with a few clicks and start using them right away.</p> <p>This section contains step-by-step guides for deploying or deleting any of the applications and services in the Marketplace.</p>"},{"location":"howto/marketplace/bareos/delete/","title":"Deleting a Bareos instance","text":"<p>This short guide covers the deletion of a self-hosted Bareos deployment.</p> <p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Provisioned Apps. All your deployed applications, including the one you are about to delete, are listed in the central pane. Notice the orange  icon at the right of the application row. Click on it, and from the drop-down menu that appears, select Delete Provisioned App.</p> <p></p> <p>A new window named About to delete a Provisioned App appears. Make sure it refers to the Bareos deployment you wish to delete. When you are ready, click on the red Yes, Delete button.</p> <p></p> <p>After a few seconds, Bareos will be deleted, and that fact will be reflected in the list of provisioned applications.</p> <p></p>"},{"location":"howto/marketplace/bareos/deploy/","title":"Creating a Bareos instance","text":"<p>This guide covers the deployment of a self-hosted Bareos service.</p> <p>To proceed, make sure you have an account in Cleura\u00a0Cloud, and you are logged in to the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"howto/marketplace/bareos/deploy/#step-by-step-deployment","title":"Step-by-step deployment","text":"<p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Discover Apps and Services. In the central pane, you will see all available applications and services. Locate the Bareos box and click the green View button.</p> <p></p> <p>You will see the Bareos information page, where you can learn more about its features, and obtain pricing information. Click the orange Deploy this App button to start the deployment process.</p> <p></p> <p>The Bareos application is hosted on a Nova VM, so now you may select a region, a name, a flavor, a public network, a keypair, and a security group for it. Regarding the security group, make sure it includes a rule allowing incoming TCP connections to port 80.</p> <p>Read and agree to the Terms and Conditions. When you are ready, click the green Create button.</p> <p></p> <p>The deployment takes some minutes to complete. To check how it is going, expand the Marketplace category in the vertical pane on the left and click Provisioned Apps. In the central pane, watch the Bareos Heat stack row. The animated icon at the left marks the deployment progress.</p> <p></p> <p>When the deployment is complete, you will see a white check mark in a green circle.</p> <p></p>"},{"location":"howto/marketplace/bareos/deploy/#logging-into-the-bareos-dashboard","title":"Logging into the Bareos dashboard","text":"<p>You must know the URL of Bareos and the password automatically generated for the <code>administrator</code> user. For that, make sure you are in the Provisioned Apps pane. Click on the Bareos row to expand it, and select the Stack Output tab.</p> <p></p> <p>Get a pop-up window revealing the URL of the particular Bareos deployment. Click the icon in the Action column of the harbor_webui_url row, then click the blue Copy Output! button. Paste that into a new text editor window, but don\u2019t save it in a new file. Instead, we recommend you create a new entry in your password manager of choice and use the URL there. Close the pop-up window by clicking on the Back button.</p> <p></p> <p>In the webui_credentials row, click the icon in the Action column. A pop-up window appears. Click the blue Copy Output! button to copy the content of the Output box to the clipboard. Paste that into a new text editor window. Move the user name (<code>administrator</code>) and the password into the password manager entry you just created. Again, the pop-up window by clicking on the Back button.</p> <p></p> <p>Using your favorite web browser, navigate to your Bareos deployment\u2019s URL. The Bareos login page appears. Type in the user name (<code>administrator</code>), paste the password from your password manager, and click the Login button below.</p> <p></p> <p>You are directed to the Bareos main page.</p> <p></p> <p>On a separate browser window or tab, navigate to the Bareos documentation page to learn how to use your new self-hosted backup and data recovery service.</p>"},{"location":"howto/marketplace/clavister/delete/","title":"Deleting a Clavister NetWall instance","text":"<p>This short guide covers the deletion of a self-hosted Clavister NetWall deployment.</p> <p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Provisioned Apps. All your deployed applications, including the one you are about to delete, are listed in the central pane. Notice the orange  icon at the right of the application row. Click on it, and from the drop-down menu that appears, select Delete Provisioned App.</p> <p></p> <p>A new window named About to delete a Provisioned App appears. Make sure it refers to the Clavister NetWall deployment you wish to delete. When you are ready, click on the red Yes, Delete button.</p> <p></p> <p>After a few seconds, Clavister NetWall will be deleted, and that fact will be reflected in the list of provisioned applications.</p> <p></p>"},{"location":"howto/marketplace/clavister/deploy/","title":"Creating a Clavister NetWall instance","text":"<p>This guide covers the deployment of a Clavister NetWall application firewall. To proceed, make sure you have an account in Cleura\u00a0Cloud, and you are logged in to the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"howto/marketplace/clavister/deploy/#step-by-step-deployment","title":"Step-by-step deployment","text":"<p>In the left-hand side vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Discover Apps and Services. In the central pane, you will see all available applications and services. Locate the Clavister NetWall box and click the green View button.</p> <p></p> <p>You will see the Clavister NetWall information page, where you can learn more about its features, and obtain pricing information. Click the orange Deploy this App button to start the deployment process.</p> <p></p> <p>The firewall is hosted on a Nova VM, so now you may select a region, a name, a flavor, a public network, and a keypair for it.</p> <p>Read and agree to the Terms and Conditions. When you are ready, click the green Create button.</p> <p></p> <p>The deployment takes a few minutes to complete. To check how it is going, expand the Marketplace category in the vertical pane on the left and click Provisioned Apps. In the central pane, watch the firewall Heat stack row. The animated icon at the left marks the deployment progress.</p> <p></p> <p>When the deployment is complete, you will see a white check mark in a green circle at the left of the row.</p> <p></p>"},{"location":"howto/marketplace/clavister/deploy/#logging-into-the-netwall-dashboard","title":"Logging into the NetWall dashboard","text":"<p>You need to know the firewall VM\u2019s public IP address and the automatically generated password for the <code>admin</code> user. For that, make sure you are in the Provisioned Apps pane. Click on the firewall row to expand it, and select the Stack Output tab.</p> <p></p> <p>In the gateway_password row, click the icon in the Action column. A pop-up window appears, and you may click the blue Copy Output! button to copy the password to the clipboard. Close the pop-up window by clicking on the Back button.</p> <p></p> <p>Then get a pop-up window with the public IP, by clicking the icon in the Action column of the gateway_public_ip row. Again, you may click the blue Copy Output! button or, better yet, jot the IP down so you won\u2019t overwrite the password in the clipboard.</p> <p></p> <p>In your favorite web browser, navigate to <code>https://&lt;public-ip&gt;</code>. You will see a warning regarding the self-signed certificate, so make an exception and allow the browser to load the page. In the login window, type <code>admin</code> in the Username text field, and paste the password in the Password field below. Click on the blue Login button to proceed.</p> <p></p> <p>The Clavister NetWall dashboard appears. At your convenience, take the time to familiarize yourself with it and explore the features the application firewall offers.</p> <p></p> <p>By now, you might want to put a new server behind the firewall.</p>"},{"location":"howto/marketplace/grafana/delete/","title":"Deleting a Grafana instance","text":"<p>This short guide covers the deletion of a self-hosted Grafana deployment.</p> <p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Provisioned Apps. All your deployed applications, including the one you are about to delete, are listed in the central pane. Notice the orange  icon at the right of the application row. Click on it, and from the drop-down menu that appears, select Delete Provisioned App.</p> <p></p> <p>A new window named About to delete a Provisioned App appears. Make sure it refers to the Grafana deployment you wish to delete. When you are ready, click on the red Yes, Delete button.</p> <p></p> <p>After a few seconds, your Grafana deployment will be deleted, and that fact will be reflected in the list of provisioned applications.</p> <p></p>"},{"location":"howto/marketplace/grafana/deploy/","title":"Creating a Grafana instance","text":"<p>This guide covers the deployment of a self-hosted Grafana service.</p> <p>To proceed, make sure you have an account in Cleura\u00a0Cloud, and you are logged in to the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"howto/marketplace/grafana/deploy/#step-by-step-deployment","title":"Step-by-step deployment","text":"<p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Discover Apps and Services. In the central pane, you will see all available applications and services. Locate the Grafana box and click the green View button.</p> <p></p> <p>You will see the Grafana information page, where you can learn more about its features, and obtain pricing information. Click the orange Deploy this App button to start the deployment process.</p> <p></p> <p>The Grafana application is hosted on a Nova VM, so now you may select a region, a name, a flavor, a public network, a keypair, and a security group for it. Regarding the security group, make sure it includes a rule allowing incoming TCP connections to port 3000.</p> <p>Read and agree to the Terms and Conditions. When you are ready, click the green Create button.</p> <p></p> <p>The deployment takes some minutes to complete. To check how it is going, expand the Marketplace category in the vertical pane on the left and click Provisioned Apps. In the central pane, watch the Grafana Heat stack row. The animated icon at the left marks the deployment progress.</p> <p></p> <p>When the deployment is complete, you will see a white check mark in a green circle.</p> <p></p>"},{"location":"howto/marketplace/grafana/deploy/#logging-into-the-grafana-dashboard","title":"Logging into the Grafana dashboard","text":"<p>You need the administrator\u2019s predefined username and password, as well as the URL of your Grafana instance. For that, make sure you are in the Provisioned Apps pane. Click on the Grafana row to expand it, and select the Stack Output tab.</p> <p></p> <p>We recommend you create a new entry in your password manager, and populate all necessary fields with values from the corresponding output keys.</p> <p>For the preset password, click the icon in the Action column of the admin_password row. A pop-up window appears. Click the blue Copy Output! button to copy the password into the clipboard. When ready, click the Back button to close the window.</p> <p></p> <p>Similarly, get the administrator\u2019s username from the admin_username row.</p> <p></p> <p>And finally, get your Grafana deployment\u2019s URL from the grafana_url row.</p> <p></p> <p>Using your favorite web browser, navigate to your Grafana deployment\u2019s URL. The Grafana login page appears. Use the default username and password, and click the Log in button.</p> <p></p> <p>The Grafana landing page appears.</p> <p></p> <p>Start from the official Grafana documentation page to learn how to use your new data visualization service.</p>"},{"location":"howto/marketplace/harbor/delete/","title":"Deleting a Harbor instance","text":"<p>This short guide covers the deletion of a self-hosted Harbor deployment.</p> <p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Provisioned Apps. All your deployed applications, including the one you are about to delete, are listed in the central pane. Notice the orange  icon at the right of the application row. Click on it, and from the drop-down menu that appears, select Delete Provisioned App.</p> <p></p> <p>A new window named About to delete a Provisioned App appears. Make sure it refers to the Harbor deployment you wish to delete. When you are ready, click on the red Yes, Delete button.</p> <p></p> <p>After a few seconds, your Harbor deployment will be deleted, and that fact will be reflected in the list of provisioned applications.</p> <p></p>"},{"location":"howto/marketplace/harbor/deploy/","title":"Creating a Harbor instance","text":"<p>This guide covers the deployment of a self-hosted Harbor service.</p> <p>To proceed, make sure you have an account in Cleura\u00a0Cloud, and you are logged in to the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"howto/marketplace/harbor/deploy/#step-by-step-deployment","title":"Step-by-step deployment","text":"<p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Discover Apps and Services. In the central pane, you will see all available applications and services. Locate the Harbor box and click the green View button.</p> <p></p> <p>You will see the Harbor information page, where you can learn more about its features, and obtain pricing information. Click the orange Deploy this App button to start the deployment process.</p> <p></p> <p>The Harbor application is hosted on a Nova VM, so now you may select a region, a name, a flavor, a public network, a keypair, and a security group for it. Regarding the security group, make sure it includes a rule allowing incoming TCP connections to port 80.</p> <p>Read and agree to the Terms and Conditions. When you are ready, click the green Create button.</p> <p></p> <p>The deployment takes some minutes to complete. To check how it is going, expand the Marketplace category in the vertical pane on the left and click Provisioned Apps. In the central pane, watch the Harbor Heat stack row. The animated icon at the left marks the deployment progress.</p> <p></p> <p>When the deployment is complete, you will see a white check mark in a green circle.</p> <p></p>"},{"location":"howto/marketplace/harbor/deploy/#logging-into-the-harbor-dashboard","title":"Logging into the Harbor dashboard","text":"<p>You must know Harbor\u2019s public IP address and the password automatically generated for the <code>admin</code> user. For that, make sure you are in the Provisioned Apps pane. Click on the Harbor row to expand it, and select the Stack Output tab.</p> <p></p> <p>In the admin_credentials row, click the icon in the Action column. A pop-up window appears. Click the blue Copy Output! button to copy the content of the Output box to the clipboard. Paste that into a new text editor window, but don\u2019t save it in a new file. Instead, we recommend you create a new entry in your password manager of choice and move the username and the password there. Close the pop-up window by clicking on the Back button.</p> <p></p> <p>Next, get a pop-up window revealing the particular Harbor deployment\u2019s public IP address. Click the icon in the Action column of the harbor_url row, then click the blue Copy Output! button. Use the IP address to create a URL of the form <code>http://&lt;public-ip&gt;</code>, and put that URL into the Harbor password manager entry.</p> <p></p> <p>Using your favorite web browser, navigate to <code>http://&lt;public-ip&gt;</code>. The Harbor login page appears. In the left vertical pane, type in the user name (<code>admin</code>), paste the password from your password manager, and click the LOG IN button below.</p> <p></p> <p>You are directed to the Harbor main page.</p> <p></p> <p>On a separate browser window or tab, navigate to the Harbor documentation page to learn how to start using your new self-hosted container registry.</p>"},{"location":"howto/marketplace/keycloak/delete/","title":"Deleting a Keycloak instance","text":"<p>This short guide covers the deletion of a self-hosted Keycloak deployment.</p> <p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Provisioned Apps. All your deployed applications, including the one you are about to delete, are listed in the central pane. Notice the orange  icon at the right of the application row. Click on it, and from the drop-down menu that appears, select Delete Provisioned App.</p> <p></p> <p>A new window named About to delete a Provisioned App appears. Make sure it refers to the Keycloak deployment you wish to delete. When you are ready, click on the red Yes, Delete button.</p> <p></p> <p>After a few seconds, your Keycloak deployment will be deleted, and that fact will be reflected in the list of provisioned applications.</p> <p></p>"},{"location":"howto/marketplace/keycloak/deploy/","title":"Creating a Keycloak instance","text":"<p>This guide covers the deployment of a self-hosted Keycloak service.</p> <p>To proceed, make sure you have an account in Cleura\u00a0Cloud, and you are logged in to the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"howto/marketplace/keycloak/deploy/#step-by-step-deployment","title":"Step-by-step deployment","text":"<p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Discover Apps and Services. In the central pane, you will see all available applications and services. Locate the Keycloak box and click the green View button.</p> <p></p> <p>You will see the Keycloak information page, where you can learn more about its features, and obtain pricing information. Click the orange Deploy this App button to start the deployment process.</p> <p></p> <p>The Keycloak application is hosted on a Nova VM, so now you may select a region, a name, a flavor, a public network, a keypair, and a security group for it. Regarding the security group, make sure it includes a rule allowing incoming TCP connections to port 8080.</p> <p>Optionally, select one of the available Keycloak versions. Then, read and agree to the Terms and Conditions. When you are ready, click the green Create button.</p> <p></p> <p>The deployment takes some minutes to complete. To check how it is going, expand the Marketplace category in the vertical pane on the left and click Provisioned Apps. In the central pane, watch the Keycloak Heat stack row. The animated icon at the left marks the deployment progress.</p> <p></p> <p>When the deployment is complete, you will see a white check mark in a green circle.</p> <p></p>"},{"location":"howto/marketplace/keycloak/deploy/#logging-into-the-keycloak-dashboard","title":"Logging into the Keycloak dashboard","text":"<p>You need the administrator\u2019s (<code>admin</code>) predefined password and the URL of your Keycloak instance. For that, make sure you are in the Provisioned Apps pane. Click on the Keycloak row to expand it, and select the Stack Output tab.</p> <p></p> <p>We recommend you create a new entry in your password manager and populate all necessary fields with values from the corresponding output keys.</p> <p>For the preset password, click the icon in the Action column of the admin_credentials row. A pop-up window appears. Click the blue Copy Output! button to copy all data below Output into the clipboard. Temporarily paste that data into a text editor, and use the password for your password manager entry. When ready, click the Back button to close the window.</p> <p></p> <p>Similarly, get the URL to your Keycloak instance from the keycloak_url row.</p> <p></p> <p>Using your favorite web browser, navigate to your Keycloak deployment\u2019s URL. The Keycloak Sign-in page appears. Use the default username (<code>admin</code>) and password from your password manager, and click the Sign in button.</p> <p></p> <p>The Keycloak welcome page appears.</p> <p></p> <p>We recommend you start with the official Keycloak guides page to learn how to use your new identity and access management service.</p>"},{"location":"howto/marketplace/matomo/delete/","title":"Deleting a Matomo instance","text":"<p>This short guide covers the deletion of a self-hosted Matomo deployment.</p> <p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Provisioned Apps. All your deployed applications, including the one you are about to delete, are listed in the central pane. Notice the orange  icon at the right of the application row. Click on it, and from the drop-down menu that appears, select Delete Provisioned App.</p> <p></p> <p>A new window named About to delete a Provisioned App appears. Make sure it refers to the Matomo deployment you wish to delete. When you are ready, click on the red Yes, Delete button.</p> <p></p> <p>After a few seconds, your Matomo deployment will be deleted, and that fact will be reflected in the list of provisioned applications.</p> <p></p>"},{"location":"howto/marketplace/matomo/deploy/","title":"Creating a Matomo instance","text":"<p>This guide covers the deployment of a self-hosted Matomo service.</p> <p>To proceed, make sure you have an account in Cleura\u00a0Cloud, and you are logged in to the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"howto/marketplace/matomo/deploy/#step-by-step-deployment","title":"Step-by-step deployment","text":"<p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Discover Apps and Services. In the central pane, you will see all available applications and services. Locate the Matomo box and click the green View button.</p> <p></p> <p>You will see the Matomo information page, where you can learn more about its features, and obtain pricing information. Click the orange Deploy this App button to start the deployment process.</p> <p></p> <p>The Matomo application is hosted on a Nova VM, so now you may select a region, a name, a flavor, a public network, a keypair, and a security group for it. Regarding the security group, make sure it includes a rule allowing incoming TCP connections to port 80.</p> <p>Read and agree to the Terms and Conditions. When you are ready, click the green Create button.</p> <p></p> <p>The deployment takes some minutes to complete. To check how it is going, expand the Marketplace category in the vertical pane on the left and click Provisioned Apps. In the central pane, watch the Matomo Heat stack row. The animated icon at the left marks the deployment progress.</p> <p></p> <p>When the deployment is complete, you will see a white check mark in a green circle.</p> <p></p>"},{"location":"howto/marketplace/matomo/deploy/#logging-into-the-matomo-dashboard","title":"Logging into the Matomo dashboard","text":"<p>Next, you need the URL of your Matomo instance. For that, make sure you are in the Provisioned Apps pane. Click on the Matomo row to expand it, and select the Stack Output tab.</p> <p></p> <p>Get a pop-up window revealing the URL of the particular Matomo deployment. Click the icon in the Action column of the matomo_url row, then click the blue Copy Output! button. Close the pop-up window by clicking on the Back button.</p> <p></p> <p>Using your favorite web browser, navigate to your Matomo deployment\u2019s URL. The Matomo welcome page appears.</p> <p></p> <p>To configure your new traffic analytics service, and to learn how to use it, start with the Matomo Help Centre.</p> <p>Keep in mind that while configuring Matomo, you will need pieces of information provided in the application\u2019s Stack Output tab.</p>"},{"location":"howto/marketplace/prometheus/delete/","title":"Deleting a Prometheus instance","text":"<p>This short guide covers the deletion of a self-hosted Prometheus deployment.</p> <p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Provisioned Apps. All your deployed applications, including the one you are about to delete, are listed in the central pane. Notice the orange  icon at the right of the application row. Click on it, and from the drop-down menu that appears, select Delete Provisioned App.</p> <p></p> <p>A new window named About to delete a Provisioned App appears. Make sure it refers to the Prometheus deployment you wish to delete. When you are ready, click on the red Yes, Delete button.</p> <p></p> <p>After a few seconds, your Prometheus deployment will be deleted, and that fact will be reflected in the list of provisioned applications.</p> <p></p>"},{"location":"howto/marketplace/prometheus/deploy/","title":"Creating a Prometheus instance","text":"<p>This guide covers the deployment of a self-hosted Prometheus service.</p> <p>To proceed, make sure you have an account in Cleura\u00a0Cloud, and you are logged in to the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"howto/marketplace/prometheus/deploy/#step-by-step-deployment","title":"Step-by-step deployment","text":"<p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Discover Apps and Services. In the central pane, you will see all available applications and services. Locate the Prometheus box and click the green View button.</p> <p></p> <p>You will see the Prometheus information page, where you can learn more about its features, and obtain pricing information. Click the orange Deploy this App button to start the deployment process.</p> <p></p> <p>The Prometheus application is hosted on a Nova VM, so now you may select a region, a name, a flavor, a public network, a keypair, and a security group for it. Regarding the security group, make sure it includes a rule allowing incoming TCP connections to port 9090.</p> <p>Read and agree to the Terms and Conditions. When you are ready, click the green Create button.</p> <p></p> <p>The deployment takes some minutes to complete. To check how it is going, expand the Marketplace category in the vertical pane on the left and click Provisioned Apps. In the central pane, watch the Prometheus Heat stack row. The animated icon at the left marks the deployment progress.</p> <p></p> <p>When the deployment is complete, you will see a white check mark in a green circle.</p> <p></p>"},{"location":"howto/marketplace/prometheus/deploy/#logging-into-the-prometheus-dashboard","title":"Logging into the Prometheus dashboard","text":"<p>You need your deployment\u2019s URL. For that, make sure you are in the Provisioned Apps pane. Click on the Prometheus row to expand it, and select the Stack Output tab.</p> <p></p> <p>Click the icon at the right of the prometheus_url row. A pop-up window appears. Click the blue Copy Output! button to copy the URL into the clipboard.</p> <p>Using your favorite web browser, navigate to your Prometheus deployment\u2019s URL. The Prometheus page appears.</p> <p></p> <p>We recommend you start with the official Getting Started page to learn how to use your new monitoring service.</p>"},{"location":"howto/marketplace/taiga/delete/","title":"Deleting a Taiga instance","text":"<p>This short guide covers the deletion of a self-hosted Taiga deployment.</p> <p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Provisioned Apps. All your deployed applications, including the one you are about to delete, are listed in the central pane. Notice the orange  icon at the right of the application row. Click on it, and from the drop-down menu that appears, select Delete Provisioned App.</p> <p></p> <p>A new window named About to delete a Provisioned App appears. Make sure it refers to the Taiga deployment you wish to delete. When you are ready, click on the red Yes, Delete button.</p> <p></p> <p>After a few seconds, your Taiga deployment will be deleted, and that fact will be reflected in the list of provisioned applications.</p> <p></p>"},{"location":"howto/marketplace/taiga/deploy/","title":"Creating a Taiga instance","text":"<p>This guide covers the deployment of a self-hosted Taiga service.</p> <p>To proceed, make sure you have an account in Cleura\u00a0Cloud, and you are logged in to the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"howto/marketplace/taiga/deploy/#step-by-step-deployment","title":"Step-by-step deployment","text":"<p>In the left vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Marketplace category and click on Discover Apps and Services. In the central pane, you will see all available applications and services. Locate the Taiga box and click the green View button.</p> <p></p> <p>You will see the Taiga information page, where you can learn more about its features, and obtain pricing information. Click the orange Deploy this App button to start the deployment process.</p> <p></p> <p>The Taiga application is hosted on a Nova VM, so now you may select a region, a name, a flavor, a public network, a keypair, and a security group for it. Regarding the security group, make sure it includes a rule allowing incoming TCP connections to port 9000.</p> <p>Read and agree to the Terms and Conditions. When you are ready, click the green Create button.</p> <p></p> <p>The deployment takes some minutes to complete. To check how it is going, expand the Marketplace category in the vertical pane on the left and click Provisioned Apps. In the central pane, watch the Taiga Heat stack row. The animated icon at the left marks the deployment progress.</p> <p></p> <p>When the deployment is complete, you will see a white check mark in a green circle.</p> <p></p>"},{"location":"howto/marketplace/taiga/deploy/#logging-into-the-taiga-dashboard","title":"Logging into the Taiga dashboard","text":"<p>You have to know Taiga\u2019s URL and the password automatically generated for the <code>admin</code> user. For that, make sure you are in the Provisioned Apps pane. Click on the Taiga row to expand it, and select the Stack Output tab.</p> <p></p> <p>In the superuser_password row, click the icon in the Action column. A pop-up window appears. Click the blue Copy Output! button to copy the password displayed in the Output box to the clipboard. Then, paste that password into a new text editor window, but don\u2019t save it in a new file. Instead, we recommend you create a new entry in your password manager of choice and move the password there. Close the pop-up window by clicking on the Back button.</p> <p></p> <p>In the superuser_username row, click the icon in the Action column. You will see that the user name in question is <code>admin</code>, so you might want to put that in the corresponding password manager entry.</p> <p>Next, get a pop-up window revealing the particular Taiga deployment URL. Click the icon in the Action column of the taiga_url row, then click the blue Copy Output! button. Paste the URL into that password manager entry you just created.</p> <p></p> <p>Using your favorite web browser, navigate to the URL you just retrieved. The Taiga dashboard loads. Notice the Login link and click on it.</p> <p></p> <p>You are directed to the Taiga login page. Type in the user name (<code>admin</code>), paste the password from your password manager, and click on the LOGIN button below.</p> <p></p> <p>You now see the main Taiga page.</p> <p></p> <p>On a separate browser window or tab, navigate to the Taiga basics community page to learn how you can start using your new self-hosted project management service.</p>"},{"location":"howto/object-storage/s3/","title":"S3 API","text":"<p>S3 is an object-access API based on HTTP and HTTPS.</p> <p>In Cleura\u00a0Cloud, you interact with the S3 API using either the <code>s3cmd</code> command-line interface (CLI), the MinIO client CLI (<code>mc</code>), or the standard <code>aws</code> CLI.</p> <p>Either way, in addition to installing and configuring the Python <code>openstackclient</code> module, you need to install one of the aforementioned utilities.</p> Debian/UbuntuMac OS X with HomebrewPython Package <pre><code>apt install s3cmd aws\n</code></pre> <pre><code>brew install minio-mc s3cmd\n</code></pre> <pre><code>pip install s3cmd\n</code></pre>"},{"location":"howto/object-storage/s3/#availability","title":"Availability","text":"<p>The S3 API is available in select Cleura\u00a0Cloud regions. Refer to the feature support matrix for details on S3 API availability.</p>"},{"location":"howto/object-storage/s3/credentials/","title":"Working with S3-compatible credentials","text":"<p>When you want to interact with object storage in Cleura\u00a0Cloud using tools that support an Amazon S3 compatible API (such as <code>s3cmd</code>, <code>rclone</code>, the <code>aws</code> CLI, or the Python <code>boto3</code> library), you need an S3-compatible access key ID and secret key.</p>"},{"location":"howto/object-storage/s3/credentials/#creating-credentials","title":"Creating credentials","text":"<p>You can create a set of S3-compatible credentials with the following command:</p> <pre><code>openstack ec2 credentials create\n</code></pre> <p>This will return an <code>Access</code> and <code>Secret</code> key that you can use to populate the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables (or whichever configuration options your application requires).</p> <p>Your S3-compatible credentials are always scoped to your Cleura\u00a0Cloud region and project. You cannot reuse an access and secret key across multiple regions or projects.</p> <p>Also, your credentials are only \u201cS3-compatible\u201d in the sense that they use the same format as AWS S3 does. They are never valid against AWS S3 itself.</p>"},{"location":"howto/object-storage/s3/credentials/#listing-credentials","title":"Listing credentials","text":"<p>You can list any previously-created credentials with:</p> <pre><code>openstack ec2 credentials list\n</code></pre>"},{"location":"howto/object-storage/s3/credentials/#configuring-your-s3-api-client","title":"Configuring your S3 API client","text":"<p>Once you have obtained your S3-compatible access and secret key, you need to configure your S3 client with it.</p> <p>How exactly you do that depends on your preferred client:</p> awsmcs3cmdrclone <p>The AWS CLI comes in two major versions with different features and configuration methods.</p> <p>Configuring AWS CLI v1</p> <p>Install the <code>endpoint</code> plugin: <pre><code>pip install awscli-plugin-endpoint\n</code></pre> Then, enable the plugin in your configuration. Open the <code>aws</code> configuration file (normally <code>~/.aws/config</code>), and add or modify the <code>[plugins]</code> section: <pre><code>[plugins]\nendpoint = awscli_plugin_endpoint\n</code></pre> Now, create a new profile, named after your Cleura\u00a0Cloud region. Set the credentials and endpoint URLs, using the following commands: <pre><code>aws configure set \\\n  --profile &lt;region&gt; \\\n  aws_access_key_id &lt;access-key&gt;\naws configure set \\\n  --profile &lt;region&gt; \\\n  aws_secret_access_key &lt;secret-key&gt;\naws configure set \\\n  --profile &lt;region&gt; \\\n  s3.endpoint_url https://s3-&lt;region&gt;.citycloud.com\naws configure set \\\n  --profile &lt;region&gt; \\\n  s3api.endpoint_url https://s3-&lt;region&gt;.citycloud.com\n</code></pre></p> <p>Configuring AWS CLI v2</p> <p>AWS CLI v2 supports custom endpoints natively without plugins. You can configure it using command-line options and configuration file. Create a new profile with credentials: <pre><code>aws configure set \\\n  --profile &lt;region&gt; \\\n  aws_access_key_id &lt;access-key&gt;\naws configure set \\\n  --profile &lt;region&gt; \\\n  aws_secret_access_key &lt;secret-key&gt;\n</code></pre> Then configure the endpoint URL using configuration file. Edit <code>~/.aws/config</code> and add: <pre><code>[profile &lt;region&gt;]\nservices = &lt;region&gt;-services\n[services &lt;region&gt;-services]\ns3 = \n  endpoint_url = https://s3-&lt;region&gt;.citycloud.com\ns3api = \n  endpoint_url = https://s3-&lt;region&gt;.citycloud.com\n</code></pre></p> <p>Create a new alias, named after your Cleura\u00a0Cloud region: <pre><code>mc alias set &lt;region&gt; \\\n  https://s3-&lt;region&gt;.citycloud.com \\\n  &lt;access-key&gt; &lt;secret-key&gt;\n</code></pre> Once you have configured an alias like this, you are able to run bucket operations with <code>mc</code> using the <code>alias/bucket</code> syntax.</p> <p><code>s3cmd</code> does not support configuration profiles, so you need to use a separate configuration file for each Cleura\u00a0Cloud region you want to use: <pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; --configure\n</code></pre></p> <ul> <li>Set your <code>Access Key</code> and <code>Secret Key</code> when prompted.</li> <li>Leave <code>Default Region</code> unchanged.</li> <li>Set <code>S3 Endpoint</code> to <code>s3-&lt;region&gt;.citycloud.com</code>.</li> <li>Set <code>DNS-style bucket+hostname:port template for accessing a bucket</code> to <code>s3-&lt;region&gt;.citycloud.com</code> as well.</li> <li>Set <code>Use HTTPS protocol</code> to <code>Yes</code> (the default).</li> <li>Configure GnuPG encryption and your HTTP proxy server, if needed.</li> <li>Test access with your supplied credentials.</li> </ul> <p>On subsequent invocations of the <code>s3cmd</code> CLI, always add the <code>-c ~/.s3cfg-&lt;region&gt;</code> option.</p> <p>Create or edit the configuration file named <code>~/.rclone.conf</code>, and insert a section named after your Cleura\u00a0Cloud region. That section should contain the following content: <pre><code>[&lt;region&gt;]\ntype = s3\nprovider = Ceph\nenv_auth = false\naccess_key_id = &lt;access key id&gt;\nsecret_access_key = &lt;secret key&gt;\nendpoint = &lt;region&gt;.citycloud.com\nacl = private\n</code></pre></p>"},{"location":"howto/object-storage/s3/credentials/#deleting-credentials","title":"Deleting credentials","text":"<p>If at any time you need to delete a set of AWS-compatible credentials, you can do so with the following command:</p> <pre><code>openstack ec2 credentials delete &lt;access-key-id&gt;\n</code></pre> <p>Deleting a set of S3-compatible credentials will immediately revoke access for any applications that were using it.</p>"},{"location":"howto/object-storage/s3/expiry/","title":"Object expiry","text":"<p>Object expiry requires that you configure your environment with working S3-compatible credentials.</p> <p>You can set a bucket\u2019s lifecycle configuration such that it automatically deletes objects after a certain number of days.</p>"},{"location":"howto/object-storage/s3/expiry/#enabling-object-expiry","title":"Enabling object expiry","text":"<p>First, you need to create a JSON file, <code>lifecycle.json</code>, that contains the lifecycle configuration rule. Be sure to set <code>Days</code> to your desired value:</p> <pre><code>{\n  \"Rules\": [{\n    \"ID\": \"cleanup\",\n    \"Status\": \"Enabled\",\n    \"Prefix\": \"\",\n    \"Expiration\": {\n        \"Days\": 5\n    }\n  }]\n}\n</code></pre> <p>Then, apply this lifecycle configuration to your bucket using one of the following commands:</p> awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3api put-bucket-lifecycle-configuration \\\n  --lifecycle-configuration file://lifecycle.json \\\n  --bucket &lt;bucket-name&gt;\n</code></pre> <pre><code>mc ilm import &lt;region&gt;/&lt;bucket-name&gt; &lt; lifecycle.json\n</code></pre> <pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; setlifecycle lifecycle.json s3://&lt;bucket-name&gt;\n</code></pre>"},{"location":"howto/object-storage/s3/expiry/#removing-object-expiry","title":"Removing object expiry","text":"<p>At some point, you might want to remove the object expiry functionality configuration from a bucket, so that objects in it no longer auto-delete after a period.</p> awsmcs3cmd <p>With the <code>aws s3api</code> command, you can remove the lifecycle configuration from a bucket: <pre><code>aws --profile &lt;region&gt; \\\n  s3api delete-bucket-lifecycle \\\n  --bucket &lt;bucket-name&gt;\n</code></pre></p> <p>With <code>mc</code>, you are able to remove just an individual bucket lifecycle rule. Assuming your rule uses the ID <code>cleanup</code>, here is how you remove it: <pre><code>mc ilm rm --id \"cleanup\" &lt;region&gt;/&lt;bucket-name&gt;\n</code></pre></p> <p>With <code>s3cmd</code>, you can remove the lifecycle configuration from a bucket: <pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; dellifecycle s3://&lt;bucket-name&gt;\n</code></pre></p>"},{"location":"howto/object-storage/s3/object-lock/","title":"Object lock","text":"<p>Object lock is a feature in S3 that enables you to lock your objects to prevent them from being deleted or overwritten. This feature can be helpful when you want to maintain data integrity, comply with regulations, or retain data for a specific period.</p> <p>Note that this feature is distinct from object expiry, which is covered in a separate guide.</p> <p>You should also keep in mind that object lock offers two methods for managing object retention: Retention periods and Legal hold.</p>"},{"location":"howto/object-storage/s3/object-lock/#prerequisites","title":"Prerequisites","text":"<p>In order to manage object lock, you must install <code>aws</code> or <code>mc</code>, and configure it with working credentials.</p> <p>You cannot (reliably) use <code>s3cmd</code> to manage this functionality.</p>"},{"location":"howto/object-storage/s3/object-lock/#enabling-object-lock","title":"Enabling object lock","text":"<p>To use the object lock feature, you must first create a new bucket.</p> <p>Existing buckets cannot have object lock enabled. To enable object lock on existing data, you must first create a new bucket with object lock, and then sync your pre-existing objects into that bucket.</p> awsmc <pre><code>aws --profile &lt;region&gt; \\\n  s3api create-bucket \\\n  --bucket &lt;bucket-name&gt; \\\n  --object-lock-enabled-for-bucket\n</code></pre> <pre><code>mc mb &lt;region&gt;/&lt;bucket&gt; \\\n  --with-lock\n</code></pre> <p>Note that this will enable bucket versioning as well.</p>"},{"location":"howto/object-storage/s3/object-lock/#configure-the-default-object-lock-mode-and-retention-period-for-a-bucket","title":"Configure the default object lock mode and retention period for a bucket","text":"<p>To configure your bucket to use the Compliance mode, use one of the following commands:</p> awsmc <pre><code>aws --profile &lt;region&gt; \\\n  s3api put-object-lock-configuration \\\n  --bucket &lt;bucket-name&gt; \\\n  --object-lock-configuration \\\n  '{ \"ObjectLockEnabled\": \"Enabled\", \"Rule\": { \"DefaultRetention\": { \"Mode\": \"COMPLIANCE\", \"Days\": 30 }}}'\n</code></pre> <pre><code>mc retention set \\\n  --default compliance 30d \\\n  &lt;region&gt;/&lt;bucket&gt;\n</code></pre> <p>The <code>--default</code> parameter sets the default object lock settings for new objects, and is optional.</p> <p>To specify a duration, use a string formatted as <code>Nd</code> for days or <code>Ny</code> for years. For example, use <code>30d</code> to indicate 30 days after the object creation, or use <code>1y</code> to indicate 1 year after the object creation.</p>"},{"location":"howto/object-storage/s3/object-lock/#configure-the-object-lock-mode-and-retention-period-for-a-single-object","title":"Configure the object lock mode and retention period for a single object","text":"<p>If you want to set a specific retention period for an object, instead of using the default retention period, use one of the following commands. You can also use these commands to update the retention period for an object:</p> awsmc <pre><code>aws --profile &lt;region&gt; \\\n  s3api put-object-retention \\\n  --bucket &lt;bucket-name&gt; \\\n  --key &lt;object-name&gt; \\\n  --retention '{ \"Mode\": \"COMPLIANCE\", \"RetainUntilDate\": \"2023-01-01T12:00:00.00Z\" }'\n</code></pre> <p>For the value of the <code>RetainUntilDate</code> parameter, use the ISO\u00a08601 date-time representation format.</p> <pre><code>mc retention set \\\n  COMPLIANCE 30d \\\n  &lt;region&gt;/&lt;bucket&gt;/&lt;object-name&gt;\n</code></pre> <p>To specify a duration, use a string formatted as <code>Nd</code> for days or <code>Ny</code> for years. For example, use <code>30d</code> to indicate 30 days after the object creation, or use <code>1y</code> to indicate 1 year after the object creation.</p>"},{"location":"howto/object-storage/s3/object-lock/#retrieve-the-object-lock-mode-and-retention-period","title":"Retrieve the object lock mode and retention period","text":""},{"location":"howto/object-storage/s3/object-lock/#bucket-level","title":"Bucket-level","text":"<p>To view the default object lock mode and retention period set on a bucket, use the following command:</p> awsmc <p><pre><code>aws --profile &lt;region&gt; \\\n  s3api get-object-lock-configuration \\\n  --bucket &lt;bucket-name&gt;\n</code></pre> Example output: <pre><code>{\n  \"ObjectLockConfiguration\": {\n    \"ObjectLockEnabled\": \"Enabled\",\n    \"Rule\": {\n      \"DefaultRetention\": {\n        \"Mode\": \"COMPLIANCE\",\n        \"Days\": 2\n      }\n    }\n  }\n}\n</code></pre></p> <p><pre><code>mc retention info --json --default &lt;region&gt;/&lt;bucket&gt;\n</code></pre> Example output: <pre><code>{\n  \"op\": \"info\",\n  \"enabled\": \"Enabled\",\n  \"mode\": \"COMPLIANCE\",\n  \"validity\": \"2DAYS\",\n  \"status\": \"success\"\n}\n</code></pre></p>"},{"location":"howto/object-storage/s3/object-lock/#object-level","title":"Object-level","text":"<p>To view the default object lock mode and retention period set on an object, use the following command:</p> awsmc <p><pre><code>aws --profile &lt;region&gt; \\\n  s3api get-object-retention \\\n  --bucket &lt;bucket-name&gt; \\\n  --key &lt;object-name&gt;\n</code></pre> Example output: <pre><code>{\n  \"Retention\": {\n    \"Mode\": \"COMPLIANCE\",\n    \"RetainUntilDate\": \"2023-02-25T20:04:23.383915+00:00\"\n  }\n}\n</code></pre></p> <p>For the value of the <code>RetainUntilDate</code> parameter, use the ISO\u00a08601 date-time representation format.</p> <p><pre><code>mc retention info --json &lt;region&gt;/&lt;bucket&gt;/&lt;object-name&gt;\n</code></pre> Example output: <pre><code>{\n  \"mode\": \"COMPLIANCE\",\n  \"until\": \"2023-02-25T20:04:23.383915502Z\",\n  \"urlpath\": \"region/bucket/object\",\n  \"versionID\": \"\",\n  \"status\": \"success\",\n  \"error\": null\n}\n</code></pre></p>"},{"location":"howto/object-storage/s3/object-lock/#configure-the-object-lock-legal-hold-for-an-object","title":"Configure the object lock legal hold for an object","text":"<p>Legal hold requires that the specified bucket has object locking enabled.</p>"},{"location":"howto/object-storage/s3/object-lock/#per-bucket","title":"Per bucket","text":"<p>To configure the legal hold for all objects in a bucket, use the following command:</p> awsmc <p>The <code>aws s3api</code> command can only set the legal hold for a single object at a time. However, you can use the <code>ls</code> command along with <code>--recursive</code> to list all objects in a bucket, and then set the legal hold for each object in your bucket.</p> <pre><code>aws --profile &lt;region&gt; \\\n  s3api list-objects \\\n  --bucket &lt;bucket-name \\\n  | jq .Contents[].Key \\\n  | xargs -n1 aws --profile &lt;region&gt; s3api put-object-legal-hold --legal-hold Status=ON --bucket &lt;bucket-name&gt; --key\n</code></pre> <pre><code>mc legalhold set \\\n  --recursive \\\n  &lt;region&gt;/&lt;bucket&gt;\n</code></pre>"},{"location":"howto/object-storage/s3/object-lock/#per-object","title":"Per object","text":"<p>To configure the legal hold for a single object, use the following command:</p> awsmc <pre><code>aws --profile &lt;region&gt; \\\n  s3api put-object-legal-hold \\\n  --bucket &lt;bucket-name&gt; \\\n  --key &lt;object-name&gt; \\\n  --version-id &lt;version-id&gt; \\\n  --legal-hold Status=ON\n</code></pre> <p>Note that if you don\u2019t specify a version ID, the legal hold will be applied to the latest version of the object.</p> <pre><code>mc legalhold set \\\n  &lt;region&gt;/&lt;bucket&gt;/&lt;object-name&gt;\n</code></pre> <p>To remove the legal hold, use the <code>clear</code> command instead of <code>set</code>.</p>"},{"location":"howto/object-storage/s3/object-lock/#retrieve-the-legal-hold-status-for-an-object","title":"Retrieve the legal hold status for an object","text":"<p>To display the legal hold status for an object, use the following command:</p> awsmc <p><pre><code>aws --profile &lt;region&gt; \\\n  s3api get-object-legal-hold \\\n  --bucket &lt;bucket-name&gt; \\\n  --key &lt;object-name&gt;\n</code></pre> Example output: <pre><code>{\n  \"LegalHold\": {\n    \"Status\": \"ON\"\n  }\n}\n</code></pre></p> <p><pre><code>mc legalhold info \\\n  --json &lt;region&gt;/&lt;bucket&gt;/&lt;object-name&gt;\n</code></pre> Example output: <pre><code>{\n  \"legalhold\": \"ON\",\n  \"urlpath\": \"https://s3-&lt;region&gt;.citycloud.com/&lt;bucket&gt;/&lt;object-name&gt;\",\n  \"key\": \"&lt;object-name&gt;\",\n  \"versionID\": \"\",\n  \"status\": \"success\"\n}\n</code></pre></p>"},{"location":"howto/object-storage/s3/presign/","title":"Pre-signed object URLs","text":"<p>You can use the S3 API to temporarily allow access to individual objects without authentication from a browser, using pre-signed URLs.</p> <p>This approach is distinct from enabling permanent anonymous access to objects in a bucket. If you are planning to use pre-signed URLs in a bucket, then that bucket should not have a public read policy.</p> <p>Pre-signed URLs are typically found in web applications using a Software Development Kit (SDK) to interact with the S3 API. An example of such an SDK would be Boto3 for Python applications.</p> <p>This how-to guide instead uses command-line utilities to illustrate the concept, and to facilitate testing pre-signed URLs in Cleura\u00a0Cloud.</p>"},{"location":"howto/object-storage/s3/presign/#prerequisites","title":"Prerequisites","text":"<p>The use of pre-signed URLs requires that you configure your environment with working S3-compatible credentials.</p>"},{"location":"howto/object-storage/s3/presign/#creating-a-pre-signed-url","title":"Creating a pre-signed URL","text":"<p>To create a pre-signed URL for an object, you use the following command (replace <code>&lt;seconds&gt;</code> with the number of seconds you want the pre-signed URL to be valid):</p> awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3 presign \\\n  --expires-in &lt;seconds&gt;\n  s3://&lt;bucket-name&gt;/&lt;object-name&gt;\n</code></pre> <p><pre><code>mc share download --expire &lt;seconds&gt;s &lt;region&gt;/&lt;bucket-name&gt;\n</code></pre> Note the <code>s</code> suffix for the <code>--expire</code> option. <code>mc</code> also supports the suffix <code>m</code> for minutes, <code>h</code> for hours, and <code>d</code> for days.</p> <p>If you do not set <code>--expire</code>, <code>mc</code> defaults to creating a pre-signed URL that is valid for 7 days.</p> <p>For pre-signed URLs generated with <code>s3cmd</code> to work correctly in Cleura\u00a0Cloud, you must use an <code>s3cmd</code> version later than 2.0.1, and set <code>signurl_use_https\u00a0=\u00a0True</code> in your configuration file.</p> <p><pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; signurl s3://&lt;bucket-name&gt;/&lt;object-name&gt; +&lt;seconds&gt;\n</code></pre> Note the <code>+</code> prefix on the expiry. <code>s3cmd</code> also supports setting an absolute expiry date, which does not use the <code>+</code> prefix and must be formatted in Unix time.</p> <p>The command will return the valid pre-signed URL.</p>"},{"location":"howto/object-storage/s3/presign/#accessing-objects-with-a-pre-signed-url","title":"Accessing objects with a pre-signed URL","text":"<p>To access an object in a public bucket from a web browser or a generic HTTP/HTTPS client like <code>curl</code>, open the URL that the pre-sign command returned:</p> <pre><code>curl -f -O https://s3-&lt;region&gt;.citycloud.com/&lt;bucket-name&gt;/&lt;object-name&gt;?AWSAccessKeyId=&lt;access-key&gt;&amp;Signature=&lt;signature&gt;&amp;Expires=&lt;expiry&gt;\n</code></pre> <p>As long as the query parameters are correct and the signature has not yet expired, this command will succeed. If the query parameters are incorrect or the pre-signed URL is past its expiry date, it will fail with HTTP 403 (Forbidden) instead.</p> <p>Pre-signed URLs are valid for HTTP <code>GET</code> requests only. Thus, even a valid pre-signed URL will result in HTTP 403 if you force a different HTTP method (such as <code>HEAD</code>, by setting <code>curl\u00a0-I</code>).</p> <p>For example, to retrieve an object named <code>bar.pdf</code> in a bucket named <code>foo</code> in the Cleura\u00a0Cloud Kna1 region via its pre-signed URL, you would run:</p> <pre><code>$ curl -o bar.pdf https://s3-kna1.citycloud.com/foo/bar.pdf?AWSAccessKeyId=07576783684248f7b2745e34356c6025&amp;Expires=1673521496&amp;Signature=%2Frm9nLV3moP%2FQz7aGCAnrESXjbk%3D\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 62703  100 62703    0     0   186k      0 --:--:-- --:--:-- --:--:--  186k\n</code></pre>"},{"location":"howto/object-storage/s3/presign/#setting-the-download-filename-on-the-object","title":"Setting the download filename on the object","text":"<p>When you use <code>curl</code> (and potentially other browsers or HTTP/HTTPS clients) to download an object from a pre-signed URL, the default behavior is to set its filename to the name of the downloaded object including the query parameters. This can lead to rather unwieldly filenames.</p> <p>To ensure that an object named <code>bar.pdf</code> in a bucket named <code>foo</code> is always downloaded and stored with <code>bar.pdf</code> as its filename, you can set its <code>Content-Disposition</code> header.</p>"},{"location":"howto/object-storage/s3/presign/#setting-content-disposition-on-object-creation","title":"Setting <code>Content-Disposition</code> on object creation","text":"awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3api put-object \\\n  --content-disposition 'attachment;filename=\"bar.pdf\"' \\\n  --bucket &lt;bucket-name&gt; \\\n  --key &lt;object-name&gt;\n</code></pre> <p>The <code>mc</code> client does not correctly support this feature.</p> <p>You can set the attribute on object creation:  <pre><code>mc cp --attr Content-Disposition='attachment;filename=\"bar.pdf\"' bar.pdf kna1/foo/\n</code></pre>  However, <code>mc</code> cuts off the <code>Content-Disposition</code> header at the semicolon, rendering it useless:  <pre><code>$ mc stat kna1/foo/bar.pdf\nName      : bar.pdf\nDate      : 2023-01-12 12:53:52 CET\nSize      : 57 KiB\nETag      : 630f6e1bf441a0eee63f9cb06804dc79\nType      : file\nMetadata  :\n  Content-Type       : application/pdf\n  X-Amz-Meta-Filename: bar.pdf\n  Content-Disposition: attachment\n</code></pre>  You should thus set the <code>Content-Disposition</code> header with a different client.</p> <pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; modify \\\n  --add-header 'Content-Disposition: attachment; filename=\"bar.pdf\"'\n  s3://foo/bar.pdf\n</code></pre>"},{"location":"howto/object-storage/s3/presign/#setting-content-disposition-on-existing-objects","title":"Setting <code>Content-Disposition</code> on existing objects","text":"awsmcs3cmd <p>To modify the <code>Content-Disposition</code> header of an existing object without downloading and re-uploading its contents, you must use <code>s3api copy-object</code>, with the object being its own copy source, and the metadata directive set to <code>replace</code>: <pre><code>aws --profile &lt;region&gt; \\\n  s3api copy-object \\\n  --copy-source &lt;bucket-name&gt;/&lt;object-name&gt;\n  --content-disposition 'attachment;filename=\"bar.pdf\"' \\\n  --metadata-directive replace\n  --bucket &lt;bucket-name&gt; \\\n  --key &lt;object-name&gt;\n</code></pre></p> <p>The <code>mc</code> client does not support modifying object metadata in-place.</p> <p><code>s3cmd</code> comes with a handy <code>modify --add-header</code> subcommand for updating object metadata in-place: <pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; modify \\\n  --add-header 'Content-Disposition: attachment; filename=\"bar.pdf\"'\n  s3://foo/bar.pdf\n</code></pre></p>"},{"location":"howto/object-storage/s3/presign/#testing-the-content-disposition-header","title":"Testing the <code>Content-Disposition</code> header","text":"<p>Once you have set the <code>Content-Disposition</code> header on an object and created a pre-signed URL for it, you can test its functionality with the <code>curl\u00a0-OJ</code> command:</p> <pre><code>$ curl -f -OJ 'https://s3-kna1.citycloud.com/foo/bar.pdf?AWSAccessKeyId=07576783684248f7b2745e34356c6025&amp;Expires=1673521496&amp;Signature=%2Frm9nLV3moP%2FQz7aGCAnrESXjbk%3D'\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 58503  100 58503    0     0   178k      0 --:--:-- --:--:-- --:--:--  178k\ncurl: Saved to filename 'bar.pdf'\n</code></pre>"},{"location":"howto/object-storage/s3/presign/#caution-do-not-abuse-content-disposition","title":"Caution: do not abuse <code>Content-Disposition</code>","text":"<p>Please keep in mind that there is nothing that keeps you from having two objects in the same bucket, with <code>Content-Disposition</code> on one object matching the object name of the other.</p> <p>For example, in the <code>foo</code> bucket, you may have</p> <ul> <li>an object named <code>spam.pdf</code> with no <code>Content-Disposition</code> header, and</li> <li>another object named <code>eggs.pdf</code>, with <code>Content-Disposition</code> set to include <code>filename=\"spam.pdf\"</code>.</li> </ul> <p>If you were now to generate a pre-signed URL on <code>eggs.pdf</code> and downloaded it with <code>curl -OJ</code>, you would end up with a local file named <code>spam.pdf</code>, with the contents of <code>eggs.pdf</code>. This is obviously not a good idea, and you should avoid setting a <code>Content-Disposition</code> header that does not agree with the object name.</p>"},{"location":"howto/object-storage/s3/public-bucket/","title":"Public buckets","text":"<p>You can use the S3 API to configure a bucket with public read access, so that anyone can download its objects with a web browser. Making a bucket globally readable entails setting a bucket policy that enables read access on all its objects.</p> <p>Setting a bucket policy affects all objects in a bucket. To avoid inadvertent disclosure of existing information, consider setting public read policies only on empty buckets.</p>"},{"location":"howto/object-storage/s3/public-bucket/#prerequisites","title":"Prerequisites","text":"<p>Object versioning requires that you configure your environment with working S3-compatible credentials.</p>"},{"location":"howto/object-storage/s3/public-bucket/#setting-a-public-read-policy-for-a-bucket","title":"Setting a public read policy for a bucket","text":"<p>First, create a local policy file named <code>policy.json</code>, with the following content (replace <code>&lt;bucket-name&gt;</code> with the name of your bucket):</p> <pre><code>{ \n  \"Statement\": [ \n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::&lt;bucket-name&gt;/*\"\n    }\n  ]\n}\n</code></pre> <p>To apply this policy to a bucket such that read-only access is permitted for everyone, run the following command:</p> awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3api put-bucket-policy \\\n  --policy file://policy.json \\\n  --bucket &lt;bucket-name&gt;\n</code></pre> <pre><code>mc anonymous set-json policy.json &lt;region&gt;/&lt;bucket-name&gt;\n</code></pre> <pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; setpolicy policy.json s3://&lt;bucket-name&gt;\n</code></pre>"},{"location":"howto/object-storage/s3/public-bucket/#accessing-objects-in-a-public-bucket","title":"Accessing objects in a public bucket","text":"<p>To access an object in a public bucket from a web browser or a generic HTTP/HTTPS client like <code>curl</code>, you must construct its URI as follows:</p> <pre><code>https://s3-&lt;region&gt;.citycloud.com/&lt;project-uuid&gt;:&lt;bucket-name&gt;/object-name\n</code></pre> <p>Your project UUID is listed as the <code>project_id</code> field in the output of the <code>openstack\u00a0ec2\u00a0credentials\u00a0create</code> command you used to create your S3-compatible credentials.</p> <p>If you did not note it down at the time of account creation, you can always retrieve it with <code>openstack\u00a0ec2\u00a0credentials\u00a0&lt;access-key&gt;</code>.</p> <p>For example, to retrieve an object named <code>bar.pdf</code> in a bucket named <code>foo</code> from the project with the UUID <code>07576783684248f7b2745e34356c6025</code> in the Cleura\u00a0Cloud Kna1 region, you would run:</p> <pre><code>$ curl -O https://s3-kna1.citycloud.com/07576783684248f7b2745e34356c6025:foo/bar.pdf\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 62703  100 62703    0     0   186k      0 --:--:-- --:--:-- --:--:--  186k\n</code></pre>"},{"location":"howto/object-storage/s3/public-bucket/#public-bucket-accessibility-via-the-swift-api","title":"Public bucket accessibility via the Swift API","text":"<p>Once you make a bucket public via the S3 API, its objects also become accessible via the corresponding Swift API path.</p> <p>Thus, the following URL paths allow you to retrieve the same public object:</p> <ul> <li><code>https://s3-kna1.citycloud.com/07576783684248f7b2745e34356c6025:foo/bar.pdf</code></li> <li><code>https://swift-kna1.citycloud.com/swift/v1/AUTH_07576783684248f7b2745e34356c6025/foo/bar.pdf</code></li> </ul>"},{"location":"howto/object-storage/s3/public-bucket/#enabling-bucket-listing","title":"Enabling bucket listing","text":"<p>The <code>policy.json</code> file above allows anyone to retrieve known objects by name, but does not enable listing the bucket\u2019s contents. Most of the time, this is what you want.</p> <p>However, in case you do need unauthenticated clients to be able to list all objects in a bucket, modify your <code>policy.json</code> file as in the following example:</p> <pre><code>{\n  \"Statement\": [ \n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::&lt;bucket-name&gt;/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:ListBucket\",\n      \"Resource\": \"arn:aws:s3:::&lt;bucket-name&gt;\"\n    }\n  ]\n}\n</code></pre> <p>Note that for the <code>s3:GetObject</code> action, <code>/*</code> follows the bucket name, whereas for the <code>s3:ListBucket</code> action you specify just the bucket name with no suffix.</p> <p>Once you have updated your local <code>policy.json</code> file, apply it just as when you created the policy.</p> <p>You can then open a bucket path in your browser, to retrieve an XML document containing a list of all objects in the bucket. You would construct the URL by the following schema:</p> <pre><code>https://s3-&lt;region&gt;.citycloud.com/&lt;project-uuid&gt;:&lt;bucket-name&gt;\n</code></pre>"},{"location":"howto/object-storage/s3/public-bucket/#removing-a-public-read-policy-from-a-bucket","title":"Removing a public read policy from a bucket","text":"<p>If you want to remove a previously-set public read policy from a bucket, and revert to its default policy that requires authentication on every object access, run the following command:</p> awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3api delete-bucket-policy \\\n  --bucket &lt;bucket-name&gt;\n</code></pre> <pre><code>mc anonymous set none &lt;region&gt;/&lt;bucket-name&gt;\n</code></pre> <pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; delpolicy s3://&lt;bucket-name&gt;\n</code></pre>"},{"location":"howto/object-storage/s3/sse-c/","title":"Server-side object encryption with customer-provided keys (SSE-C)","text":"<p>You can use object encryption via the S3 API, according to the Amazon SSE-C specification. This means that you need to provide an encryption/decryption key with each request to the object.</p> <p>You can store the encryption key in Barbican, and provide it to the S3 client at runtime.</p>"},{"location":"howto/object-storage/s3/sse-c/#requirements","title":"Requirements","text":"<p>This guide assumes familiarity with the following tools:</p> <ul> <li><code>python-openstackclient</code> (with the <code>python-barbicanclient</code> plugin),</li> <li><code>pwgen</code>,</li> <li><code>rclone</code> version 1.54 or later, or the <code>aws</code> command-line interface (<code>awscli</code>) version 1.</li> </ul>"},{"location":"howto/object-storage/s3/sse-c/#prerequisites","title":"Prerequisites","text":"<p>In order to manipulate S3 objects using SSE-C, you must</p> <ul> <li>configure the OpenStack CLI,</li> <li>obtain S3-compatible credentials.</li> </ul>"},{"location":"howto/object-storage/s3/sse-c/#creating-encryption-details","title":"Creating encryption details","text":"<p>According to the SSE-C specification, in order to use server-side encryption, any S3 client needs to provide three pieces of information, which it includes in the request headers for each S3 request being made:</p> <ul> <li>Encryption algorithm: the only valid option here is AES256.</li> <li>Encryption key: a valid AES key, which means that key length must be 32 bytes.</li> <li>Encryption key checksum: the MD5 checksum of the encryption key.   Your S3 API client uses this for integrity checks, and normally adds it automatically to your request.</li> </ul> <p>In order to generate encryption key and store it in Barbican, proceed as follows.</p> <ol> <li> <p>Generate an encryption secret:    <pre><code>secret_raw=$(pwgen 32 1)\n</code></pre></p> </li> <li> <p>Store the secret in Barbican:    <pre><code>barbican_secret_url=$(openstack secret store --name objectSecret --algorithm aes --bit-length 256 --payload ${secret_raw} -f value -c 'Secret href')\n</code></pre></p> </li> <li> <p>Then, whenever you need to upload or download encrypted objects, retrieve the secret from Barbican:    <pre><code>secret=$(openstack secret get ${barbican_secret_url} -p -c Payload -f value)\n</code></pre></p> </li> </ol>"},{"location":"howto/object-storage/s3/sse-c/#managing-encrypted-objects-in-s3","title":"Managing encrypted objects in S3","text":"<p>Once you have your encryption secret available, you can create or access enabled objects.</p> awsmcs3cmdrclone <ol> <li>Create an S3 bucket:    <pre><code>aws --profile &lt;region&gt; \\\n  s3 mb s3://cleura\u00a0cloud-encrypted\n</code></pre></li> <li>Sync a directory to the S3 bucket, encrypting the files it contains on upload:    <pre><code>aws --profile &lt;region&gt; \\\n  s3 sync \\\n  ~/media/ s3://cleura\u00a0cloud-encrypted \\\n  --sse-c AES256 \\\n  --sse-c-key ${secret}\n</code></pre></li> <li>Retrieve a file from S3 and decrypt it:    <pre><code>aws --profile &lt;region&gt; \\\n  s3 cp \\\n  s3://cleura\u00a0cloud-encrypted/file.png . \\\n  --sse-c AES256 \\\n  --sse-c-key ${secret}\n</code></pre></li> </ol> <p>Note that attempting to download an encrypted file without providing an encryption key results in an immediate HTTP 400 (\u201cBad Request\u201d) error: <pre><code>$ aws --profile &lt;region&gt; \\\n  s3 cp \\\n  s3://cleura\u00a0cloud-encrypted/file.png .\nfatal error: An error occurred (400) when calling the HeadObject operation: Bad Request\n</code></pre></p> <ol> <li>Create an S3 bucket:    <pre><code>mc mb &lt;region&gt;/cleura\u00a0cloud-encrypted\n</code></pre></li> <li>Sync a directory to the S3 bucket, encrypting the files it contains on upload.    Note that you must specify the encryption secret as the argument to the <code>--encrypt-key</code> option, using a syntax of <code>&lt;minio-alias&gt;/&lt;bucket-name&gt;=&lt;encryption-key&gt;</code>:    <pre><code>mc cp \\\n  --recursive \\\n  --encrypt-key \"&lt;region&gt;/cleura\u00a0cloud-encrypted=${secret}\"\n  ~/media/ &lt;region&gt;/cleura\u00a0cloud-encrypted\n</code></pre></li> <li>Retrieve a file from S3 and decrypt it.    Again, specify the encryption key in the same manner:    <pre><code>mc cp \\\n  --encrypt-key \"&lt;region&gt;/cleura\u00a0cloud-encrypted=${secret}\"\n  &lt;region&gt;/cleura\u00a0cloud-encrypted/file.png .\n</code></pre></li> </ol> <p>Note that attempting to download an encrypted file without providing an encryption key results in an immediate HTTP 400 (\u201cBad Request\u201d) error: <pre><code>$ mc cp \\\n  &lt;region&gt;/cleura\u00a0cloud-encrypted/file.png .\nmc: &lt;ERROR&gt; Unable to validate source `&lt;region&gt;/cleura\u00a0cloud-encrypted/file.png`.\n</code></pre></p> <p>You cannot use <code>s3cmd</code> in combination with SSE-C.</p> <p><code>s3cmd</code> does contain a client side encryption facility, using GnuPG for encryption. It also supports SSE-KMS, which is a different SSE flavor that is currently not available in Cleura\u00a0Cloud.</p> <p>SSE-C encryption has been implemented/fixed with version 1.54. Earlier <code>rclone</code> versions won\u2019t work.</p> <ol> <li>To start with, modify the section in your configuration file that is named after your target region, adding the <code>sse_customer_algorithm</code> option:    <pre><code>[&lt;region&gt;]\ntype = s3\nprovider = Ceph\nenv_auth = false\naccess_key_id = &lt;access key id&gt;\nsecret_access_key = &lt;secret key&gt;\nendpoint = &lt;region&gt;.citycloud.com\nacl = private\nsse_customer_algorithm = AES256\n</code></pre></li> <li>Create an S3 bucket:    <pre><code>rclone mkdir cleura\u00a0cloud-encrypted\n</code></pre></li> <li>Sync a directory to the S3 bucket, encrypting the files it contains on upload:    <pre><code>rclone sync ~/media/ cleura\u00a0cloud-encrypted \\\n  --s3-sse-customer-key=${secret}\n</code></pre></li> <li>Retrieve a file from S3 and decrypt it:    <pre><code>rclone copy cleura\u00a0cloud-encrypted/file.png \\\n  --s3-sse-customer-key=${secret}\n</code></pre></li> </ol> <p>For more examples on how to use rclone, please use its reference documentation.</p>"},{"location":"howto/object-storage/s3/utilization/","title":"Object storage utilization","text":"<p>You may be interested in the number of objects, or their cumulative size, currently held in a bucket.</p>"},{"location":"howto/object-storage/s3/utilization/#prerequisites","title":"Prerequisites","text":"<p>To show the number or cumulative size of objects in a bucket, you must install <code>aws</code>, <code>mc</code>, or <code>s3cmd</code>, and configure it with working credentials.</p>"},{"location":"howto/object-storage/s3/utilization/#showing-the-number-of-objects-in-a-bucket","title":"Showing the number of objects in a bucket","text":"<p>To show the number of objects in a given bucket, use one of the following commands:</p> awsmcs3cmd <p><pre><code>aws --profile &lt;region&gt; \\\n  s3 ls \\\n  --recursive \\\n  --summarize \\\n  s3://&lt;bucket&gt;\n</code></pre> The object count is in the line prefixed with <code>Total\u00a0Objects</code>, at the end of the output.</p> <p><pre><code>mc ls &lt;region&gt;/&lt;bucket&gt; \\\n  --recursive \\\n  --summarize\n</code></pre> The object count is in the line prefixed with <code>Total\u00a0Objects</code>, at the end of the output.</p> <p>Alternatively, you may also use the <code>du</code> subcommand: <pre><code>mc du &lt;region&gt;/&lt;bucket&gt;\n</code></pre> Here, the object count is the second column of the output.</p> <p><pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; \\\n  du \\\n  s3://&lt;bucket&gt;\n</code></pre> The object count is the second column of the output.</p>"},{"location":"howto/object-storage/s3/utilization/#showing-the-total-size-of-objects-in-a-bucket","title":"Showing the total size of objects in a bucket","text":"<p>To show the overall size of all objects in a given bucket, use one of the following commands:</p> awsmcs3cmd <p>You use the same command as for counting objects: <pre><code>aws --profile &lt;region&gt; \\\n  s3 ls \\\n  --recursive \\\n  --summarize \\\n  s3://&lt;bucket&gt;\n</code></pre> The total object size is in the line prefixed with <code>Total\u00a0Size</code>, at the end of the output.</p> <p>By default, the total size is given in bytes. If you prefer more sensible units, add the <code>--human-readable</code> option: <pre><code>aws --profile &lt;region&gt; \\\n  s3 ls \\\n  --recursive \\\n  --summarize \\\n  --human-readable \\\n  s3://&lt;bucket&gt;\n</code></pre></p> <p>You could use the same command as for counting objects: <pre><code>mc ls &lt;region&gt;/&lt;bucket&gt; \\\n  --recursive \\\n  --summarize\n</code></pre> The total object size is in the line prefixed with <code>Total\u00a0Size</code>, at the end of the output. It is shown in KiB, MiB, GiB, or TiB, and the value shown may thus be approximate.</p> <p>Alternatively, you may also use the <code>du</code> subcommand: <pre><code>mc du &lt;region&gt;/&lt;bucket&gt;\n</code></pre> The total object size is the first column of the output.</p> <p><pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; \\\n  du \\\n  s3://&lt;bucket&gt;\n</code></pre> The total object size is the first column of the output.</p>"},{"location":"howto/object-storage/s3/versioning/","title":"Object versioning","text":"<p>If you are unfamiliar with object versioning, see our brief explanation of the concept.</p> <p>Object versioning requires that you configure your environment with working S3-compatible credentials.</p>"},{"location":"howto/object-storage/s3/versioning/#enabling-bucket-versioning","title":"Enabling bucket versioning","text":"<p>To enable versioning in a bucket, use one of the following commands:</p> awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3api put-bucket-versioning \\\n  --versioning-configuration Status=Enabled \\\n  --bucket &lt;bucket-name&gt;\n</code></pre> <pre><code>mc version enable &lt;region&gt;/&lt;bucket-name&gt;\n</code></pre> <p>This functionality is not available with the <code>s3cmd</code> command.</p>"},{"location":"howto/object-storage/s3/versioning/#checking-bucket-versioning-status","title":"Checking bucket versioning status","text":"<p>To check whether object versioning is enabled on a bucket, use one of the following commands:</p> awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3api get-bucket-versioning \\\n  --bucket &lt;bucket-name&gt;\n</code></pre> <pre><code>mc version info &lt;region&gt;/&lt;bucket-name&gt;\n</code></pre> <p>This functionality is not available with the <code>s3cmd</code> command.</p>"},{"location":"howto/object-storage/s3/versioning/#suspending-bucket-versioning","title":"Suspending bucket versioning","text":"<p>To suspend versioning on a bucket (versioning cannot be completely disabled once enabled), use one of the following commands:</p> awsmc <pre><code>aws --profile &lt;region&gt; \\\ns3api put-bucket-versioning \\\n--versioning-configuration Status=Suspended \\\n--bucket &lt;bucket-name&gt;\n</code></pre> <pre><code>mc version suspend &lt;region&gt;/&lt;bucket-name&gt;\n</code></pre>"},{"location":"howto/object-storage/s3/versioning/#creating-a-versioned-object","title":"Creating a versioned object","text":"<p>Once object versioning is enabled on a bucket, the normal object creation and replacement commands behave in a manner different from that in unversioned buckets:</p> <ul> <li>If the object does not already exist, it is created (as in an unversioned bucket).</li> <li>If the object does exist, it is not replaced.   Instead, the new version becomes the current one.</li> </ul> awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3api put-object \\\n  --bucket &lt;bucket-name&gt; \\\n  --key &lt;object-name&gt; \\\n  --body &lt;local-filename&gt;\n</code></pre> <pre><code>mc cp \\\n  &lt;local-filename&gt; \\\n  &lt;region&gt;/&lt;bucket-name&gt;/&lt;object-name&gt;\n</code></pre> <pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; put &lt;local-filename&gt; s3://&lt;bucket&gt;\n</code></pre>"},{"location":"howto/object-storage/s3/versioning/#listing-object-versions","title":"Listing object versions","text":"<p>In a bucket that has versioning enabled, you may list the versions available for an object:</p> awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3api list-object-versions \\\n  --bucket &lt;bucket-name&gt; \\\n  --key &lt;object-name&gt;\n</code></pre> <p><pre><code>mc stat --versions &lt;region&gt;/&lt;bucket-name&gt;\n</code></pre> This functionality may be impacted by bugs in several versions of the <code>mc</code> client.</p> <p>This functionality is not available with the <code>s3cmd</code> command.</p>"},{"location":"howto/object-storage/s3/versioning/#retrieving-a-versioned-object","title":"Retrieving a versioned object","text":"<p>To download a specific version of an object in a bucket, use one of the following commands:</p> awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3api get-object \\\n  --bucket &lt;bucket-name&gt; \\\n  --key &lt;object-name&gt; \\\n  --version-id &lt;versionid&gt; \\\n  &lt;local-filename&gt;\n</code></pre> <pre><code>mc cp \\\n  --version-id &lt;versionid&gt; \\\n  &lt;region&gt;/&lt;bucket-name&gt;/&lt;object-name&gt; \\\n  &lt;local-filename&gt;\n</code></pre> <p>This functionality is not available with the <code>s3cmd</code> command.</p> <p>When you download an object from a versioned bucket without specifying a version identifier, your S3 client will download the latest version of that object.</p>"},{"location":"howto/object-storage/s3/versioning/#deleting-a-versioned-object","title":"Deleting a versioned object","text":"<p>Like the commands to create objects, the commands to delete them behave differently once object versioning is enabled on a bucket.</p> <p>The command to delete an object will not delete it, but put a delete marker on it instead. This will keep all the versions but return a \u201cNot found\u201d 404 on any request not specifying a valid version id.</p> awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3api delete-object \\\n  --bucket &lt;bucket-name&gt; \\\n  --key &lt;object-name&gt;\n</code></pre> <pre><code>mc rm \\\n  &lt;region&gt;/&lt;bucket-name&gt;/&lt;object-name&gt;\n</code></pre> <pre><code>s3cmd -c ~/.s3cfg-&lt;region&gt; del s3://&lt;bucket-name&gt;/&lt;object-name&gt;\n</code></pre> <p>You also have the option of deleting not the latest version, but a specific object version:</p> awsmcs3cmd <pre><code>aws --profile &lt;region&gt; \\\n  s3api delete-object \\\n  --version-id &lt;versionid&gt; \\\n  --bucket &lt;bucket-name&gt; \\\n  --key &lt;object-name&gt;\n</code></pre> <pre><code>mc rm \\\n  --version-id &lt;versionid&gt; \\\n  &lt;region&gt;/&lt;bucket-name&gt;/&lt;object-name&gt;\n</code></pre> <p>This functionality is not available with the <code>s3cmd</code> command.</p>"},{"location":"howto/object-storage/s3/versioning/#deleting-a-versioned-bucket","title":"Deleting a versioned bucket","text":"<p>Sometimes, you may want to delete a whole bucket that</p> <ul> <li>is versioning-enabled,</li> <li>contains lots of objects, and</li> <li>contains multiple versions of each object.</li> </ul> <p>In that case, the deletion process can become quite involved.</p> <p>However, you can use a shortcut via object expiry: you set a lifecycle policy that defines the minimum expiry age of 1 day, and includes both older versions and delete markers in object expiry.</p> <p>A lifecycle policy applicable for this purpose would be this:</p> <pre><code>{\n  \"Rules\": [{\n    \"ID\": \"empty-bucket\",\n    \"Status\": \"Enabled\",\n    \"Prefix\": \"\",\n    \"Expiration\": {\n      \"Days\": 1\n    },\n    \"NoncurrentVersionExpiration\": {\n      \"NoncurrentDays\": 1\n    }\n  },\n  {\n    \"ID\": \"expire-delete-markers\",\n    \"Status\": \"Enabled\",\n    \"Prefix\": \"\",\n    \"Expiration\": {\n      \"ExpiredObjectDeleteMarker\": true\n    },\n    \"NoncurrentVersionExpiration\": {\n      \"NoncurrentDays\": 1\n    }\n  }]\n}\n</code></pre> <p>Enable this policy on the bucket you want to delete, and wait 24 hours. Then, you should be able to delete the bucket, which will at that point be empty.</p>"},{"location":"howto/object-storage/swift/","title":"Swift API","text":"<p>OpenStack Swift (not to be confused with the programming language of the same name) is an object-access API similar to, but distinct from, the S3 object storage API.</p> <p>In Cleura\u00a0Cloud, you interact with the Swift API using either the <code>swift</code> command-line interface (CLI), or the standard <code>openstack</code> CLI.</p> <p>Either way, in addition to installing and configuring the Python <code>openstackclient</code> module, you need to install the Python <code>swiftclient</code> module. Use either the package manager of your operating system, or <code>pip</code>:</p> Debian/UbuntuMac OS X with HomebrewPython Package <pre><code>apt install python3-swiftclient\n</code></pre> <p>This Python module is unavailable via <code>brew</code>, but you can install it via <code>pip</code>.</p> <pre><code>pip install python-swiftclient\n</code></pre>"},{"location":"howto/object-storage/swift/#availability","title":"Availability","text":"<p>The OpenStack Swift API is available in select Cleura\u00a0Cloud regions. Refer to the feature support matrix for details on Swift API availability.</p>"},{"location":"howto/object-storage/swift/expiry/","title":"Object expiry","text":"<p>Using the Swift API, you have the option for objects to automatically be deleted, after they have passed an expiry threshold.</p>"},{"location":"howto/object-storage/swift/expiry/#prerequisites","title":"Prerequisites","text":"<p>In order to manage object expiry, be sure that you have installed and configured the <code>swift</code> command-line interface (CLI). There is presently no way to set object expiry with the <code>openstack</code> CLI.</p>"},{"location":"howto/object-storage/swift/expiry/#auto-deletion-at-a-fixed-date","title":"Auto-deletion at a fixed date","text":"<p>In order for an object to be automatically deleted at a certain date, you must first convert that date to a POSIX timestamp. You may do so with the <code>date</code> command. For example, to retrieve the POSIX timestamp for February 29, 2024, at 0000 UTC, use this command:</p> <pre><code>$ TZ=Etc/UTC date -d '2024-02-29' +'%s'\n1709164800\n</code></pre> <p>You can then set the <code>X-Delete-At</code> header on an object, so that it is automatically deleted at that time:</p> <pre><code>$ swift post -H \"X-Delete-At: 1709164800\" private-container testobj.txt\n</code></pre> <p>Then, you can read back the header with <code>swift\u00a0stat</code>:</p> <pre><code>$ swift stat private-container testobj.txt\n               Account: AUTH_30a7768a0ffc40359d6110f21a6e7d88\n             Container: private-container\n                Object: testobj.txt\n          Content Type: binary/octet-stream\n        Content Length: 12\n         Last Modified: Mon, 05 Dec 2022 14:09:02 GMT\n                  ETag: 6f5902ac237024bdd0c176cb93063dc4\n         Accept-Ranges: bytes\n           X-Timestamp: 1670249342.53870\n           X-Delete-At: 1709164800\n            X-Trans-Id: tx00000646596cd018a2d7b-00638dfb91-300de11-default\nX-Openstack-Request-Id: tx00000646596cd018a2d7b-00638dfb91-300de11-default\n</code></pre>"},{"location":"howto/object-storage/swift/expiry/#auto-deletion-after-a-time-period","title":"Auto-deletion after a time period","text":"<p>Instead of giving an absolute time with <code>X-Delete-At</code>, you can also use <code>X-Delete-After</code> (in seconds), so that the object is automatically deleted after that timespan. This example uses 600\u00a0seconds or 10\u00a0minutes:</p> <pre><code>$ swift post -H \"X-Delete-After: 600\" private-container testobj.txt\n</code></pre> <p>The Swift API then converts this into an <code>X-Delete-At</code> header, adding the specified time span to the date the request is received (indicated by the <code>X-Timestamp</code> header).</p> <p>You can then read back the object metadata. Observe that in this example, the difference between the <code>X-Timestamp</code> and <code>X-Delete-At</code> headers is 600 seconds:</p> <pre><code>$ swift stat private-container testobj.txt\n               Account: AUTH_30a7768a0ffc40359d6110f21a6e7d88\n             Container: private-container\n                Object: testobj.txt\n          Content Type: binary/octet-stream\n        Content Length: 12\n         Last Modified: Mon, 05 Dec 2022 14:09:55 GMT\n                  ETag: 6f5902ac237024bdd0c176cb93063dc4\n         Accept-Ranges: bytes\n           X-Timestamp: 1670249395.20576\n           X-Delete-At: 1670249995\n            X-Trans-Id: tx0000065937c08551ba2be-00638dfbb6-301ddeb-default\nX-Openstack-Request-Id: tx0000065937c08551ba2be-00638dfbb6-301ddeb-default\n</code></pre>"},{"location":"howto/object-storage/swift/private-container/","title":"Working with a private Swift container","text":""},{"location":"howto/object-storage/swift/private-container/#prerequisites","title":"Prerequisites","text":"<p>In order to create a Swift container, be sure that you have installed and configured the required command-line interface (CLI) tools.</p>"},{"location":"howto/object-storage/swift/private-container/#creating-a-private-container","title":"Creating a private container","text":"<p>To create a private container (that is, one that can only be accessed with proper Swift API credentials), use the following command:</p> OpenStack CLISwift CLI <pre><code>$ openstack container create private-container\n+---------------------------------------+-------------------+----------------------------------------------------+\n| account                               | container         | x-trans-id                                         |\n+---------------------------------------+-------------------+----------------------------------------------------+\n| AUTH_30a7768a0ffc40359d6110f21a6e7d88 | private-container | tx00000ddb0f9e2a50ad881-00638dbf9c-300de11-default |\n+---------------------------------------+-------------------+----------------------------------------------------+\n</code></pre> <p><pre><code>swift post private-container\n</code></pre> This command produces no output.</p>"},{"location":"howto/object-storage/swift/private-container/#retrieving-container-information","title":"Retrieving container information","text":"<p>To create a list of all containers accessible with your current set of credentials, use this command:</p> OpenStack CLISwift CLI <pre><code>$ openstack container list\n+-------------------+\n| Name              |\n+-------------------+\n| private-container |\n+-------------------+\n</code></pre> <pre><code>$ swift list\nprivate-container\n</code></pre> <p>To retrieve more detailed information about an individual container, you can also use this command:</p> OpenStack CLISwift CLI <pre><code>$ openstack container show private-container\n+----------------+---------------------------------------+\n| Field          | Value                                 |\n+----------------+---------------------------------------+\n| account        | AUTH_30a7768a0ffc40359d6110f21a6e7d88 |\n| bytes_used     | 0                                     |\n| container      | private-container                     |\n| object_count   | 0                                     |\n| storage_policy | default-placement                     |\n+----------------+---------------------------------------+\n</code></pre> <pre><code>$ swift stat private-container\n                      Account: AUTH_30a7768a0ffc40359d6110f21a6e7d88\n                    Container: private-container\n                      Objects: 0\n                        Bytes: 0\n                     Read ACL:\n                    Write ACL:\n                      Sync To:\n                     Sync Key:\n                  X-Timestamp: 1670234012.31534\nX-Container-Bytes-Used-Actual: 0\n             X-Storage-Policy: default-placement\n              X-Storage-Class: STANDARD\n                Last-Modified: Mon, 05 Dec 2022 09:53:32 GMT\n                   X-Trans-Id: tx0000073eebb42acd6e7e1-00638dbfe8-301ddeb-default\n       X-Openstack-Request-Id: tx0000073eebb42acd6e7e1-00638dbfe8-301ddeb-default\n                Accept-Ranges: bytes\n                 Content-Type: text/plain; charset=utf-8\n</code></pre>"},{"location":"howto/object-storage/swift/private-container/#uploading-data","title":"Uploading data","text":"<p>To upload an object into the container, create a local test file:</p> <pre><code>echo \"hello world\" &gt; testobj.txt\n</code></pre> <p>Then, upload the file (as a Swift object) into your container, and read back its metadata:</p> OpenStack CLISwift CLI <pre><code>$ openstack object create private-container testobj.txt\n+-------------+-------------------+----------------------------------+\n| object      | container         | etag                             |\n+-------------+-------------------+----------------------------------+\n| testobj.txt | private-container | 6f5902ac237024bdd0c176cb93063dc4 |\n+-------------+-------------------+----------------------------------+\n\n$ openstack object show private-container testobj.txt\n+----------------+---------------------------------------+\n| Field          | Value                                 |\n+----------------+---------------------------------------+\n| account        | AUTH_30a7768a0ffc40359d6110f21a6e7d88 |\n| container      | private-container                     |\n| content-length | 12                                    |\n| content-type   | text/plain                            |\n| etag           | 6f5902ac237024bdd0c176cb93063dc4      |\n| last-modified  | Mon, 05 Dec 2022 10:00:34 GMT         |\n| object         | testobj.txt                           |\n| properties     | mtime='1670234292.370177'             |\n+----------------+---------------------------------------+\n</code></pre> <pre><code>$ swift upload private-container testobj.txt\ntestobj.txt\n\n$ swift stat private-container testobj.txt\n               Account: AUTH_30a7768a0ffc40359d6110f21a6e7d88\n             Container: private-container\n                Object: testobj.txt\n          Content Type: text/plain\n        Content Length: 12\n         Last Modified: Mon, 05 Dec 2022 10:00:34 GMT\n                  ETag: 6f5902ac237024bdd0c176cb93063dc4\n            Meta Mtime: 1670234292.370177\n         Accept-Ranges: bytes\n           X-Timestamp: 1670234434.67877\n            X-Trans-Id: tx000000f26ccf73c19f596-00638dc160-300de11-default\nX-Openstack-Request-Id: tx000000f26ccf73c19f596-00638dc160-300de11-default\n</code></pre>"},{"location":"howto/object-storage/swift/private-container/#downloading-data","title":"Downloading data","text":"<p>To download an object from your Swift container, use the following command:</p> OpenStack CLISwift CLI <p><pre><code>$ openstack object save --file - private-container testobj.txt\nhello world\n</code></pre> The <code>--file -</code> option prints the file contents to stdout. If instead you want to save the object\u2019s content to a local file, use <code>--file &lt;filename&gt;</code>.</p> <p>If you omit the <code>--file</code> argument altogether, <code>openstack object save</code> will create a local file named like the object you are downloading (in this case, <code>testobj.txt</code>).</p> <p><pre><code>$ swift download -o - private-container testobj.txt\nhello world\n</code></pre> The <code>-o -</code> option prints the file contents to stdout. If instead you want to save the object\u2019s content to a local file, use <code>-o &lt;filename&gt;</code>.</p> <p>If you omit the <code>-o</code> argument altogether, <code>swift download</code> will create a local file named like the object you are downloading (in this case, <code>testobj.txt</code>).</p>"},{"location":"howto/object-storage/swift/public-container/","title":"Working with a public Swift container","text":""},{"location":"howto/object-storage/swift/public-container/#prerequisites","title":"Prerequisites","text":"<p>In order to create a Swift container, be sure that you have installed and configured the required command-line interface (CLI) tools.</p>"},{"location":"howto/object-storage/swift/public-container/#creating-the-container","title":"Creating the container","text":"<p>To create a public container (that is, one whose contents can be accessed without credentials), use the following command:</p> OpenStack CLISwift CLI <pre><code>$ openstack container create --public public-container\n+---------------------------------------+------------------+----------------------------------------------------+\n| account                               | container        | x-trans-id                                         |\n+---------------------------------------+------------------+----------------------------------------------------+\n| AUTH_30a7768a0ffc40359d6110f21a6e7d88 | public-container | tx00000d4f7d958e3e0c9aa-00638dc6ac-300de11-default |\n+---------------------------------------+------------------+----------------------------------------------------+\n</code></pre> <p><pre><code>$ swift post --read-acl \".r:*,.rlistings\" public-container\n</code></pre> This command produces no output.</p>"},{"location":"howto/object-storage/swift/public-container/#retrieving-container-information","title":"Retrieving container information","text":"<p>To create a list of all containers accessible with your current set of credentials, use this command:</p> OpenStack CLISwift CLI <pre><code>$ openstack container list\n+-------------------+\n| Name              |\n+-------------------+\n| private-container |\n| public-container  |\n+-------------------+\n</code></pre> <pre><code>$ swift list\nprivate-container\npublic-container\n</code></pre> <p>To retrieve more detailed information about an individual container, you can also use this command. Observe that the Read access control list (ACL) contains the entry <code>.r:*,.rlistings</code>, which enables read access to all objects in a container, and to a list of objects included in the container.</p> OpenStack CLISwift CLI <pre><code>openstack container show public-container\n+----------------+---------------------------------------+\n| Field          | Value                                 |\n+----------------+---------------------------------------+\n| account        | AUTH_30a7768a0ffc40359d6110f21a6e7d88 |\n| bytes_used     | 0                                     |\n| container      | public-container                      |\n| object_count   | 0                                     |\n| read_acl       | .r:*,.rlistings                       |\n| storage_policy | default-placement                     |\n+----------------+---------------------------------------+\n</code></pre> <pre><code>$ swift stat public-container\n                      Account: AUTH_30a7768a0ffc40359d6110f21a6e7d88\n                    Container: public-container\n                      Objects: 0\n                        Bytes: 0\n                     Read ACL: .r:*,.rlistings\n                    Write ACL:\n                      Sync To:\n                     Sync Key:\n                  X-Timestamp: 1670235997.87682\nX-Container-Bytes-Used-Actual: 0\n             X-Storage-Policy: default-placement\n              X-Storage-Class: STANDARD\n                Last-Modified: Mon, 05 Dec 2022 10:26:37 GMT\n                   X-Trans-Id: tx00000cd9e7c26095ab862-00638dc78a-301ddeb-default\n       X-Openstack-Request-Id: tx00000cd9e7c26095ab862-00638dc78a-301ddeb-default\n                Accept-Ranges: bytes\n                 Content-Type: text/plain; charset=utf-8\n</code></pre>"},{"location":"howto/object-storage/swift/public-container/#uploading-data","title":"Uploading data","text":"<p>To upload an object into the container, create a local test file:</p> <pre><code>echo \"hello world\" &gt; testobj.txt\n</code></pre> <p>Then, upload the file (as a Swift object) into your container, and read back its metadata:</p> OpenStack CLISwift CLI <pre><code>$ openstack object create public-container testobj.txt\n+-------------+------------------+----------------------------------+\n| object      | container        | etag                             |\n+-------------+------------------+----------------------------------+\n| testobj.txt | public-container | 6f5902ac237024bdd0c176cb93063dc4 |\n+-------------+------------------+----------------------------------+\n\n$ openstack object show public-container testobj.txt\n+----------------+---------------------------------------+\n| Field          | Value                                 |\n+----------------+---------------------------------------+\n| account        | AUTH_30a7768a0ffc40359d6110f21a6e7d88 |\n| container      | public-container                      |\n| content-length | 12                                    |\n| content-type   | text/plain                            |\n| etag           | 6f5902ac237024bdd0c176cb93063dc4      |\n| last-modified  | Mon, 05 Dec 2022 10:28:09 GMT         |\n| object         | testobj.txt                           |\n+----------------+---------------------------------------+\n</code></pre> <pre><code>$ swift upload public-container testobj.txt\ntestobj.txt\n\n$ swift stat public-container testobj.txt\n               Account: AUTH_30a7768a0ffc40359d6110f21a6e7d88\n             Container: public-container\n                Object: testobj.txt\n          Content Type: text/plain\n        Content Length: 12\n         Last Modified: Mon, 05 Dec 2022 10:28:09 GMT\n                  ETag: 6f5902ac237024bdd0c176cb93063dc4\n         Accept-Ranges: bytes\n           X-Timestamp: 1670236089.75015\n            X-Trans-Id: tx0000075bca59e9149bc53-00638dc7fa-301ddeb-default\nX-Openstack-Request-Id: tx0000075bca59e9149bc53-00638dc7fa-301ddeb-default\n</code></pre>"},{"location":"howto/object-storage/swift/public-container/#downloading-data","title":"Downloading data","text":"<p>To download an object from your public Swift container, you can use the following commands (as with a private container):</p> OpenStack CLISwift CLI <p><pre><code>$ openstack object save --file - private-container testobj.txt\nhello world\n</code></pre> The <code>--file -</code> option prints the file contents to stdout. If instead you want to save the object\u2019s content to a local file, use <code>--file &lt;filename&gt;</code>.</p> <p>If you omit the <code>--file</code> argument altogether, <code>openstack object save</code> will create a local file named like the object you are downloading (in this case, <code>testobj.txt</code>).</p> <p><pre><code>$ swift download -o - private-container testobj.txt\nhello world\n</code></pre> The <code>-o -</code> option prints the file contents to stdout. If instead you want to save the object\u2019s content to a local file, use <code>-o &lt;filename&gt;</code>.</p> <p>If you omit the <code>-o</code> argument altogether, <code>swift download</code> will create a local file named like the object you are downloading (in this case, <code>testobj.txt</code>).</p> <p>However, this being a public container, you can also retrieve your object using any regular HTTP/HTTPS client, using a public URL. This URL is composed as follows:</p> <ol> <li>The Swift API\u2019s base URL, which differs by Cleura\u00a0Cloud region (<code>https://swift\u2011&lt;region&gt;.citycloud.com:&lt;port&gt;/swift/v1/</code>),</li> <li>the container\u2019s account string, starting with <code>AUTH_</code>,</li> <li>the container name (in our example, <code>public-container</code>),</li> <li>the object name (in our example, <code>testobj.txt</code>).</li> </ol> <p>Rather than composing the public URL manually, you can also retrieve it by parsing the CLI\u2019s debug output:</p> OpenStack CLISwift CLI <pre><code>$ openstack object show --debug public-container testobj.txt 2&gt;&amp;1 \\\n  | grep -o \"https://.*testobj.txt\"\nhttps://swift-fra1.citycloud.com:8080/swift/v1/AUTH_30a7768a0ffc40359d6110f21a6e7d88/public-container/testobj.txt\nhttps://swift-fra1.citycloud.com:8080 \"HEAD /swift/v1/AUTH_30a7768a0ffc40359d6110f21a6e7d88/public-container/testobj.txt\nhttps://swift-fra1.citycloud.com:8080/swift/v1/AUTH_30a7768a0ffc40359d6110f21a6e7d88/public-container/testobj.txt\n</code></pre> <pre><code>$ swift stat --debug public-container testobj.txt 2&gt;&amp;1 \\\n  | grep -o \"https://.*testobj.txt\"\nhttps://swift-fra1.citycloud.com:8080/swift/v1/AUTH_30a7768a0ffc40359d6110f21a6e7d88/public-container/testobj.txt\n</code></pre> <p>Once you have retrieved your public URL, you can fetch the object\u2019s contents using the client of your choice. This example uses <code>curl</code>:</p> <pre><code>$ curl https://swift-fra1.citycloud.com:8080/swift/v1/AUTH_30a7768a0ffc40359d6110f21a6e7d88/public-container/testobj.txt\nhello world\n</code></pre>"},{"location":"howto/object-storage/swift/public-container/#public-bucket-accessibility-via-the-s3-api","title":"Public bucket accessibility via the S3 API","text":"<p>Once you make a container public via the Swift API, its objects also become accessible via the corresponding S3 API path.</p> <p>Thus, the following URL paths allow you to retrieve the same public object:</p> <ul> <li><code>https://swift-fra1.citycloud.com:8080/swift/v1/AUTH_30a7768a0ffc40359d6110f21a6e7d88/public-container/testobj.txt</code></li> <li><code>https://s3-fra1.citycloud.com:8080/30a7768a0ffc40359d6110f21a6e7d88:public-container/testobj.txt</code></li> </ul>"},{"location":"howto/object-storage/swift/tempurl/","title":"Using temporary URLs","text":"<p>Even though an object might be stored in a private container, you may still grant temporary access to it. This is known as a temporary URL, or TempURL.</p>"},{"location":"howto/object-storage/swift/tempurl/#prerequisites","title":"Prerequisites","text":"<p>In order to manage TempURLs, be sure that you have installed and configured the <code>swift</code> command-line interface (CLI). There is presently no way to create TempURLs with the <code>openstack</code> CLI.</p> <p>Also, ensure that you have configured a private container, i.e. one with an empty Read ACL. The examples in this how-to guide assume that your container is named <code>private-container</code>.</p>"},{"location":"howto/object-storage/swift/tempurl/#setting-a-tempurl-shared-secret","title":"Setting a TempURL shared secret","text":"<p>In order to be able to create TempURLs, you must first create a shared secret at the account level. You should create a secret that is hard to guess, such as one generated by a utility like <code>pwgen</code>:</p> <pre><code>TEMP_URL_KEY=`pwgen 32 1`\n</code></pre> <p>To set the account-level secret, proceed with the following command:</p> OpenStack CLISwift CLI <pre><code>$ openstack object store account set --property Temp-URL-Key=${TEMP_URL_KEY}\n</code></pre> <p><pre><code>$ swift post -m Temp-Url-Key:${TEMP_URL_KEY}\n</code></pre> Note that since this an account-level setting, you invoke <code>swift\u00a0post</code> without a container or object name.</p> <p>The TempURL secret is not encrypted or hashed; you can read it back at the account level with the following command:</p> OpenStack CLISwift CLI <pre><code>$ openstack object store account show\n+------------+-------------------------------------------------+\n| Field      | Value                                           |\n+------------+-------------------------------------------------+\n| Account    | AUTH_30a7768a0ffc40359d6110f21a6e7d88           |\n| Bytes      | 24                                              |\n| Containers | 2                                               |\n| Objects    | 2                                               |\n| properties | temp-url-key='tooNgeiNgieJe6bohg7teik8eiDeeMai' |\n+------------+-------------------------------------------------+\n</code></pre> <pre><code>$ swift stat\n                                    Account: AUTH_30a7768a0ffc40359d6110f21a6e7d88\n                                 Containers: 2\n                                    Objects: 2\n                                      Bytes: 24\nObjects in policy \"default-placement-bytes\": 0\n  Bytes in policy \"default-placement-bytes\": 0\n   Containers in policy \"default-placement\": 2\n      Objects in policy \"default-placement\": 2\n        Bytes in policy \"default-placement\": 24\n                          Meta Temp-Url-Key: tooNgeiNgieJe6bohg7teik8eiDeeMai\n                                X-Timestamp: 1670245963.98328\n                X-Account-Bytes-Used-Actual: 8192\n                                 X-Trans-Id: tx00000fbce1bedc1e2b138-00638dee4b-301ddeb-default\n                     X-Openstack-Request-Id: tx00000fbce1bedc1e2b138-00638dee4b-301ddeb-default\n                              Accept-Ranges: bytes\n                               Content-Type: text/plain; charset=utf-8\n</code></pre>"},{"location":"howto/object-storage/swift/tempurl/#creating-a-tempurl-for-an-object","title":"Creating a TempURL for an object","text":"<p>To create a temporary URL for an object in a private container, select a duration for which you want it to be valid. The example below uses 1\u00a0hour (3,600\u00a0seconds).</p> <p>Then, use <code>swift tempurl</code> and specify</p> <ul> <li>the HTTP method for which the TempURL should apply (usually <code>GET</code>),</li> <li>the TempURL lifetime, in seconds,</li> <li>the full path to the object including</li> <li>the <code>/v1</code> prefix,</li> <li>the account identifier starting with <code>AUTH_</code>,</li> <li>the container name,</li> <li>the object name,</li> <li>the TempURL key.</li> </ul> <p>When specified in this way, the command returns a path similar to the following:</p> <pre><code>$ swift tempurl GET 3600 \\\n    /v1/AUTH_30a7768a0ffc40359d6110f21a6e7d88/private-container/testobj.txt     \\\n    tooNgeiNgieJe6bohg7teik8eiDeeMai\n/v1/AUTH_30a7768a0ffc40359d6110f21a6e7d88/private-container/testobj.txt?temp_url_sig=995d136bf2a8b1140d4b26886c9a8fc73bfb6c0d&amp;temp_url_expires=1670250048\n</code></pre>"},{"location":"howto/object-storage/swift/tempurl/#accessing-objects-via-their-tempurl","title":"Accessing objects via their TempURL","text":"<p>You must then use your freshly generated TempURL path as the path in a URL pointing to the object. This will enable you to fetch the object using a simple HTTP client, like <code>curl</code>:</p> <pre><code>$ curl 'https://swift-fra1.citycloud.com:8080/swift/v1/AUTH_30a7768a0ffc40359d6110f21a6e7d88/private-container/testobj.txt?temp_url_sig=995d136bf2a8b1140d4b26886c9a8fc73bfb6c0d&amp;temp_url_expires=1670250048'\nhello world\n</code></pre> <p>If you (or someone else) were to attempt to fetch the same URL after its lifetime expired, they would be met with an HTTP 401 error:</p> <pre><code>$ curl -i 'https://swift-fra1.citycloud.com:8080/swift/v1/AUTH_30a7768a0ffc40359d6110f21a6e7d88/private-container/testobj.txt?temp_url_sig=995d136bf2a8b1140d4b26886c9a8fc73bfb6c0d&amp;temp_url_expires=1670250048'\nHTTP/1.1 401 Unauthorized\ncontent-length: 12\nx-trans-id: tx0000001113c5020d8a1de-00638df0ea-301ddeb-default\nx-openstack-request-id: tx0000001113c5020d8a1de-00638df0ea-301ddeb-default\naccept-ranges: bytes\ncontent-type: text/plain; charset=utf-8\ndate: Mon, 05 Dec 2022 14:23:54 GMT\n</code></pre>"},{"location":"howto/object-storage/swift/utilization/","title":"Object storage utilization","text":"<p>You may be interested in the number of objects, or their cumulative size, currently held in a container.</p>"},{"location":"howto/object-storage/swift/utilization/#prerequisites","title":"Prerequisites","text":"<p>To examine a Swift container, be sure that you have installed and configured the required command-line interface (CLI) tools.</p>"},{"location":"howto/object-storage/swift/utilization/#showing-the-number-of-objects-in-a-container","title":"Showing the number of objects in a container","text":"<p>To show the number of objects in a given container, use one of the following commands:</p> OpenStack CLISwift CLI <pre><code>openstack container show &lt;container&gt; -c object_count\n</code></pre> <p><pre><code>swift stat &lt;container&gt;\n</code></pre> The object count is in the line prefixed with <code>Objects</code>.</p>"},{"location":"howto/object-storage/swift/utilization/#showing-the-total-size-of-objects-in-a-container","title":"Showing the total size of objects in a container","text":"<p>To show the overall size of all objects in a given container, use one of the following commands:</p> OpenStack CLISwift CLI <p><pre><code>openstack container show &lt;container&gt; -c bytes_used\n</code></pre> The total object size is always given in bytes.</p> <p><pre><code>swift stat &lt;container&gt;\n</code></pre> The total object size is in the line prefixed with <code>Bytes</code>.</p> <p>To scale the total size output to human-readable units, add the <code>--lh</code> option:</p> <pre><code>swift stat --lh &lt;container&gt;\n</code></pre>"},{"location":"howto/object-storage/swift/versioning/","title":"Object versioning","text":"<p>Using the Swift API, you have the option to retain multiple versions of an object, rather than overwriting it in place.</p>"},{"location":"howto/object-storage/swift/versioning/#prerequisites","title":"Prerequisites","text":"<p>In order to manage object versioning, be sure that you have installed and configured the <code>swift</code> command-line interface (CLI). There is presently no way to set object expiry with the <code>openstack</code> CLI.</p> <p>Also, ensure that you have configured a private container, i.e. one with an empty Read ACL. The examples in this how-to guide assume that your container is named <code>private-container</code>, and that it presently contains a single object named <code>testobj.txt</code>.</p>"},{"location":"howto/object-storage/swift/versioning/#creating-a-container-for-non-current-versioned-objects","title":"Creating a container for non-current versioned objects","text":"<p>In addition to the container holding your live objects, <code>private-container</code>, you will also need one for your prior versions. In this example, we use the name <code>private-container-versions</code> for that container:</p> <p><pre><code>swift post private-container-versions\n</code></pre> This command produces no output.</p>"},{"location":"howto/object-storage/swift/versioning/#setting-x-versions-location","title":"Setting <code>X-Versions-Location</code>","text":"<p>You must next set the <code>X-Versions-Location</code> header on the original container:</p> <pre><code>swift post -H \"X-Versions-Location: private-container-versions\" private-container\n</code></pre>"},{"location":"howto/object-storage/swift/versioning/#testing-object-versioning","title":"Testing object versioning","text":"<p>You can now verify that object versioning is working correctly.</p> <ol> <li>Upload a new version of <code>testobj.txt</code>:    <pre><code>$ echo \"bye bye\" &gt; testobj.txt\n$ swift upload private-container testobj.txt\n</code></pre></li> <li>Observe that there is now a newly created object in the <code>private-container-versions</code> container:    <pre><code>$ swift list private-container-versions\n00btestobj.txt/1670250073.717985\n</code></pre></li> <li>Issue a command to delete the object:    <pre><code>$ swift delete private-container testobj.txt\ntestobj.txt\n</code></pre></li> <li>Read back the object.    Since the container is now versioned, the \u201cdeletion\u201d in step 3 only caused a roll-back to the object\u2019s prior state:    <pre><code>$ swift download -o - private-container testobj.txt\nhello world\n</code></pre></li> </ol> <p>Object versioning does not prevent deleting the last remaining (i.e. first created) version of an object. Do not use object versioning as a prevention mechanism for inadvertent object deletion; it is not suitable for that purpose.</p>"},{"location":"howto/openstack/barbican/","title":"Using Barbican for secret storage","text":"<p>Barbican is OpenStack\u2019s secret storage facility. In Cleura\u00a0Cloud, Barbican is supported for the following purposes:</p> <ul> <li>Generic secret storage,</li> <li>encryption for persistent volumes,</li> <li>certificate storage for HTTPS load balancers.</li> </ul> <p>To manage secrets with Barbican, you will need the <code>openstack</code> command line interface (CLI), and its Barbican plugin. You can install them both with the following commands:</p> <pre><code>pip install python-openstackclient python-barbicanclient\n</code></pre> <p>On Debian/Ubuntu platforms, you may also install these utilities via their APT packages:</p> <pre><code>apt install python3-openstackclient python3-barbicanclient\n</code></pre>"},{"location":"howto/openstack/barbican/generic-secret/","title":"Generic secret storage","text":"<p>The simplest way to use Barbican is to create and retrieve a securely stored, generic secret.</p>"},{"location":"howto/openstack/barbican/generic-secret/#how-to-store-a-generic-secret","title":"How to store a generic secret","text":"<p>It is possible to store any secret data with Barbican. The command below will create a secret of the type <code>passphrase</code>, named <code>mysecret</code>, which contains the passphrase <code>my very secret passphrase</code>.</p> <pre><code>openstack secret store \\\n  --secret-type passphrase \\\n  -p \"my very secret passphrase\" \\\n  -n mysecret\n</code></pre> <p>The example output below uses Cleura\u00a0Cloud\u2019s <code>Fra1</code> region. In other regions, the secret URIs will differ.</p> <pre><code>+---------------+--------------------------------------------------------------------------------+\n| Field         | Value                                                                          |\n+---------------+--------------------------------------------------------------------------------+\n| Secret href   | https://fra1.citycloud.com:9311/v1/secrets/33ef0985-f89e-4bf0-b318-887ecac0cba |\n| Name          | mysecret                                                                       |\n| Created       | None                                                                           |\n| Status        | None                                                                           |\n| Content types | None                                                                           |\n| Algorithm     | aes                                                                            |\n| Bit length    | 256                                                                            |\n| Secret type   | passphrase                                                                     |\n| Mode          | cbc                                                                            |\n| Expiration    | None                                                                           |\n+---------------+--------------------------------------------------------------------------------+\n</code></pre> <p>Note that <code>passphrase</code> type secrets are symmetrically encrypted, using the AES encryption algorithm with a 256-bit key length. You can select other bit lengths and algorithms with the <code>-b</code> and <code>-a</code> command line options, if desired.</p>"},{"location":"howto/openstack/barbican/generic-secret/#how-to-retrieve-secrets","title":"How to retrieve secrets","text":"<p>Secrets are stored in Barbican in an encrypted format. You can see a list of secrets created for your user with the following command:</p> <pre><code>$ openstack secret list\n+--------------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------------------+-----------+------------+-------------+------+------------+\n| Secret href                                                                    | Name     | Created                   | Status | Content types                           | Algorithm | Bit length | Secret type | Mode | Expiration |\n+--------------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------------------+-----------+------------+-------------+------+------------+\n| https://fra1.citycloud.com:9311/v1/secrets/33ef0985-f89e-4bf0-b318-887ecac0cba | mysecret | 2021-04-29T10:33:18+00:00 | ACTIVE | {'default': 'application/octet-stream'} | aes       |        256 | passphrase  | cbc  | None       |\n| https://fra1.citycloud.com:9311/v1/secrets/ad628532-53b8-4d2f-91e5-0097b51da4e | None     | 2021-04-27T13:52:10+00:00 | ACTIVE | {'default': 'application/octet-stream'} | aes       |        256 | symmetric   | None | None       |\n+--------------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------------------+-----------+------------+-------------+------+------------+\n</code></pre> <p>You can retrieve the decrypted secret with the <code>openstack secret get</code> command, adding the <code>-p</code> (or <code>--payload</code>) option:</p> <pre><code>$ openstack secret get -p \\\n  https://fra1.citycloud.com:9311/v1/secrets/33ef0985-f89e-4bf0-b318-887ecac0cba\n+---------+---------------------------+\n| Field   | Value                     |\n+---------+---------------------------+\n| Payload | my very secret passphrase |\n+---------+---------------------------+\n</code></pre> <p>Unlike many other OpenStack services, which allow you to retrieve object references by name or UUID, Barbican only lets you retrieve secrets by their full URI. That URI must include the <code>https://&lt;region&gt;.citycloud.com:9311/v1/secrets/</code> prefix.</p>"},{"location":"howto/openstack/barbican/share-secret/","title":"Sharing secrets via ACLs","text":"<p>Normally, a Barbican secret is only available to the OpenStack API user that created it. However, under some circumstances it may be desirable to make a secret available to another user.</p> <p>To do so, you will need</p> <ul> <li>the secret\u2019s URI,</li> <li>the other user\u2019s OpenStack API user ID.</li> </ul> <p>Any Cleura\u00a0Cloud user can always retrieve their own user ID with the following command:</p> <pre><code>openstack token issue -f value -c user_id\n</code></pre> <p>Once you have this information, you can proceed with the <code>openstack acl user add</code> command:</p> <pre><code>openstack acl user add \\\n  --user &lt;user_id&gt; \\\n  --operation-type read \\\n  https://&lt;region&gt;.citycloud.com:9311/v1/secrets/&lt;secret_id&gt;\n</code></pre> <p>If you want to unshare the secret again, you use the corresponding <code>openstack acl user remove</code> command:</p> <pre><code>openstack acl user remove \\\n  --user &lt;user_id&gt; \\\n  --operation-type read \\\n  https://&lt;region&gt;.citycloud.com:9311/v1/secrets/&lt;secret_id&gt;\n</code></pre>"},{"location":"howto/openstack/cinder/encrypted-volumes/","title":"Encrypted volumes","text":"<p>Using Barbican secrets for block storage encryption, you can store data in persistent storage volumes in an encrypted fashion.</p> <p>That encryption is transparent to virtual machines (instances) that you attach the volume to.</p>"},{"location":"howto/openstack/cinder/encrypted-volumes/#creating-an-encrypted-volume","title":"Creating an encrypted volume","text":"<p>For the creation of an encrypted volume, you need to provide a specific volume type. You can retrieve the list of available volume types with the following command:</p> <pre><code>$ openstack volume type list\n+--------------------------------------+-----------------------+-----------+\n| ID                                   | Name                  | Is Public |\n+--------------------------------------+-----------------------+-----------+\n| a479a6b0-b283-41a5-b38b-5b08e7f902ca | cbs-encrypted         | True      |\n| d9dfa98a-238d-4ca0-9abf-701fceb05623 | __DEFAULT__           | True      |\n| 86796611-fb12-4628-b6b1-e09469e301d7 | cbs                   | True      |\n+--------------------------------------+-----------------------+-----------+\n</code></pre> <p>In Cleura\u00a0Cloud, all volume types that support encryption use the suffix <code>_encrypted</code>.</p> <p>To create a volume with encryption, you need to explicitly specify the <code>--type</code> option to the <code>openstack volume create</code> command. The following example creates a volume using the <code>cbs-encrypted</code> type, naming it <code>enc_drive</code> and setting its size to 10 GiB:</p> <pre><code>$ openstack volume create \\\n  --type cbs-encrypted \\\n  --size 10 \\\n  enc_drive\n+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| attachments         | []                                   |\n| availability_zone   | nova                                 |\n| bootable            | false                                |\n| consistencygroup_id | None                                 |\n| created_at          | 2021-04-27T13:52:10.000000           |\n| description         | None                                 |\n| encrypted           | True                                 |\n| id                  | 33211b21-8d4f-48e9-b76f-ec73ffd19def |\n| multiattach         | False                                |\n| name                | enc_drive                            |\n| properties          |                                      |\n| replication_status  | None                                 |\n| size                | 10                                   |\n| snapshot_id         | None                                 |\n| source_volid        | None                                 |\n| status              | creating                             |\n| type                | cbs-encrypted                        |\n| updated_at          | None                                 |\n| user_id             | 966ad341f4e14920b5f589f900246ccc     |\n+---------------------+--------------------------------------+\n</code></pre> <p>Upon volume creation, this will create a one-off encryption key, which is stored in Barbican and applies to this one volume only. In other words, the key created for this volume will be unable to decrypt any other volumes except the one it was created for.</p>"},{"location":"howto/openstack/cinder/encrypted-volumes/#retrieving-a-volumes-encryption-key","title":"Retrieving a volume\u2019s encryption key","text":"<p>Once you have created an encrypted volume, you may retrieve a reference to the Barbican secret that represents its encryption key. You do this with the following command:</p> <pre><code>openstack volume show \\\n  --os-volume-api-version 3.66 \\\n  -f value \\\n  -c encryption_key_id \\\n  enc_drive\n</code></pre> <p>Instead of the volume name, you can of course also specify its UUID:</p> <pre><code>openstack volume show \\\n  --os-volume-api-version 3.66 \\\n  -f value \\\n  -c encryption_key_id \\\n  33211b21-8d4f-48e9-b76f-ec73ffd19def\n</code></pre>"},{"location":"howto/openstack/cinder/encrypted-volumes/#deleting-an-encrypted-volume","title":"Deleting an encrypted volume","text":"<p>When you decide you no longer need an encrypted volume and want to delete it, you can do so with the <code>openstack volume delete</code> command. As long as you do this with the same user account as the one that created the volume, this will succeed without further intervention.</p> <p>However, if you are trying to delete a volume that was created by a different user, you\u2019ll run into the limitation that the secret associated with the volume is owned by that user. As a result, the deletion of the encrypted volume using your own user credentials will fail.</p> <p>There are two options to work around this limitation:</p> <ol> <li>You can switch to the user credentials of the user that created the volume (if you have access to them), and proceed with the deletion.</li> <li>You can ask the user that created the volume to add you to the Access Control List (ACL) for the secret.    This will enable you to read the secret, and to delete the volume using your own credentials.</li> </ol>"},{"location":"howto/openstack/cinder/encrypted-volumes/#block-device-encryption-caveats","title":"Block device encryption caveats","text":"<p>Once a volume is configured to use encryption and is also attached to an instance in Cleura\u00a0Cloud, some caveats apply that you might want to keep in mind.</p> <p>Sometimes, automatically or through administrator intervention, we move one of your instances to another physical machine. This process is known as live migration, and it normally does not interrupt the instance\u2019s functionality at all \u2014 typically, neither you nor the application users notice that live migration has even happened. This is a very common occurrence when we do routine upgrades of the Cleura\u00a0Cloud platform, during our pre-announced maintenance windows.</p> <p>The same considerations apply to physical node failure. If the physical machine running your instance fails, we can automatically recover it onto another machine \u2014 an action known as evacuation.</p> <p>Live migration or evacuation including encrypted volumes does, however, require that whoever does the migration also has at least read access to the volume\u2019s encryption secret.</p> <p>This means that you have two options:</p> <ol> <li>If you do trust us to include your instances in live migrations and evacuations, even if they attach encrypted volumes, then you can add our administrative account to the Access Control List (ACL) for your secrets.</li> <li>If you don\u2019t want to share your secrets but you still want to use encrypted volumes, you should build your own mechanism or process (preferably automated) so that your instances recover in case they become non-functional.</li> </ol>"},{"location":"howto/openstack/cinder/resize-volumes/","title":"Resizing a volume","text":"<p>A server may require additional capacity for a persistent storage volume that is attached to it. You can expand a volume online, while it is attached to a running server.</p>"},{"location":"howto/openstack/cinder/resize-volumes/#prerequisites","title":"Prerequisites","text":"<p>In order to expand a volume online (that is, without detaching it from its server), you must use the OpenStack CLI. Make sure you have enabled it.</p>"},{"location":"howto/openstack/cinder/resize-volumes/#checking-the-volumes-state","title":"Checking the volume\u2019s state","text":"<p>Assume you have a volume named <code>testvol</code> that is currently attached to a server named <code>testsrv</code>:</p> <pre><code>$ openstack volume list\n+-----------------------------------+---------+--------+------+----------------------------------+\n| ID                                | Name    | Status | Size | Attached to                      |\n+-----------------------------------+---------+--------+------+----------------------------------+\n| 357e1022-4156-4383-a49d-729f86e84 | testvol | in-use |    2 | Attached to testsrv on /dev/vdb  |\n| 347                               |         |        |      |                                  |\n+-----------------------------------+---------+--------+------+----------------------------------+\n</code></pre> <p>As you can see, this volume\u2019s status is <code>in-use</code>, meaning it is currently attached to a server, and the volume\u2019s size is 2\u00a0GiB.</p> <p>You can also use <code>openstack\u00a0server\u00a0ssh</code> to verify the current state of the block device. In this example, the virtual block device <code>/dev/vdb</code> contains an XFS file system and is mounted to <code>/srv</code>:</p> <pre><code>$ openstack server ssh testsrv -- -l ubuntu \"mount | grep /dev/vdb\"\n/dev/vdb on /srv type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)\n</code></pre>"},{"location":"howto/openstack/cinder/resize-volumes/#expanding-the-volume","title":"Expanding the volume","text":"<p>You can now change the volume\u2019s size (in this example, we\u2019ll use a new size of 5\u00a0GiB). Note that in order to be able to do this while the volume is attached to a server, you must invoke the <code>openstack</code> command with the <code>--os-volume-api-version</code> flag:</p> <pre><code>$ openstack --os-volume-api-version 3.42 volume set --size 5 testvol\n\n$ openstack volume list \n+-----------------------------------+---------+--------+------+----------------------------------+\n| ID                                | Name    | Status | Size | Attached to                      |\n+-----------------------------------+---------+--------+------+----------------------------------+\n| 357e1022-4156-4383-a49d-729f86e84 | testvol | in-use |    5 | Attached to testsrv on /dev/vdb  |\n| 347                               |         |        |      |                                  |\n+-----------------------------------+---------+--------+------+----------------------------------+\n</code></pre> <p>When using one of the VirtIO storage drivers, the operating system on the server immediately becomes aware of the new device size. There is no need to rescan the block devices from within the guest operating system.</p> <p>You can verify this by using the <code>blockdev</code> command in a shell session, querying the block device size in bytes: <pre><code>$ openstack server ssh testsrv -- -l ubuntu \"sudo blockdev --getsize64 /dev/vdb\"\n5368709120\n</code></pre></p>"},{"location":"howto/openstack/cinder/resize-volumes/#using-the-resized-volume","title":"Using the resized volume","text":"<p>Resizing a volume only resizes the block device, but does not touch any data structures using that block device. In our example, this means that in order to use the block device\u2019s expanded capacity, you must also resize its filesystem:</p> <pre><code>$ openstack server ssh testsrv -- -l ubuntu \"sudo xfs_growfs /dev/vdb\"\nmeta-data=/dev/vdb               isize=512    agcount=4, agsize=131072 blks\n         =                       sectsz=512   attr=2, projid32bit=1\n         =                       crc=1        finobt=1, sparse=1, rmapbt=0\n         =                       reflink=1\ndata     =                       bsize=4096   blocks=524288, imaxpct=25\n         =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0, ftype=1\nlog      =internal log           bsize=4096   blocks=2560, version=2\n         =                       sectsz=512   sunit=0 blks, lazy-count=1\nrealtime =none                   extsz=4096   blocks=0, rtextents=0\ndata blocks changed from 524288 to 1310720\n</code></pre> <p>Once this is complete, you can verify that the filesystem is now available at the desired (expanded) capacity:</p> <pre><code>$ openstack server ssh testsrv -- -l ubuntu \"df -h /srv\"\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/vdb        5.0G   69M  5.0G   2% /srv\n</code></pre>"},{"location":"howto/openstack/cinder/retype-volumes/","title":"Changing a volume\u2019s type","text":"<p>You may occasionally need to change the type of a volume. This may be due to a volume type being phased out on Cleura\u00a0Cloud\u2019s part, necessitating data migration. Or you might want to enable or disable volume-level encryption.</p> <p>Changing the type of a volume (\u201cretyping\u201d) is an offline operation that requires detaching the volume from its server, and setting its new type. The resulting downtime may be quite substantial, particularly for large volumes. As such, when you need to retype volumes, you should plan ahead well in advance.</p>"},{"location":"howto/openstack/cinder/retype-volumes/#prerequisites","title":"Prerequisites","text":"<p>In order to retype volumes, you must use the OpenStack CLI, so make sure you have it enabled.</p> <p>If you are about to retype a large volume, or one that holds data associated with a critical service, you may be interested in an estimate of how long the retype operation will take. In that case, please file a support request with our Service\u00a0Center.</p>"},{"location":"howto/openstack/cinder/retype-volumes/#checking-the-volumes-state","title":"Checking the volume\u2019s state","text":"<p>Assume you have a volume named <code>testvol</code> that is currently attached to a server named <code>testsrv</code>:</p> <pre><code>$ openstack volume list --long\n+----------------+---------+--------+------+---------+----------+----------------+------------+\n| ID             | Name    | Status | Size | Type    | Bootable | Attached to    | Properties |\n+----------------+---------+--------+------+---------+----------+----------------+------------+\n| e233e7f3-f33b- | testvol | in-use |   50 | default | false    | Attached to    |            |\n| 4d7a-8f5b-785b |         |        |      |         |          | testsrv on     |            |\n| 34f670bf       |         |        |      |         |          | /dev/vdb       |            |\n+----------------+---------+--------+------+---------+----------+----------------+------------+\n</code></pre> <p>In this example, this volume status is <code>in-use</code>, meaning it is currently attached to a server, and the volume type is <code>default</code>.</p>"},{"location":"howto/openstack/cinder/retype-volumes/#detaching-the-volume","title":"Detaching the volume","text":"<p>You cannot retype a volume while it is attached to a server. You must thus detach it first. It is safest to do this while the server is shut down:</p> <pre><code>$ openstack server stop testsrv\n\n$ openstack server list -c Name -c Status\n+---------+---------+\n| Name    | Status  |\n+---------+---------+\n| testsrv | SHUTOFF |\n+---------+---------+\n</code></pre> <p>Once your server is in the <code>SHUTOFF</code> state, you can safely proceed to detaching the volume. This will change the volume status from <code>in-use</code> to <code>available</code>.</p> <pre><code>$ openstack server remove volume testsrv testvol\n\n$ openstack volume list\n+--------------------------------------+---------+-----------+------+-------------+\n| ID                                   | Name    | Status    | Size | Attached to |\n+--------------------------------------+---------+-----------+------+-------------+\n| e233e7f3-f33b-4d7a-8f5b-785b34f670bf | testvol | available |   50 |             |\n+--------------------------------------+---------+-----------+------+-------------+\n</code></pre>"},{"location":"howto/openstack/cinder/retype-volumes/#retyping-the-volume","title":"Retyping the volume","text":"<p>With the volume safely detached, you can now change its volume type. If you were to change the volume type from <code>default</code> to <code>ceph_hdd</code>, you would proceed as follows:</p> <pre><code>$ openstack volume set --type ceph_hdd --retype-policy on-demand testvol\n\n$ openstack volume list --long\n+---------------+---------+----------+------+---------+----------+-------------+------------+\n| ID            | Name    | Status   | Size | Type    | Bootable | Attached to | Properties |\n+---------------+---------+----------+------+---------+----------+-------------+------------+\n| e233e7f3-f33b | testvol | retyping |   50 | default | false    |             |            |\n| -4d7a-8f5b-78 |         |          |      |         |          |             |            |\n| 5b34f670bf    |         |          |      |         |          |             |            |\n+---------------+---------+----------+------+---------+----------+-------------+------------+\n</code></pre> <p>Note that the volume status changes from <code>available</code> to <code>retyping</code>: this status change kicks off the actual data migration, which might take a significant amount of time.</p>"},{"location":"howto/openstack/cinder/retype-volumes/#re-attaching-the-volume","title":"Re-attaching the volume","text":"<p>If you attempt to re-attach the volume while it is still retyping, you will receive an error:</p> <pre><code>$ openstack server add volume testsrv testvol\nBadRequestException: 400: Client Error for url: https://sto2.citycloud.com:8774/v2.1/servers/b6a26b0e-f911-4ea2-8e45-51f16442da03/os-volume_attachments, Invalid input received: Invalid volume: Volume e233e7f3-f33b-4d7a-8f5b-785b34f670bf status must be available or downloading to reserve, but the current status is retyping. (HTTP 400) (Request-ID: req-36c4f2e5-ef2f-4ff4-b912-0baed2594f4d)\n</code></pre> <p>You must now wait until the volume status changes back from <code>retyping</code> to <code>available</code>. One way to do this is with a bash <code>until</code> loop:</p> <pre><code>until [ `openstack volume show -f value -c status testvol` = \"available\" ]; do\n  sleep 5\ndone\n</code></pre> <p>Once the volume has returned to the <code>available</code> status, you can re-attach it to the server:</p> <pre><code>$ openstack server add volume testsrv testvol\n+-----------------------+--------------------------------------+\n| Field                 | Value                                |\n+-----------------------+--------------------------------------+\n| ID                    | d2a22868-a133-40a1-b1a6-0cbae3feaf8d |\n| Server ID             | 23a391f7-57ba-4c7f-bdd1-d1b89d6e39b2 |\n| Volume ID             | e233e7f3-f33b-4d7a-8f5b-785b34f670bf |\n| Device                | /dev/vdb                             |\n| Tag                   | None                                 |\n| Delete On Termination | False                                |\n+-----------------------+--------------------------------------+\n</code></pre> <p>Finally, restart the server:</p> <pre><code>$ openstack server start testsrv\n\n$ openstack server list -c Name -c Status\n+---------+--------+\n| Name    | Status |\n+---------+--------+\n| testsrv | ACTIVE |\n+---------+--------+\n</code></pre>"},{"location":"howto/openstack/cinder/sync-volumes/","title":"Transferring data between volumes","text":"<p>From time to time, you may want to transfer data from one persistent storage volume to another, while keeping your data within the same Cleura\u00a0Cloud region.</p> <p>For example, you might prefer to select a volume type that has become newly available in that region, but find the downtime associated with retyping a single volume prohibitive. In this case, you can choose an on-line synchronization approach, which comes with much reduced downtime.</p> <p>The process described here assumes that the volume whose contents you are about to transfer is not a boot volume \u2014 in other words, that the volume is normally attached as <code>/dev/vdb</code> or <code>/dev/sdc</code> or similar, but not as <code>/dev/vda</code> or <code>/dev/sda</code>. If the volume you need to retype is a boot volume, you should plan system downtime and opt for an offline retype instead.</p>"},{"location":"howto/openstack/cinder/sync-volumes/#prerequisites","title":"Prerequisites","text":"<p>Creating volumes from snapshots (and optionally retyping them) requires using the OpenStack CLI, so make sure you have it enabled.</p>"},{"location":"howto/openstack/cinder/sync-volumes/#checking-the-source-volumes-state","title":"Checking the source volume\u2019s state","text":"<p>Assume you have a volume named <code>sourcevol</code> that is currently attached to a server named <code>testsrv</code>:</p> <pre><code>$ openstack volume list --long\n+---------------+-----------+--------+------+---------+----------+---------------+------------+\n| ID            | Name      | Status | Size | Type    | Bootable | Attached to   | Properties |\n+---------------+-----------+--------+------+---------+----------+---------------+------------+\n| 526f7741-2c44 | sourcevol | in-use |   50 | default | false    | Attached to   |            |\n| -4d04-a0c8-86 |           |        |      |         |          | testsrv on    |            |\n| b8d039e674    |           |        |      |         |          | /dev/vdb      |            |\n+---------------+-----------+--------+------+---------+----------+---------------+------------+\n</code></pre> <p>In this example, the volume status is <code>in-use</code> (meaning the volume is currently attached to a server), and the volume type is <code>default</code>.</p>"},{"location":"howto/openstack/cinder/sync-volumes/#taking-a-snapshot-of-the-source-volume","title":"Taking a snapshot of the source volume","text":"<p>First take a snapshot (a consistent, read-only, point-in-time copy) of your source volume. Note that since you are taking a snapshot of an <code>in-use</code> volume, you need to use the <code>--force</code> option with the following command:</p> <pre><code>$ openstack volume snapshot create --force --volume sourcevol sourcevol-snap\n+-------------+--------------------------------------+\n| Field       | Value                                |\n+-------------+--------------------------------------+\n| created_at  | 2023-01-05T14:34:46.409705           |\n| description | None                                 |\n| id          | 05ece580-4fb0-45dd-96e8-06a46399c38e |\n| name        | sourcevol-snap                       |\n| properties  |                                      |\n| size        | 50                                   |\n| status      | creating                             |\n| updated_at  | None                                 |\n| volume_id   | 526f7741-2c44-4d04-a0c8-86b8d039e674 |\n+-------------+--------------------------------------+\n</code></pre> <p>The snapshot status should change from <code>creating</code> to <code>available</code> in a matter of seconds. You can subsequently read back its state with the following command:</p> <pre><code>$ openstack volume snapshot show sourcevol-snap\n+--------------------------------------------+--------------------------------------+\n| Field                                      | Value                                |\n+--------------------------------------------+--------------------------------------+\n| created_at                                 | 2023-01-05T14:34:46.000000           |\n| description                                | None                                 |\n| id                                         | 05ece580-4fb0-45dd-96e8-06a46399c38e |\n| name                                       | sourcevol-snap                       |\n| properties                                 |                                      |\n| size                                       | 50                                   |\n| status                                     | available                            |\n| updated_at                                 | 2023-01-05T14:34:49.000000           |\n| volume_id                                  | 526f7741-2c44-4d04-a0c8-86b8d039e674 |\n+--------------------------------------------+--------------------------------------+\n</code></pre>"},{"location":"howto/openstack/cinder/sync-volumes/#creating-the-target-volume","title":"Creating the target volume","text":"<p>Once you have created a snapshot of your source volume, you can use it to preseed your target volume:</p> <pre><code>$ openstack volume create --snapshot sourcevol-snap targetvol\n+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| attachments         | []                                   |\n| availability_zone   | nova                                 |\n| bootable            | false                                |\n| consistencygroup_id | None                                 |\n| created_at          | 2023-01-05T14:44:10.132096           |\n| description         | None                                 |\n| encrypted           | False                                |\n| id                  | c8e0ff43-f837-478b-ad42-1bdb20dbbdb3 |\n| multiattach         | False                                |\n| name                | targetvol                            |\n| properties          |                                      |\n| replication_status  | None                                 |\n| size                | 50                                   |\n| snapshot_id         | 05ece580-4fb0-45dd-96e8-06a46399c38e |\n| source_volid        | None                                 |\n| status              | creating                             |\n| type                | default                              |\n| updated_at          | None                                 |\n| user_id             | 51ce99c11f9e4ed08e92acca176c33ca     |\n+---------------------+--------------------------------------+\n</code></pre> <p>You must now wait until the volume status changes from <code>creating</code> to <code>available</code>. One way to do this is with a bash <code>until</code> loop:</p> <pre><code>until [ `openstack volume show -f value -c status targetvol` = \"available\" ]; do\n  sleep 5\ndone\n</code></pre>"},{"location":"howto/openstack/cinder/sync-volumes/#retyping-the-target-volume-optional","title":"Retyping the target volume (optional)","text":"<p>If you want to retain the current type of your target volume, you can safely skip this step.</p> <p>If you do need to select a different volume type for your target volume (see the relevant how-to guide for details on retyping), now is the time to do so. Set the new volume type, and then wait for the retype operation to complete.</p> <pre><code>$ openstack volume set --type &lt;new-type&gt; --retype-policy on-demand targetvol\n\n$ until [ `openstack volume show -f value -c status targetvol` = \"available\" ]; do\n  sleep 5\ndone\n</code></pre>"},{"location":"howto/openstack/cinder/sync-volumes/#attaching-the-target-volume","title":"Attaching the target volume","text":"<p>You can now attach the target volume to your server:</p> <pre><code>$ openstack server add volume testsrv targetvol\n+-----------------------+--------------------------------------+\n| Field                 | Value                                |\n+-----------------------+--------------------------------------+\n| ID                    | d2a22868-a133-40a1-b1a6-0cbae3feaf8d |\n| Server ID             | 23a391f7-57ba-4c7f-bdd1-d1b89d6e39b2 |\n| Volume ID             | e233e7f3-f33b-4d7a-8f5b-785b34f670bf |\n| Device                | /dev/vdc                             |\n| Tag                   | None                                 |\n| Delete On Termination | False                                |\n+-----------------------+--------------------------------------+\n</code></pre>"},{"location":"howto/openstack/cinder/sync-volumes/#synchronizing-data-between-the-source-and-target-volume","title":"Synchronizing data between the source and target volume","text":"<p>At this point, your server contains current data on the source volume (<code>sourcevol</code>), and outdated data on the target volume (<code>targetvol</code>), since your application has continued to write data since you took the snapshot.</p> <p>Thus, you must now conduct a final synchronization of your data. How you do this precisely depends on your workload, but certain rules of thumb apply based on the guest operating system.</p> Linux/BSD/UnixWindows <ol> <li>Mount the device corresponding to the target volume to a temporary path.    This may entail that you make some modifications to the filesystem prior to mounting.    For example, an XFS filesystem will need a new UUID, which you can set with <code>xfs_admin\u00a0-U\u00a0generate\u00a0&lt;device&gt;</code></li> <li>Synchronize your data between the source volume\u2019s mount point and the target volume\u2019s temporary one, for example:    <pre><code>rsync -av /srv/data /mnt\n</code></pre>    You can repeat this step as often as necessary.</li> <li>Stop any services accessing data on the source volume (this marks the start of your migration downtime).</li> <li>Run a final synchronization:    <pre><code>rsync -av /srv/data /mnt\n</code></pre></li> <li>Unmount the source volume.</li> <li>Remount the target volume to the source volume\u2019s prior mount point.</li> <li>Start the service accessing data on the target volume (this marks the end of your migration downtime).</li> </ol> <ol> <li>Assign a drive letter to the device corresponding to the target volume (this example assumes <code>E:</code>)</li> <li>Synchronize your data between the source volume\u2019s drive letter (this example assumes <code>D:</code>) and the target volume\u2019s temporary one, for example:    <pre><code>%SystemRoot%\\system32\\robocopy.exe D: E: /MT:16 /R:0 /W:0 /ZB /NP /COPYALL /DCOPY:T /MIR /NFL /NDL /XJD /XO\n</code></pre>    You can repeat this step as often as necessary.</li> <li>Stop any services accessing data on the source volume (this marks the start of your migration downtime).</li> <li>Run a final synchronization:    <pre><code>%SystemRoot%\\system32\\robocopy.exe D: E: /MT:16 /R:0 /W:0 /ZB /NP /COPYALL /DCOPY:T /MIR /NFL /NDL /XJD /XO\n</code></pre></li> <li>Unassign the drive letter (<code>D:</code>) from the device corresponding to the source volume.</li> <li>Change the drive letter of the device corresponding to the target volume (<code>E:</code>) to that previously used by the device corresponding to the source volume (<code>D:</code>)</li> <li>Start the service accessing data on the target volume (this marks the end of your migration downtime).</li> </ol>"},{"location":"howto/openstack/cinder/sync-volumes/#detaching-the-source-volume","title":"Detaching the source volume","text":"<p>Finally, detach the source volume from the server.</p> <pre><code>$ openstack server remove volume testsrv sourcevol\n</code></pre>"},{"location":"howto/openstack/cinder/sync-volumes/#marking-the-source-volume-read-only-optional","title":"Marking the source volume read-only (optional)","text":"<p>If you do not want to delete the source volume straight away, but retain it as a backup in case anything has gone wrong in the migration, it makes good sense to mark it as read-only. That way, if the source volume is accidentally attached to a server, its data cannot be modified.</p> <pre><code>$ openstack volume set --read-only sourcevol\n$ openstack volume show sourcevol\n+------------------------------+--------------------------------------+\n| Field                        | Value                                |\n+------------------------------+--------------------------------------+\n| attachments                  | []                                   |\n| availability_zone            | nova                                 |\n| bootable                     | false                                |\n| consistencygroup_id          | None                                 |\n| created_at                   | 2023-01-05T14:00:40.000000           |\n| description                  | None                                 |\n| encrypted                    | False                                |\n| id                           | 526f7741-2c44-4d04-a0c8-86b8d039e674 |\n| multiattach                  | False                                |\n| name                         | sourcevol                            |\n| properties                   | readonly='True'                      |\n| replication_status           | None                                 |\n| size                         | 50                                   |\n| snapshot_id                  | None                                 |\n| source_volid                 | None                                 |\n| status                       | available                            |\n| type                         | default                              |\n| updated_at                   | 2023-01-05T16:03:55.000000           |\n| user_id                      | 51ce99c11f9e4ed08e92acca176c33ca     |\n+------------------------------+--------------------------------------+\n</code></pre>"},{"location":"howto/openstack/glance/","title":"Managing operating system images with Glance","text":"<p>Glance is OpenStack\u2019s operating system image management facility. In Cleura\u00a0Cloud, you use Glance images to rapidly create servers using any of our supported base images.</p>"},{"location":"howto/openstack/glance/custom/","title":"Managing custom images","text":"<p>You can use the <code>openstack</code> CLI to upload a custom image to any of the regions you have access to. In addition to that, you can create an image from a server\u2019s boot volume.</p> <p>Whenever possible, you should use the supported base images instead of creating your own.</p>"},{"location":"howto/openstack/glance/custom/#uploading-a-custom-image","title":"Uploading a custom image","text":"<p>To upload a custom image, use <code>openstack</code> like so:</p> <pre><code>openstack image create --disk-format &lt;format&gt; --file &lt;local-filename&gt; &lt;image-name&gt;\n</code></pre> <p>For instance, see how to upload a Debian 13 cloud image, which you can subsequently use to create cloud servers in Cleura\u00a0Cloud:</p> <pre><code>$ openstack image create \\\n    --disk-format qcow2 \\\n    --file debian-13-genericcloud-amd64-daily-20250623-2152.qcow2 \\\n    debian-13-daily\n\n+------------------+-------------------------------------------------------------------------------+\n| Field            | Value                                                                         |\n+------------------+-------------------------------------------------------------------------------+\n| checksum         | 5d073d57c0b89afa3a7a3352a42fd073                                              |\n| container_format | bare                                                                          |\n| created_at       | 2025-06-23T15:32:37Z                                                          |\n| disk_format      | qcow2                                                                         |\n| file             | /v2/images/b8e875da-cf0a-4732-ac12-5aad29adc2c0/file                          |\n| id               | b8e875da-cf0a-4732-ac12-5aad29adc2c0                                          |\n| min_disk         | 0                                                                             |\n| min_ram          | 0                                                                             |\n| name             | debian-13-daily-20250623-2152                                                 |\n| owner            | dfc700467396428bacba4376e72cc3e9                                              |\n| properties       | direct_url='rbd://2ae305d1-6742-4b1c-af69-825a3bae8b53/images/b8e875da-       |\n|                  | cf0a-4732-ac12-5aad29adc2c0/snap', locations='[{'url':                        |\n|                  | 'rbd://2ae305d1-6742-4b1c-af69-825a3bae8b53/images/b8e875da-                  |\n|                  | cf0a-4732-ac12-5aad29adc2c0/snap', 'metadata': {'store': 'rbd'}}]',           |\n|                  | os_hash_algo='sha512', os_hash_value='8f7c42267ca2b592f5d0b4d2155951e4cbc33be |\n|                  | 0c9e64063a986424b123ac15acfba33c93a14211ba2071f5a18601de6665f7dae6e989fec8578 |\n|                  | f1f1a800d06f', os_hidden='False',                                             |\n|                  | owner_specified.openstack.md5='5d073d57c0b89afa3a7a3352a42fd073',             |\n|                  | owner_specified.openstack.object='images/debian-13-daily-20250623-2152', owne |\n|                  | r_specified.openstack.sha256='440c4c8be83225f855de5a91e009b2c3257bfb84beb9550 |\n|                  | 7c5beffeeaaa59a32', self='/v2/images/b8e875da-cf0a-4732-ac12-5aad29adc2c0',   |\n|                  | stores='rbd'                                                                  |\n| protected        | False                                                                         |\n| schema           | /v2/schemas/image                                                             |\n| size             | 337444864                                                                     |\n| status           | active                                                                        |\n| tags             |                                                                               |\n| updated_at       | 2025-06-23T15:36:54Z                                                          |\n| virtual_size     | 3221225472                                                                    |\n| visibility       | shared                                                                        |\n+------------------+-------------------------------------------------------------------------------+\n</code></pre> <p>You may also set properties like <code>os_type</code>, <code>os_distro</code>, and <code>os_admin_user</code>, so it becomes easier to filter through your list of custom images:</p> <pre><code>$ openstack image create \\\n    --disk-format qcow2 \\\n    --property os_type=linux \\\n    --property os_distro=freebsd \\\n    --property os_admin_user=freebsd \\\n    --file freebsd-14.2-ufs-2024-12-08.qcow2 \\\n    freebsd-14.2-ufs\n</code></pre> <p>Please keep in mind that although the uploaded disk image is in the <code>qcow2</code> format, whenever you create a boot volume from it, that volume will be in the <code>raw</code> format.</p>"},{"location":"howto/openstack/glance/custom/#creating-an-image-from-a-boot-volume","title":"Creating an image from a boot volume","text":"<p>You can create an image from an existing server\u2019s boot volume. Let\u2019s assume you have a server named <code>app-srv</code>, which you have customized to your liking with the applications you want. The server has one boot volume, and you need to know its ID:</p> <pre><code>$ openstack server show -c volumes_attached -f value app-srv\nid='a90787d6-00e0-4ad2-b923-5e4659736b27'\n</code></pre> <p>To create an image from the boot volume, you first have to delete the parent server.</p> <p>Before you delete the parent server, you should be sure that during its creation, its boot server was not configured for automatic deletion in the event of server deletion.</p> <p>You can now create that image:</p> <pre><code>$ openstack image create \\\n    --volume a90787d6-00e0-4ad2-b923-5e4659736b27 \\\n    app-srv-img\n\n+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| container_format    | bare                                 |\n| disk_format         | raw                                  |\n| display_description |                                      |\n| id                  | a90787d6-00e0-4ad2-b923-5e4659736b27 |\n| image_id            | 2d842eea-f523-4ffc-9407-20de439dd4ed |\n| image_name          | app-srv-img                          |\n| protected           | False                                |\n| size                | 16                                   |\n| status              | uploading                            |\n| updated_at          | 2025-06-24T13:14:34.000000           |\n| visibility          | shared                               |\n| volume_type         | cbs                                  |\n+---------------------+--------------------------------------+\n</code></pre> <p>You may check the progress of the whole process using the <code>openstack image show</code> command:</p> <pre><code>$ openstack image show -c status app-srv-img\n\n+--------+--------+\n| Field  | Value  |\n+--------+--------+\n| status | saving |\n+--------+--------+\n</code></pre> <p>When the image creation is complete, its status will be <code>active</code>:</p> <pre><code>$ openstack image show -c status app-srv-img\n\n+--------+--------+\n| Field  | Value  |\n+--------+--------+\n| status | active |\n+--------+--------+\n</code></pre>"},{"location":"howto/openstack/glance/examine/","title":"Examining images","text":"<p>You can use the <code>openstack</code> CLI in combination with the <code>jq</code> command-line JSON processor to inspect an image\u2019s properties.</p>"},{"location":"howto/openstack/glance/examine/#default-user","title":"Default user","text":"<p>The <code>image_original_user</code> property reflects the username of the default non-root user of the corresponding operating system \u2013 and that piece of information may come in handy in various automation scenarios. So, to find out the username of the default non-root user in the <code>Ubuntu 22.04 Jammy Jellyfish x86_64</code> image, you can type the following:</p> <pre><code>$ openstack image show -f json \"Ubuntu 22.04 Jammy Jellyfish x86_64\" \\\n    | jq '.properties.image_original_user'\n\"ubuntu\"\n</code></pre>"},{"location":"howto/openstack/glance/examine/#image-update-frequency","title":"Image update frequency","text":"<p>Images in Cleura\u00a0Cloud are updated regularly, and that is something you can deduce from the <code>replace_frequency</code> property. See, for example, the value of this property in the <code>Ubuntu 22.04 Jammy Jellyfish x86_64</code> image:</p> <pre><code>$ openstack image show -f json \"Ubuntu 22.04 Jammy Jellyfish x86_64\" \\\n    | jq '.properties.replace_frequency'\n\"monthly\"\n</code></pre> <p>Per the SCS reference, <code>monthly</code> here means that the image is replaced at least once per month. Newer images have operating systems with all the latest package updates and security fixes.</p>"},{"location":"howto/openstack/glance/examine/#image-build-date","title":"Image build date","text":"<p>To find out when a particular image was most recently updated, inspect its <code>image_build_date</code> property:</p> <pre><code>$ openstack image show -f json \"Ubuntu 22.04 Jammy Jellyfish x86_64\" \\\n    | jq '.properties.image_build_date'\n\"2023-08-11\"\n</code></pre>"},{"location":"howto/openstack/glance/filter/","title":"Listing and filtering images","text":"<p>You can use either the <code>openstack</code> command-line interface (CLI), or the Cleura\u00a0Cloud Management\u00a0Panel, to list available public images in Cleura\u00a0Cloud.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Using the <code>openstack\u00a0image\u00a0list</code> command, the CLI gives you several ways of filtering images.</p>"},{"location":"howto/openstack/glance/filter/#when-creating-a-server","title":"When creating a server","text":"<p>Images are sorted by their Operating System on the Create a Server  menu.</p> <p></p> <p></p>"},{"location":"howto/openstack/glance/filter/#on-the-images-tab","title":"On the Images tab","text":"<p>You can also see the full list of images in the Images \u2192 Public Images menu item. On this page you can filter images based on their name, UUID or even tags.</p> <p> </p>"},{"location":"howto/openstack/glance/filter/#filter-by-properties","title":"Filter by properties","text":"<p>You can retrieve a filtered list of available images, based on specific image properties:</p> <pre><code>$ openstack image list \\\n  --public \\\n  --status active \\\n  --property os_distro=ubuntu \\\n  --property os_version=20.04 \\\n+--------------------------------------+---------------------------------+--------+\n| ID                                   | Name                            | Status |\n+--------------------------------------+---------------------------------+--------+\n| f0babf5c-65b6-40d7-b5ce-e60361f2cb09 | Ubuntu 20.04 Focal Fossa x86_64 | active |\n+--------------------------------------+---------------------------------+--------+\n</code></pre> <p>As shown in the example, you can specify multiple tags when filtering by properties. However, you cannot use properties as negated filters. For example, you cannot use the <code>--property</code> option to list public and active images that are not Ubuntu.</p>"},{"location":"howto/openstack/glance/filter/#filter-by-tag","title":"Filter by tag","text":"<p>In order to list images that match a specific tag, you can use the following command:</p> <pre><code>$ openstack image list \\\n  --public \\\n  --status active \\\n  --tag os:ubuntu \\\n  --tag os_version:20.04\n+--------------------------------------+---------------------------------+--------+\n| ID                                   | Name                            | Status |\n+--------------------------------------+---------------------------------+--------+\n| f0babf5c-65b6-40d7-b5ce-e60361f2cb09 | Ubuntu 20.04 Focal Fossa x86_64 | active |\n+--------------------------------------+---------------------------------+--------+\n</code></pre>"},{"location":"howto/openstack/glance/filter/#filter-by-name","title":"Filter by name","text":"<p>Since we aim to make image names consistent, you can also apply filtering based on image names.</p> <p>For example:</p> <pre><code>$ openstack image list \\\n  --public\\\n  --status active \\\n  --name \"Ubuntu 20.04 Focal Fossa x86_64\"\n+--------------------------------------+---------------------------------+--------+\n| ID                                   | Name                            | Status |\n+--------------------------------------+---------------------------------+--------+\n| f0babf5c-65b6-40d7-b5ce-e60361f2cb09 | Ubuntu 20.04 Focal Fossa x86_64 | active |\n+--------------------------------------+---------------------------------+--------+\n</code></pre>"},{"location":"howto/openstack/keystone/app-creds/","title":"Application credentials","text":"<p>You can create application credentials and thus enable your applications to use them to authenticate against OpenStack Keystone. Thanks to this type of credentials, you do not have to resort to your user credentials. Applications can still authenticate using the application credentials ID along with a secret string, which has nothing to do with your user password. That way, this sensitive piece of information does not have to be embedded in any particular application configuration.</p> <p>One major security feature of application credentials is their capacity for expiration. More often than not, application credentials are made to automatically expire after a certain date and time. Passwords, on the other hand, persist essentially forever \u2014 unless they are invalidated manually.</p>"},{"location":"howto/openstack/keystone/app-creds/#prerequisites","title":"Prerequisites","text":"<p>In Cleura\u00a0Cloud, you manage application credentials with the OpenStack CLI, so make sure to enable it for the region you are interested in.</p>"},{"location":"howto/openstack/keystone/app-creds/#creating-application-credentials","title":"Creating application credentials","text":"<p>To create new application credentials with an expiration date, decide on a name and then use the <code>openstack</code> client like so:</p> <pre><code>$ openstack application credential create --expiration '2024-04-30T12:00:00' angry_lamarr\n\n+--------------+-----------------------------------------------------------------------------------+\n| Field        | Value                                                                             |\n+--------------+-----------------------------------------------------------------------------------+\n| description  | None                                                                              |\n| expires_at   | 2024-04-30T12:00:00.000000                                                        |\n| id           | 98a2804b2d294b6784ae30ec67697382                                                  |\n| name         | angry_lamarr                                                                      |\n| project_id   | dfc700467396428bacba4376e72cc3e9                                                  |\n| roles        | creator reader swiftoperator load-balancer_member member                          |\n| secret       | BGXRFpTtEeh6Wi2MsohpFajz5V6ZVGMELjzH8gn53M1SD0voaOEYKlZsmtgO346FqONCHb0bEyt1wnUcv |\n|              | PYYzA                                                                             |\n| system       | None                                                                              |\n| unrestricted | False                                                                             |\n| user_id      | e50719d594f84f16ad13f88da540f762                                                  |\n+--------------+-----------------------------------------------------------------------------------+\n</code></pre> <p>The <code>--expiration</code> parameter accepts ISO 8601 strings, used for specifying date and time-related data.</p> <p>In the example above, we chose the name <code>angry_lamarr</code>. Also, a secret has been automatically generated for us. Keep in mind that secrets are displayed only once, upon creation of the application credentials.</p> <p>You may instead create a secret yourself and indicate it to the <code>openstack application credential create</code> command, like in the example below:</p> <pre><code>$ PRETTY_GOOD_SECRET=\"$(pwgen -Bcns 24 1)\"\n$ openstack application credential create --expiration '2024-05-31T18:30:00' --secret $PRETTY_GOOD_SECRET elated_jones\n\n+--------------+----------------------------------------------------------+\n| Field        | Value                                                    |\n+--------------+----------------------------------------------------------+\n| description  | None                                                     |\n| expires_at   | 2024-05-31T18:30:00.000000                               |\n| id           | a39676b7b0bd4c0c84fa06bdde44b090                         |\n| name         | elated_jones                                             |\n| project_id   | dfc700467396428bacba4376e72cc3e9                         |\n| roles        | creator reader swiftoperator load-balancer_member member |\n| secret       | afduKgbMoqvf4UXyvYJAsWXN                                 |\n| system       | None                                                     |\n| unrestricted | False                                                    |\n| user_id      | e50719d594f84f16ad13f88da540f762                         |\n+--------------+----------------------------------------------------------+\n</code></pre> <p>When creating new application credentials, you may optionally provide a description. You can do that using the <code>--description</code> parameter, like in the example below:</p> <pre><code>$ openstack application credential create \\\n    --description 'Seasonal route between GOH, YFB' \\\n    --expiration '2024-08-31T23:59:59' \\\n    -f yaml goofy_dijkstra\n\ndescription: Seasonal route between GOH, YFB\nexpires_at: '2024-08-31T23:59:59.000000'\nid: 43649f148bbe46ab8f1d745eea095db9\nname: goofy_dijkstra\nproject_id: dfc700467396428bacba4376e72cc3e9\nroles: swiftoperator load-balancer_member reader creator member\nsecret: XS7YUGrH_FGZG9cEjT75tEYB7ciu-3YHgCWOoguTV3sezww8n8xcq67ae_4phzoL_vLY2ryp6avcy6eG8ihiMA\nsystem: null\nunrestricted: false\nuser_id: e50719d594f84f16ad13f88da540f762\n</code></pre> <p>Note that we opted for a YAML-formatted output, for this format makes it very easy to select the secret and copy it.</p>"},{"location":"howto/openstack/keystone/app-creds/#listing-and-viewing-application-credentials","title":"Listing and viewing application credentials","text":"<p>To list all application credentials you have created, type the following:</p> <pre><code>$ openstack application credential list\n\n+-------------------+----------------+-------------------+-------------------+---------------------+\n| ID                | Name           | Project ID        | Description       | Expires At          |\n+-------------------+----------------+-------------------+-------------------+---------------------+\n| 98a2804b2d294b678 | angry_lamarr   | dfc700467396428ba | None              | 2024-04-            |\n| 4ae30ec67697382   |                | cba4376e72cc3e9   |                   | 30T12:00:00.000000  |\n| a39676b7b0bd4c0c8 | elated_jones   | dfc700467396428ba | None              | 2024-05-            |\n| 4fa06bdde44b090   |                | cba4376e72cc3e9   |                   | 31T18:30:00.000000  |\n| 43649f148bbe46ab8 | goofy_dijkstra | dfc700467396428ba | Seasonal route    | 2024-08-            |\n| f1d745eea095db9   |                | cba4376e72cc3e9   | between GOH, YFB  | 31T23:59:59.000000  |\n+-------------------+----------------+-------------------+-------------------+---------------------+\n</code></pre> <p>You may select specific columns\u2026</p> <pre><code>$ openstack application credential list -c ID -c Name -c \"Expires At\"\n\n+----------------------------------+----------------+----------------------------+\n| ID                               | Name           | Expires At                 |\n+----------------------------------+----------------+----------------------------+\n| 98a2804b2d294b6784ae30ec67697382 | angry_lamarr   | 2024-04-30T12:00:00.000000 |\n| a39676b7b0bd4c0c84fa06bdde44b090 | elated_jones   | 2024-05-31T18:30:00.000000 |\n| 43649f148bbe46ab8f1d745eea095db9 | goofy_dijkstra | 2024-08-31T23:59:59.000000 |\n+----------------------------------+----------------+----------------------------+\n</code></pre> <p>\u2026or choose a different output format, like JSON:</p> <pre><code>$ openstack application credential list -f json\n\n[\n  {\n    \"ID\": \"98a2804b2d294b6784ae30ec67697382\",\n    \"Name\": \"angry_lamarr\",\n    \"Project ID\": \"dfc700467396428bacba4376e72cc3e9\",\n    \"Description\": null,\n    \"Expires At\": \"2024-04-30T12:00:00.000000\"\n  },\n  {\n    \"ID\": \"a39676b7b0bd4c0c84fa06bdde44b090\",\n    \"Name\": \"elated_jones\",\n    \"Project ID\": \"dfc700467396428bacba4376e72cc3e9\",\n    \"Description\": null,\n    \"Expires At\": \"2024-05-31T18:30:00.000000\"\n  },\n  {\n    \"ID\": \"43649f148bbe46ab8f1d745eea095db9\",\n    \"Name\": \"goofy_dijkstra\",\n    \"Project ID\": \"dfc700467396428bacba4376e72cc3e9\",\n    \"Description\": \"Seasonal route between GOH, YFB\",\n    \"Expires At\": \"2024-08-31T23:59:59.000000\"\n  }\n]\n</code></pre> <p>You can view specific application credentials like so:</p> <pre><code>$ openstack application credential show goofy_dijkstra\n\n+--------------+----------------------------------------------------------+\n| Field        | Value                                                    |\n+--------------+----------------------------------------------------------+\n| description  | Seasonal route between GOH, YFB                          |\n| expires_at   | 2024-08-31T23:59:59.000000                               |\n| id           | 43649f148bbe46ab8f1d745eea095db9                         |\n| name         | goofy_dijkstra                                           |\n| project_id   | dfc700467396428bacba4376e72cc3e9                         |\n| roles        | swiftoperator reader member creator load-balancer_member |\n| system       | None                                                     |\n| unrestricted | False                                                    |\n| user_id      | e50719d594f84f16ad13f88da540f762                         |\n+--------------+----------------------------------------------------------+\n</code></pre> <p>No matter how you choose to list or view application credentials, secrets are never displayed.</p>"},{"location":"howto/openstack/keystone/app-creds/#restricted-vs-unrestricted-credentials","title":"Restricted vs unrestricted credentials","text":"<p>By default, application credentials are created as being restricted. That is why, in the <code>openstack application credential show</code> output above, the <code>unrestricted</code> parameter is set to <code>False</code>. You cannot use restricted application credentials for Heat or Magnum, or for managing other application credentials or trusts. This restricted-by-default policy acts as a safeguard, so compromised application credentials cannot be used for creating other sets of application credentials. If your application has to be able to perform such actions, and you accept the risks involved, you may create unrestricted application credentials like this:</p> <pre><code>$ openstack application credential create --expiration '2024-02-15T23:59:59' modest_mccarthy --unrestricted\n\n+--------------+---------------------------------------------------------------+\n| Field        | Value                                                         |\n+--------------+---------------------------------------------------------------+\n| description  | None                                                          |\n| expires_at   | 2024-02-15T23:59:59.000000                                    |\n| id           | c3505060e42542a29ce1b77435112590                              |\n| name         | modest_mccarthy                                               |\n| project_id   | dfc700467396428bacba4376e72cc3e9                              |\n| roles        | creator reader swiftoperator load-balancer_member member      |\n| secret       | qsV9G-qklvbZ8Fz5m7arghW7hHYDRCYWE3vim5dm3-EcXgQn-             |\n|              | pkvrDAOAW5nR7vfj32_HwtQbLjq0TyWM-gETg                         |\n| system       | None                                                          |\n| unrestricted | True                                                          |\n| user_id      | e50719d594f84f16ad13f88da540f762                              |\n+--------------+---------------------------------------------------------------+\n</code></pre> <p>In the example output above, the <code>unrestricted</code> parameter is set to <code>True</code>.</p>"},{"location":"howto/openstack/keystone/app-creds/#using-application-credentials","title":"Using application credentials","text":"<p>One way to use application credentials is by instantiating certain <code>OS_</code> variables. Then, you may work with the OpenStack CLI as you typically do. Let us see how you can go about this, by first creating a new set of application credentials:</p> <pre><code>$ PRETTY_GOOD_SECRET=\"$(pwgen -Bcns 32 1)\"\n$ openstack application credential create --expiration '2024-02-29T23:59:59' practical_swanson --secret $PRETTY_GOOD_SECRET\n$ openstack application credential show practical_swanson\n\n+--------------+----------------------------------------------------------+\n| Field        | Value                                                    |\n+--------------+----------------------------------------------------------+\n| description  | None                                                     |\n| expires_at   | 2024-02-29T23:59:59.000000                               |\n| id           | 3e04d504d754445e8b8d503e0e7af3c5                         |\n| name         | practical_swanson                                        |\n| project_id   | dfc700467396428bacba4376e72cc3e9                         |\n| roles        | swiftoperator reader member creator load-balancer_member |\n| system       | None                                                     |\n| unrestricted | False                                                    |\n| user_id      | e50719d594f84f16ad13f88da540f762                         |\n+--------------+----------------------------------------------------------+\n</code></pre> <p>You need to instantiate four <code>OS_</code> variables:</p> <ul> <li><code>OS_AUTH_URL</code>, which, in our example, is set to <code>https://fra1.citycloud.com:5000</code>,</li> <li><code>OS_AUTH_TYPE</code>, which should be set to <code>v3applicationcredential</code>,</li> <li><code>OS_APPLICATION_CREDENTIAL_ID</code>, which should be set to the value of <code>id</code> (as displayed in the table above),</li> <li><code>OS_APPLICATION_CREDENTIAL_SECRET</code>, which should be set to <code>$PRETTY_GOOD_SECRET</code>.</li> </ul> <p>Instead of <code>OS_APPLICATION_CREDENTIAL_ID</code>, you may instead initialize <code>OS_APPLICATION_CREDENTIAL_NAME</code> by simply setting it to the name of the application credentials. That way, you will not have to bother with the application credentials ID.</p> <p>Create a new file named <code>acRC</code>, with the following content:</p> <pre><code>export OS_AUTH_URL=https://fra1.citycloud.com:5000\nexport OS_AUTH_TYPE=v3applicationcredential\nexport OS_APPLICATION_CREDENTIAL_ID=3e04d504d754445e8b8d503e0e7af3c5\nexport OS_APPLICATION_CREDENTIAL_SECRET=&lt;value_of_PRETTY_GOOD_SECRET&gt;\n</code></pre> <p>In your setup, replace the values of <code>OS_AUTH_URL</code>, <code>OS_APPLICATION_CREDENTIAL_ID</code>, and <code>OS_APPLICATION_CREDENTIAL_SECRET</code> accordingly.</p> <p>Next, switch to a new local environment (e.g., start a new shell), where none of the variables in your OpenStack RC file is initialized. There, source the <code>acRC</code> file you just created:</p> <pre><code>source acRC\n</code></pre> <p>You can now use the <code>openstack</code> client as usual. One example:</p> <pre><code>$ openstack token issue -f yaml\n\nexpires: 2024-01-23T22:40:49+0000\nid: vgAAAAABlr5exIHPqjgksbMfmbdg24xN7uqLHLm4...\nproject_id: dfc700467396428bacba4376e72cc3e9\nuser_id: e50719d594f84f16ad13f88da540f762\n</code></pre>"},{"location":"howto/openstack/keystone/app-creds/#troubleshooting-application-credentials-usage","title":"Troubleshooting application credentials usage","text":"<p>Having created a fresh set of application credentials to use with OpenStack CLI, you may find out that the <code>openstack</code> client starts behaving like its privileges are suddenly limited. Any command you might try will fail, returning an error like in the following example:</p> <pre><code>$ openstack token issue\n\nError authenticating with application credential: Application credentials cannot request a scope. (HTTP 401) (Request-ID: req-d59edba0-2cb5-450b-83c7-f094bd8e5654)\n</code></pre> <p>This happens because either of the <code>OS_PROJECT_NAME</code> or <code>OS_TENANT_NAME</code> variables is initialized, causing the <code>openstack</code> client to request a project scope for your token \u2014 and that is not allowed.</p> <p>This is why we advised making sure no pre-existing <code>OS_</code> variable is set before initializing the four <code>OS_</code> variables <code>openstack</code> needs, in order to use your application credentials.</p> <p>You might want to add this function to your <code>.bashrc</code>:</p> <pre><code>function unset_os_vars() {\n    unset -v `env | grep -o \"^OS_[^=]*\"`\n}\n</code></pre> <p>From then on, you can unset all <code>OS_</code> variables by simply typing <code>unset_os_vars</code>.</p>"},{"location":"howto/openstack/keystone/app-creds/#deleting-application-credentials","title":"Deleting application credentials","text":"<p>If there are application credentials you suspect are compromised or simply do not need anymore, you may go ahead and delete them by specifying the corresponding ID or name, like so:</p> <pre><code>$ openstack application credential delete goofy_dijkstra\n$ openstack application credential show goofy_dijkstra\n\nNo applicationcredential with a name or ID of 'goofy_dijkstra' exists.\n</code></pre> <p>Even though expired application credentials can no longer be used for authentication, they stay around until you explicitly delete them.</p>"},{"location":"howto/openstack/magnum/kubectl/","title":"Managing a Kubernetes cluster","text":"<p>Once you have launched a new cluster, you can interact with it using <code>kubectl</code> and a kubeconfig file.</p>"},{"location":"howto/openstack/magnum/kubectl/#prerequisites","title":"Prerequisites","text":"<p>You must install the Kubernetes command line tool, <code>kubectl</code>, on your local computer, and run commands against your cluster. To install <code>kubectl</code>, follow the relevant Kubernetes documentation.</p>"},{"location":"howto/openstack/magnum/kubectl/#extracting-the-kubeconfig-file","title":"Extracting the kubeconfig file","text":"<p>Due to Magnum\u2019s security policy configuration, you cannot use the OpenStack CLI for downloading the kubeconfig of a cluster that was created with Cleura\u00a0Cloud Management\u00a0Panel, or vice versa.</p> <p>To fetch your kubeconfig, you must always use the same facility that you used to deploy the cluster.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>In the left-hand side pane of the Cleura\u00a0Cloud Management\u00a0Panel, select Magnum \u2192 Clusters. Click on the cluster row to expand the details view, then click the KubeConfig tab. In a second or two, you will see the contents of the kubeconfig file. Click the blue Download KubeConfig button to download it locally.</p> <p></p> <p>The kubeconfig file you get has a name similar to this one:</p> <pre><code>kubeconfig--&lt;cluster_name&gt;--&lt;region_name&gt;--&lt;alphanum_id&gt;.yaml\n</code></pre> <p>Feel free to rename it to something simpler, like <code>config</code>.</p> <p>To download the kubeconfig file for your Kubernetes cluster, type the following:</p> <pre><code>openstack coe cluster config --dir=${PWD} &lt;cluster-name&gt;\n</code></pre> <p>After saving the kubeconfig file locally, set the value of variable <code>KUBECONFIG</code> to the full path of the file. Type, for example:</p> <pre><code>export KUBECONFIG=${PWD}/config\n</code></pre> <p>If you are currently managing only one cluster, and you already have its kubeconfig file stored as <code>~/.kube/config</code>, then you do not need to set the <code>KUBECONFIG</code> variable.</p>"},{"location":"howto/openstack/magnum/kubectl/#accessing-the-kubernetes-cluster-with-kubectl","title":"Accessing the Kubernetes cluster with kubectl","text":"<p>You may now use <code>kubectl</code> to run commands against your cluster. See, for instance, all cluster nodes\u2026</p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME                           STATUS   ROLES    AGE    VERSION\nbangor-id6nijycp2wy-master-0   Ready    master   113m   v1.18.6\nbangor-id6nijycp2wy-node-0     Ready    &lt;none&gt;   111m   v1.18.6\n</code></pre> <p>\u2026or all running pods in every namespace:</p> <pre><code>kubectl get pods --all-namespaces\n</code></pre> <pre><code>NAMESPACE     NAME                                         READY   STATUS    RESTARTS   AGE\nkube-system   coredns-786ffb7797-tw2hg                     1/1     Running   0          167m\nkube-system   coredns-786ffb7797-vbqwn                     1/1     Running   0          167m\nkube-system   csi-cinder-controllerplugin-0                5/5     Running   0          167m\nkube-system   csi-cinder-nodeplugin-4nr69                  2/2     Running   0          166m\nkube-system   csi-cinder-nodeplugin-vtwqf                  2/2     Running   0          167m\nkube-system   dashboard-metrics-scraper-6b4884c9d5-4mlrg   1/1     Running   0          167m\nkube-system   k8s-keystone-auth-wk5v2                      1/1     Running   0          167m\nkube-system   kube-dns-autoscaler-75859754fd-2wsd9         1/1     Running   0          167m\nkube-system   kube-flannel-ds-7z9dp                        1/1     Running   0          167m\nkube-system   kube-flannel-ds-dmvk6                        1/1     Running   0          166m\nkube-system   kubernetes-dashboard-c98496485-stn42         1/1     Running   0          167m\nkube-system   magnum-metrics-server-79556d6999-xdlpm       1/1     Running   0          167m\nkube-system   npd-5p6gk                                    1/1     Running   0          165m\nkube-system   openstack-cloud-controller-manager-44rz9     1/1     Running   0          167m\n</code></pre>"},{"location":"howto/openstack/magnum/kubectl/#defining-a-default-storage-class","title":"Defining a default storage class","text":"<p>An OpenStack Magnum-managed cluster does not automatically define a default storage class for dynamic volume provisioning. You should define one immediately upon cluster creation.</p> <p>To do so, create a file named <code>storageclass.yaml</code> with the following content:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: csi-sc-cinderplugin\n  annotations:\n    \"storageclass.kubernetes.io/is-default-class\": \"true\"\nprovisioner: cinder.csi.openstack.org\n</code></pre> <p>You can use an alternate <code>name</code> if you prefer.</p> <p>Then, apply the storage class definition:</p> <pre><code>$ kubectl apply -f storageclass.yaml\nstorageclass.storage.k8s.io/csi-sc-cinderplugin created\n</code></pre> <p>Subsequently, any persistent volume claims will default to using this storage class, unless you choose to override the default by setting the <code>spec.storageClassName</code> property.</p>"},{"location":"howto/openstack/magnum/new-k8s-cluster/","title":"Creating a Kubernetes cluster","text":"<p>By employing OpenStack Magnum you can create Kubernetes clusters via OpenStack, using the Cleura\u00a0Cloud Management\u00a0Panel or the OpenStack CLI.</p>"},{"location":"howto/openstack/magnum/new-k8s-cluster/#prerequisites","title":"Prerequisites","text":"<p>First and foremost, you need an account in Cleura\u00a0Cloud. Should you choose to work from your terminal, you will also need to enable the OpenStack CLI. In that case, in addition to the Python <code>openstackclient</code> module, make sure you also install the corresponding plugin module for Magnum. Use either the package manager of your operating system or <code>pip</code>:</p> Debian/UbuntuMac OS X with HomebrewPython Package <pre><code>apt install python3-magnumclient\n</code></pre> <p>This Python module is unavailable via <code>brew</code>, but you can install it via <code>pip</code>.</p> <pre><code>pip install python-magnumclient\n</code></pre>"},{"location":"howto/openstack/magnum/new-k8s-cluster/#creating-a-kubernetes-cluster_1","title":"Creating a Kubernetes cluster","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Fire up your favorite web browser, navigate to the Cleura\u00a0Cloud Management\u00a0Panel start page, and log into your Cleura\u00a0Cloud account. On the top right-hand side of the Cleura\u00a0Cloud Management\u00a0Panel, click the Create button. A new pane titled Create slides into view.</p> <p></p> <p>You will notice several rounded boxes on that pane, each for defining, configuring, and instantiating a different Cleura\u00a0Cloud object. Go ahead and click the Magnum Cluster box. A new vertical pane titled Create a Magnum Cluster slides over. At the top, type in a name for the new cluster and select one of the available regions.</p> <p></p> <p>A bit further below, use the drop-down menus to select a template and a keypair for the cluster nodes. Notice the Docker volume size option which, by default, is set to 50 GiB. This pertains to the size of the extra block device each cluster node will have. That whole storage capacity will be used for saving persistent data. If you believe the default size is too much or too little, change it accordingly. Unless you want to define the number of master nodes or the number of worker nodes, click the green Create button now.</p> <p></p> <p>If you do want to play with the aforementioned parameters, then before clicking the Create button go ahead and expand the Advanced Options section. Since the new cluster templates come with the master load balancer enabled by default, you might want to equip your new cluster with 3 master nodes. Optionally, increase the number of worker nodes from 1 (the default) to, say, 3. Contrary to the number of master nodes, you may change the number of worker nodes after the cluster is created.</p> <p></p> <p>When the cluster creation process begins, please keep in mind that it takes some time to complete. While waiting, bring the vertical pane on the left-hand side of the Cleura\u00a0Cloud Management\u00a0Panel in full view, select Magnum \u2192 Clusters, and in the main pane, take a look at the creation progress. You can tell when the whole process is complete by the animated icon at the left of the cluster row.</p> <p></p> <p>A simple, general command for creating a new Kubernetes cluster with Magnum looks like this:</p> <pre><code>openstack coe cluster create \\\n    --cluster-template $CLUSTER_TMPL \\\n    --keypair $KEYPAIR \\\n    --docker-volume-size $PERSIST_VOL_SIZE \\\n    $CLUSTER_NAME\n</code></pre> <p>First, list all available templates in the region:</p> <pre><code>$ openstack coe cluster template list\n+--------------------------------------+---------------------------------------------+------+\n| uuid                                 | name                                        | tags |\n+--------------------------------------+---------------------------------------------+------+\n| 3f476f01-b3de-4687-a188-6829ed947db0 | Kubernetes 1.15.5 on Fedora-atomic 29       | None |\n|                                      | 4C-8GB-20GB No Master LB                    |      |\n| c458f02d-54b0-4ef8-abbc-e1c25b61165a | Kubernetes 1.15.5 on Fedora-atomic 29       | None |\n|                                      | 2C-4GB-20GB No Master LB                    |      |\n| f9e1a2ea-b1ff-43e7-8d1e-6dd5861b82cf | Kubernetes 1.18.6 on Fedora-coreos 33       | None |\n|                                      | 2C-4GB-20GB No Master LB                    |      |\n| 59bd894b-0f5f-4a6e-98d3-a3eb7040faab | Kubernetes v1.23.3 on Fedora-coreos 35      | None |\n| 9ca03308-996e-4eaa-b507-5730dcc19fcc | Kubernetes v1.24.16 on Fedora-coreos 37     | None |\n+--------------------------------------+---------------------------------------------+------+\n</code></pre> <p>Select the template you want by setting the corresponding <code>name</code> value to the <code>CLUSTER_TMPL</code> variable:</p> <pre><code>$ CLUSTER_TMPL=\"Kubernetes v1.24.16 on Fedora-coreos 37\"\n</code></pre> <p>Then, list all available keypairs\u2026</p> <pre><code>$ openstack keypair list\n+----------+-------------------------------------------------+------+\n| Name     | Fingerprint                                     | Type |\n+----------+-------------------------------------------------+------+\n| lefkanti | e7:e9:c5:95:ee:7b:72:37:3c:89:c5:fc:6e:8c:a1:72 | ssh  |\n+----------+-------------------------------------------------+------+\n</code></pre> <p>\u2026and set the <code>KEYPAIR</code> variable to the <code>Name</code> of the keypair you want:</p> <pre><code>$ KEYPAIR=\"lefkanti\" # this is just an example\n</code></pre> <p>Besides the OS disk, all cluster nodes have an extra disk for permanently storing application data. The size of this extra disk is specified during cluster creation via the <code>--docker-volume-size</code> parameter. Staying loyal to the theme of setting parameters via shell variables, decide on the size of this extra volume like so:</p> <pre><code>$ PERSIST_VOL_SIZE=50\n</code></pre> <p>The size is expressed in Gibibytes, and in the example above, we decided to go with a 50GiB extra volume. Finally, decide on a name for your new Kubernetes cluster:</p> <pre><code>$ CLUSTER_NAME=\"bangor\"\n</code></pre> <p>With everything in place, go ahead and create your new Kubernetes cluster like so:</p> <pre><code>$ openstack coe cluster create \\\n    --cluster-template \"$CLUSTER_TMPL\" \\\n    --keypair $KEYPAIR \\\n    --docker-volume-size $PERSIST_VOL_SIZE \\\n    $CLUSTER_NAME\n</code></pre> <p>New Magnum clusters start with 1 master node and 1 worker node by default. Since the new cluster templates have the master load balancer enabled, you might want to give your cluster 3 master nodes from the get-go. Optionally, you can increase the number of worker nodes from 1 to 3 or to 5. Contrary to the number of master nodes, you can always change the number of worker nodes after the cluster is created. So, to start the new cluster with 3 master nodes and 5 worker nodes, type the following:</p> <pre><code>$ openstack coe cluster create \\\n    --cluster-template \"$CLUSTER_TMPL\" \\\n    --keypair $KEYPAIR \\\n    --docker-volume-size $PERSIST_VOL_SIZE \\\n    --master-count 3 \\\n    --node-count 3 \\\n    $CLUSTER_NAME\n</code></pre> <p>In any case, if everything went well with your request for a new cluster, on the terminal, you will see a message like the following:</p> <pre><code>Request to create cluster 7ca7838a-aa33-4259-8784-02e5941a2cf0 accepted\n</code></pre> <p>The cluster creation process takes some time to complete, and while you are waiting, you can check if everything is progressing smoothly:</p> <pre><code>$ openstack coe cluster list -c status\n+--------------------+\n| status             |\n+--------------------+\n| CREATE_IN_PROGRESS |\n+--------------------+\n</code></pre> <p>If everything is going well, the message you will get will be <code>CREATE_IN_PROGRESS</code>. When Magnum has finished creating the cluster, the message will be <code>CREATE_COMPLETE</code>.</p> <pre><code>$ openstack coe cluster list -c status\n+-----------------+\n| status          |\n+-----------------+\n| CREATE_COMPLETE |\n+-----------------+\n</code></pre>"},{"location":"howto/openstack/magnum/new-k8s-cluster/#viewing-the-kubernetes-cluster","title":"Viewing the Kubernetes cluster","text":"<p>After the Kubernetes cluster is ready, you may at any time view it and get detailed information about it.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Bring the vertical pane on the left-hand side of the Cleura\u00a0Cloud Management\u00a0Panel in full view, then select Magnum \u2192 Clusters. In the main pane, take a look at the row of the cluster you are interested in. In our example, there is only one cluster, hence only one row.</p> <p></p> <p>To see more of the cluster, just click on its row. Then, all relative information will appear below.</p> <p></p> <p>To list all available Kubernetes clusters, type:</p> <pre><code>$ openstack coe cluster list\n+---------------+--------+----------+------------+--------------+---------------+---------------+\n| uuid          | name   | keypair  | node_count | master_count | status        | health_status |\n+---------------+--------+----------+------------+--------------+---------------+---------------+\n| 7ca7838a-     | bangor | lefkanti |          1 |            1 | CREATE_COMPLE | HEALTHY       |\n| aa33-4259-    |        |          |            |              | TE            |               |\n| 8784-         |        |          |            |              |               |               |\n| 02e5941a2cf0  |        |          |            |              |               |               |\n+---------------+--------+----------+------------+--------------+---------------+---------------+\n</code></pre> <p>For many more details on a specific cluster, issue a command like this:</p> <pre><code>$ openstack coe cluster show $CLUSTER_NAME\n+----------------------+---------------------------------------------------------------------------+\n| Field                | Value                                                                     |\n+----------------------+---------------------------------------------------------------------------+\n| status               | CREATE_COMPLETE                                                           |\n| health_status        | HEALTHY                                                                   |\n| cluster_template_id  | 9ca03308-996e-4eaa-b507-5730dcc19fcc                                      |\n| node_addresses       | ['198.51.100.5']                                                          |\n| uuid                 | 7ca7838a-aa33-4259-8784-02e5941a2cf0                                      |\n| stack_id             | 923c938a-81cd-4a0d-b645-0681ff507bc5                                      |\n| status_reason        | None                                                                      |\n| created_at           | 2024-07-05T12:00:41+00:00                                                 |\n| updated_at           | 2024-07-05T12:08:52+00:00                                                 |\n| coe_version          | v1.24.16-rancher1                                                         |\n| labels               | {'container_runtime': 'containerd', 'cinder_csi_enabled': 'True',         |\n|                      | 'cloud_provider_enabled': 'True', 'docker_volume_size': '20', 'kube_tag': |\n|                      | 'v1.24.16-rancher1', 'hyperkube_prefix': 'docker.io/rancher/'}            |\n| labels_overridden    | {}                                                                        |\n| labels_skipped       | {}                                                                        |\n| labels_added         | {}                                                                        |\n| fixed_network        | None                                                                      |\n| fixed_subnet         | None                                                                      |\n| floating_ip_enabled  | True                                                                      |\n| faults               |                                                                           |\n| keypair              | lefkanti                                                                  |\n| api_address          | https://203.0.113.129:6443                                                |\n| master_addresses     | ['203.0.113.144']                                                         |\n| master_lb_enabled    | True                                                                      |\n| create_timeout       | 60                                                                        |\n| node_count           | 1                                                                         |\n| discovery_url        | https://discovery.etcd.io/f1473529bd109bea2fb02ac936497b95                |\n| docker_volume_size   | 50                                                                        |\n| master_count         | 1                                                                         |\n| container_version    | 1.12.6                                                                    |\n| name                 | bangor                                                                    |\n| master_flavor_id     | b.2c4gb                                                                   |\n| flavor_id            | b.4c24gb                                                                  |\n| health_status_reason | {'bangor-xh3fuqin3arw-master-0.Ready': 'True', 'bangor-xh3fuqin3arw-      |\n|                      | node-0.Ready': 'True', 'api': 'ok'}                                       |\n| project_id           | dfc700467396428bacba4376e72cc3e9                                          |\n+----------------------+---------------------------------------------------------------------------+\n</code></pre>"},{"location":"howto/openstack/magnum/new-k8s-cluster/#interacting-with-your-cluster","title":"Interacting with your cluster","text":"<p>Once your new Magnum-managed Kubernetes cluster is operational, you can start interacting with it.</p>"},{"location":"howto/openstack/neutron/create-security-groups/","title":"Creating security groups","text":"<p>By definition, security groups are \u201d[\u2026] sets of IP filter rules that are applied to all project instances, which define networking access to the instance. Group rules are project specific; project members can edit the default rules for their group and add new rule sets.\u201d</p>"},{"location":"howto/openstack/neutron/create-security-groups/#creating-a-security-group","title":"Creating a security group","text":"<p>Navigate to the Cleura\u00a0Cloud Management\u00a0Panel page, and log into your Cleura\u00a0Cloud account. On the other hand, if you prefer to work with the OpenStack CLI, please do not forget to source the RC file first.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>To create a security group, first make sure the left-hand side vertical pane is fully visible. Click on Security\u00a0Groups, and then on the top-right corner of the central pane, click on Create new Security Group.</p> <p></p> <p>An alternative way to create a Security\u00a0Group is by clicking on the round-cornered Create button, in the top bar.</p> <p>Type in a Name for the new security group, and choose a Region to create it in. You may optionally type in a Description for the security group. Click on the green Create button when you are ready.</p> <p></p> <p>To create a security group, use the following command:</p> <pre><code>openstack security group create &lt;name&gt;\n</code></pre> <p>When the command is executed successfully, you will get information regarding your new security group:</p> <pre><code>+-----------------+--------------------------------------------------------------------------------+\n| Field           | Value                                                                          |\n+-----------------+--------------------------------------------------------------------------------+\n| created_at      | 2022-11-14T09:15:14Z                                                           |\n| description     | &lt;name&gt;                                                                         |\n| id              | 736da1d1-aa98-4da4-9ba4-2ab9aeea6a57                                           |\n| name            | &lt;name&gt;                                                                         |\n| project_id      | cb43f189f7904fb88f3bbcfa22653ab8                                               |\n| revision_number | 1                                                                              |\n| rules           | created_at='2022-11-14T09:15:14Z', direction='egress', ethertype='IPv4',       |\n|                 | id='1f4c57cb-8e34-420c-a7e3-3b5625c79481', standard_attr_id='10579829',        |\n|                 | updated_at='2022-11-14T09:15:14Z'                                              |\n|                 | created_at='2022-11-14T09:15:14Z', direction='egress', ethertype='IPv6',       |\n|                 | id='7c2c287e-9596-42ef-a5a8-0b09e38b206a', standard_attr_id='10579832',        |\n|                 | updated_at='2022-11-14T09:15:14Z'                                              |\n| stateful        | True                                                                           |\n| tags            | []                                                                             |\n| updated_at      | 2022-11-14T09:15:14Z                                                           |\n+-----------------+--------------------------------------------------------------------------------+\n</code></pre>"},{"location":"howto/openstack/neutron/create-security-groups/#removing-default-ingress-rules","title":"Removing default ingress rules","text":"<p>By default, a security group named <code>default</code> has already been created for you. Its rules block all traffic from any source (ingress), except from servers and ports in the same security group. All traffic to any destination (egress) is allowed by default.</p> <p>For accounts created before 2022-11-16, the default security group ingress rules allow all incoming traffic. See Adjusting a permissive default security group, to learn how to configure this security group according to our recommendations.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Navigate to the Security\u00a0Groups page. Click on the <code>default</code> security group and select the Rules tab to view its rules.</p> <p></p> <p>View the details of the <code>default</code> security group using the following command:</p> <pre><code>openstack security group show default\n</code></pre> <p>You will get a printout similar to this:</p> <pre><code>+-----------------+--------------------------------------------------------------------------------+\n| Field           | Value                                                                          |\n+-----------------+--------------------------------------------------------------------------------+\n| created_at      | 2022-09-12T15:00:57Z                                                           |\n| description     | Default security group                                                         |\n| id              | 935b1317-a0c0-42e9-b68d-7cf16637df14                                           |\n| name            | default                                                                        |\n| project_id      | cb43f189f7904fb88f3bbcfa22653ab8                                               |\n| revision_number | 5                                                                              |\n| rules           | created_at='2022-09-12T15:00:59Z', direction='ingress', ethertype='IPv4',      |\n|                 | id='5e5e9f4d-1faa-492d-91f1-c105b464072b', protocol='0',                       |\n|                 | remote_group_id='60776d43-a78c-4eb4-8998-cea7a04c5f9b',                        |\n|                 | standard_attr_id='10422245', updated_at='2022-09-12T15:00:59Z'                 |\n|                 | created_at='2022-09-12T15:00:59Z', direction='ingress', ethertype='IPv6',      |\n|                 | id='86b9413a-ad23-46c4-a35e-9306945dc63c', protocol='0',                       |\n|                 | remote_group_id='60776d43-a78c-4eb4-8998-cea7a04c5f9b',                        |\n|                 | standard_attr_id='10422248', updated_at='2022-09-12T15:00:59Z'                 |\n|                 | created_at='2022-09-12T15:00:57Z', direction='egress', ethertype='IPv6',       |\n|                 | id='ad4a19ef-7fab-4eba-9982-e5b109be121c', standard_attr_id='10422242',        |\n|                 | updated_at='2022-09-12T15:00:57Z'                                              |\n|                 | created_at='2022-09-12T15:00:57Z', direction='egress', ethertype='IPv4',       |\n|                 | id='f53b1a12-edbb-480b-910b-a71c4836346f', standard_attr_id='10422236',        |\n|                 | updated_at='2022-09-12T15:00:57Z'                                              |\n| stateful        | True                                                                           |\n| tags            | []                                                                             |\n| updated_at      | 2022-09-12T15:00:59Z                                                           |\n+-----------------+--------------------------------------------------------------------------------+\n</code></pre> <p>If you want to restrict the ingress rules to disallow access from other servers and ports in the group, you need to remove the default two ingress rules.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Click each of the red  buttons on the right-hand side of the IPv4 ingress and also of the IPv6 ingress rows.</p> <p>Your <code>default</code> or newly created security group rules will now look like the following example.</p> <p></p> <p>To view the rules, use the following command:</p> <pre><code>openstack security group rule list default\n</code></pre> <p>The printout will be similar to this:</p> <pre><code>+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n| ID        | IP Protocol | Ethertype | IP Range  | Port Range | Direction | Remote Security Group | Remote Address Group |\n+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n| 5e5e9f4d- | None        | IPv4      | 0.0.0.0/0 |            | ingress   | 60776d43-a78c-4eb4-   | None                 |\n| 1faa-     |             |           |           |            |           | 8998-cea7a04c5f9b     |                      |\n| 492d-     |             |           |           |            |           |                       |                      |\n| 91f1-     |             |           |           |            |           |                       |                      |\n| c105b4640 |             |           |           |            |           |                       |                      |\n| 72b       |             |           |           |            |           |                       |                      |\n| 86b9413a- | None        | IPv6      | ::/0      |            | ingress   | 60776d43-a78c-4eb4-   | None                 |\n| ad23-     |             |           |           |            |           | 8998-cea7a04c5f9b     |                      |\n| 46c4-     |             |           |           |            |           |                       |                      |\n| a35e-     |             |           |           |            |           |                       |                      |\n| 9306945dc |             |           |           |            |           |                       |                      |\n| 63c       |             |           |           |            |           |                       |                      |\n| ad4a19ef- | None        | IPv6      | ::/0      |            | egress    | None                  | None                 |\n| 7fab-     |             |           |           |            |           |                       |                      |\n| 4eba-     |             |           |           |            |           |                       |                      |\n| 9982-     |             |           |           |            |           |                       |                      |\n| e5b109be1 |             |           |           |            |           |                       |                      |\n| 21c       |             |           |           |            |           |                       |                      |\n| f53b1a12- | None        | IPv4      | 0.0.0.0/0 |            | egress    | None                  | None                 |\n| edbb-     |             |           |           |            |           |                       |                      |\n| 480b-     |             |           |           |            |           |                       |                      |\n| 910b-     |             |           |           |            |           |                       |                      |\n| a71c48363 |             |           |           |            |           |                       |                      |\n| 46f       |             |           |           |            |           |                       |                      |\n+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n</code></pre> <p>The IDs of the two ingress rules, one for IPv4 traffic and one for IPv6, in this case, are: <code>5e5e9f4d-1faa-492d-91f1-c105b464072b</code> and <code>86b9413a-ad23-46c4-a35e-9306945dc63c</code>.</p> <p>Delete them by using the following command:</p> <pre><code>openstack security group rule delete \\\n  5e5e9f4d-1faa-492d-91f1-c105b464072b 86b9413a-ad23-46c4-a35e-9306945dc63c\n</code></pre> <p>Print the rules again:</p> <pre><code>openstack security group rule list default\n</code></pre> <p>Now, the remaining rules are only the egress ones.</p> <pre><code>+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n| ID        | IP Protocol | Ethertype | IP Range  | Port Range | Direction | Remote Security Group | Remote Address Group |\n+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n| ad4a19ef- | None        | IPv6      | ::/0      |            | egress    | None                  | None                 |\n| 7fab-     |             |           |           |            |           |                       |                      |\n| 4eba-     |             |           |           |            |           |                       |                      |\n| 9982-     |             |           |           |            |           |                       |                      |\n| e5b109be1 |             |           |           |            |           |                       |                      |\n| 21c       |             |           |           |            |           |                       |                      |\n| f53b1a12- | None        | IPv4      | 0.0.0.0/0 |            | egress    | None                  | None                 |\n| edbb-     |             |           |           |            |           |                       |                      |\n| 480b-     |             |           |           |            |           |                       |                      |\n| 910b-     |             |           |           |            |           |                       |                      |\n| a71c48363 |             |           |           |            |           |                       |                      |\n| 46f       |             |           |           |            |           |                       |                      |\n+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n</code></pre>"},{"location":"howto/openstack/neutron/create-security-groups/#allowing-ssh-access","title":"Allowing SSH access","text":"<p>The next thing to do is allow SSH access on port 22 for IPv4 and IPv6 client connections \u2013 but only from specific addresses or subnets.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>To do this, while on the Rules tab, click on the green Create new rule button. A pane named Create a Security Group Rule will slide over from the right-hand side of the browser window.</p> <p>For the IPv4 ingress SSH rule, make sure you set Protocol to TCP, Direction to Ingress, and Ether\u00a0Type to IPv4. Then, set From to Network/IP and, in the Custom\u00a0CIDR text box below, type in either the IPv4 address of your client host or the CIDR of your client subnet.</p> <p>To create the new rule, click the green Create button.</p> <p></p> <p>You may work similarly for the IPv6 ingress SSH rule; just be sure to set Ether\u00a0Type to IPv6.</p> <p></p> <p>When you are done creating the two ingress rules for SSH, you see them listed in the Rules tab of the security group.</p> <p></p> <p>To create this rule for IPv4 client connections, use the following command:</p> <pre><code>openstack security group rule create \\\n  --protocol tcp --dst-port 22 \\\n  --remote-ip 203.0.113.0/24 \\\n  default\n</code></pre> <p>To create the same rule but this time for IPv6 client connections, use a command like the following:</p> <pre><code>openstack security group rule create \\\n  --protocol tcp --dst-port 22 --ethertype IPv6 \\\n  --remote-ip 2001:db8::/32 \\\n  default\n</code></pre> <p>If you don\u2019t know your IPv4 or IPv6 address, visit icanhazip.com.</p> <p>In this example, your IPv4 address is 203.0.113.58, and if you want to allow SSH access from this address only, enter <code>203.0.113.58/32</code> as CIDR. If you want to allow SSH access from any address in that Class C subnet, instead enter <code>203.0.113.0/24</code> as CIDR. Regarding the IPv6 address, in the example we use the <code>2001:db8::/32</code> address block. Alternatively, you may use a single IPv6 address, like <code>2001:db8:ffff:ffff:ffff:ffff:ffff:ffff/128</code>.</p>"},{"location":"howto/openstack/neutron/create-security-groups/#allowing-web-traffic","title":"Allowing Web Traffic","text":"<p>Next, create the rules that allow anyone to access a server on port 80 and port 443.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Following a similar routine as before, begin by clicking on the green Create new rule button. To create an ingress rule for IPv4 connections to 80/TCP, set Protocol, Direction, and Ether\u00a0Type accordingly. Then, in each of the two Port range text boxes, type in <code>80</code>. This time, leave CIDR empty, essentially allowing incoming traffic from any IPv4 client. Click the green Create button to instantiate the new rule.</p> <p></p> <p>The same for the ingress rule for IPv4 connections to 443/TCP. The only difference is in the Port range text boxes; in each of the two, you should now type <code>443</code>.</p> <p></p> <p>Just like you did for incoming IPv4 connections, create a new ingress rule for IPv6 clients connecting to 80/TCP\u2026</p> <p></p> <p>\u2026and a new ingress rule for IPv6 clients connecting to 443/TCP.</p> <p></p> <p>When you are done creating the new ingress rules, you will see them all listed in the Rules tab of the <code>default</code> security group.</p> <p></p> <p>This time don\u2019t specify <code>--remote-ip</code>, to allow traffic from all IPv4 and IPv6 sources:</p> <pre><code>openstack security group rule create --protocol tcp --dst-port 80 default\n</code></pre> <pre><code>openstack security group rule create --protocol tcp --dst-port 80 --ethertype IPv6 default\n</code></pre> <p>One more time for port 443:</p> <pre><code>openstack security group rule create --protocol tcp --dst-port 443 default\n</code></pre> <pre><code>openstack security group rule create --protocol tcp --dst-port 443 --ethertype IPv6 default\n</code></pre> <p>To view the updated rules, print them again:</p> <pre><code>openstack security group rule list default\n</code></pre> <pre><code>+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n| ID        | IP Protocol | Ethertype | IP Range  | Port Range | Direction | Remote Security Group | Remote Address Group |\n+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n| 742bcc46- | tcp         | IPv4      | 0.0.0.0/0 | 80:80      | ingress   | None                  | None                 |\n| beb5-     |             |           |           |            |           |                       |                      |\n| 47a5-     |             |           |           |            |           |                       |                      |\n| 8eb1-     |             |           |           |            |           |                       |                      |\n| eb35da800 |             |           |           |            |           |                       |                      |\n| 6ed       |             |           |           |            |           |                       |                      |\n| 86ab9224- | tcp         | IPv6      | ::/0      | 80:80      | ingress   | None                  | None                 |\n| 4120-     |             |           |           |            |           |                       |                      |\n| 11f0-     |             |           |           |            |           |                       |                      |\n| af79-     |             |           |           |            |           |                       |                      |\n| 5f799899a |             |           |           |            |           |                       |                      |\n| 9cb       |             |           |           |            |           |                       |                      |\n| ad4a19ef- | None        | IPv6      | ::/0      |            | egress    | None                  | None                 |\n| 7fab-     |             |           |           |            |           |                       |                      |\n| 4eba-     |             |           |           |            |           |                       |                      |\n| 9982-     |             |           |           |            |           |                       |                      |\n| e5b109be1 |             |           |           |            |           |                       |                      |\n| 21c       |             |           |           |            |           |                       |                      |\n| cef0cd36- | tcp         | IPv4      | 203.0.113 | 22:22      | ingress   | None                  | None                 |\n| ad78-     |             |           | .58/32    |            |           |                       |                      |\n| 4dbd-     |             |           |           |            |           |                       |                      |\n| b806-     |             |           |           |            |           |                       |                      |\n| 597300fd9 |             |           |           |            |           |                       |                      |\n| e6a       |             |           |           |            |           |                       |                      |\n| f53b1a12- | None        | IPv4      | 0.0.0.0/0 |            | egress    | None                  | None                 |\n| edbb-     |             |           |           |            |           |                       |                      |\n| 480b-     |             |           |           |            |           |                       |                      |\n| 910b-     |             |           |           |            |           |                       |                      |\n| a71c48363 |             |           |           |            |           |                       |                      |\n| 46f       |             |           |           |            |           |                       |                      |\n| f90c598c- | tcp         | IPv4      | 0.0.0.0/0 | 443:443    | ingress   | None                  | None                 |\n| 3a5e-     |             |           |           |            |           |                       |                      |\n| 459f-     |             |           |           |            |           |                       |                      |\n| 8ed3-     |             |           |           |            |           |                       |                      |\n| 3c2538e7a |             |           |           |            |           |                       |                      |\n| 24f       |             |           |           |            |           |                       |                      |\n| dde1a0d8- | tcp         | IPv6      | ::/0      | 443:443    | ingress   | None                  | None                 |\n| 4120-     |             |           |           |            |           |                       |                      |\n| 11f0-     |             |           |           |            |           |                       |                      |\n| acdc-     |             |           |           |            |           |                       |                      |\n| 93df8d8fa |             |           |           |            |           |                       |                      |\n| fae       |             |           |           |            |           |                       |                      |\n+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n</code></pre> <p>All the rules for a simple web server are now in place.</p> <p>For any additional protocol or ingress rule, simply follow the same procedure as above.</p>"},{"location":"howto/openstack/neutron/create-security-groups/#adjusting-a-permissive-default-security-group","title":"Adjusting a permissive default security group","text":"<p>If your Cleura\u00a0Cloud account was created before 2022-11-16, and you didn\u2019t configure the <code>default</code> security group, it is most likely permissive for all incoming traffic. We recommend either creating and using a new security group, other than the <code>default</code> one, or restricting ingress traffic to specific ports and sources.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>To check how your <code>default</code> security group is configured, click on it and select the Rules tab to view its rules. If you have an old, permissive <code>default</code> group, the rules should look like this:</p> <p></p> <p>The ingress rules with the remote access filters of <code>::/0</code> and <code>0.0.0.0/0</code>, mean that incoming traffic from all sources is allowed.</p> <p>If you want to use the <code>default</code> group, remove the two ingress rules that allow all incoming traffic. Click on the  button on the right-hand side for both ingress rule rows.</p> <p>Your <code>default</code> or newly created security group rules will now look like the following.</p> <p></p> <p>To view the rules, use the following command:</p> <pre><code>openstack security group rule list default\n</code></pre> <p>The printout will be similar to this:</p> <pre><code>+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n| ID        | IP Protocol | Ethertype | IP Range  | Port Range | Direction | Remote Security Group | Remote Address Group |\n+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n| 5e5e9f4d- | None        | IPv4      | 0.0.0.0/0 |            | ingress   | None                  | None                 |\n| 1faa-     |             |           |           |            |           |                       |                      |\n| 492d-     |             |           |           |            |           |                       |                      |\n| 91f1-     |             |           |           |            |           |                       |                      |\n| c105b4640 |             |           |           |            |           |                       |                      |\n| 72b       |             |           |           |            |           |                       |                      |\n| 86b9413a- | None        | IPv6      | ::/0      |            | ingress   | None                  | None                 |\n| ad23-     |             |           |           |            |           |                       |                      |\n| 46c4-     |             |           |           |            |           |                       |                      |\n| a35e-     |             |           |           |            |           |                       |                      |\n| 9306945dc |             |           |           |            |           |                       |                      |\n| 63c       |             |           |           |            |           |                       |                      |\n| ad4a19ef- | None        | IPv6      | ::/0      |            | egress    | None                  | None                 |\n| 7fab-     |             |           |           |            |           |                       |                      |\n| 4eba-     |             |           |           |            |           |                       |                      |\n| 9982-     |             |           |           |            |           |                       |                      |\n| e5b109be1 |             |           |           |            |           |                       |                      |\n| 21c       |             |           |           |            |           |                       |                      |\n| f53b1a12- | None        | IPv4      | 0.0.0.0/0 |            | egress    | None                  | None                 |\n| edbb-     |             |           |           |            |           |                       |                      |\n| 480b-     |             |           |           |            |           |                       |                      |\n| 910b-     |             |           |           |            |           |                       |                      |\n| a71c48363 |             |           |           |            |           |                       |                      |\n| 46f       |             |           |           |            |           |                       |                      |\n+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n</code></pre> <p>If the ingress rules have <code>::/0</code> and <code>0.0.0.0/0</code> values in the <code>IP Range</code> column, and <code>None</code> in the <code>Remote Security Group</code>, then incoming traffic from any source is allowed.</p> <p>The IDs of the two ingress rules, one for IPv4 traffic and one for IPv6, are: <code>5e5e9f4d-1faa-492d-91f1-c105b464072b</code> and <code>86b9413a-ad23-46c4-a35e-9306945dc63c</code> respectively.</p> <p>Delete them by using the following command:</p> <p><pre><code>openstack security group rule delete \\\n  5e5e9f4d-1faa-492d-91f1-c105b464072b 86b9413a-ad23-46c4-a35e-9306945dc63c\n</code></pre> Print the rules again:</p> <pre><code>openstack security group rule list default\n</code></pre> <p>Now the remaining rules are only the egress ones.</p> <pre><code>+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n| ID        | IP Protocol | Ethertype | IP Range  | Port Range | Direction | Remote Security Group | Remote Address Group |\n+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n| ad4a19ef- | None        | IPv6      | ::/0      |            | egress    | None                  | None                 |\n| 7fab-     |             |           |           |            |           |                       |                      |\n| 4eba-     |             |           |           |            |           |                       |                      |\n| 9982-     |             |           |           |            |           |                       |                      |\n| e5b109be1 |             |           |           |            |           |                       |                      |\n| 21c       |             |           |           |            |           |                       |                      |\n| f53b1a12- | None        | IPv4      | 0.0.0.0/0 |            | egress    | None                  | None                 |\n| edbb-     |             |           |           |            |           |                       |                      |\n| 480b-     |             |           |           |            |           |                       |                      |\n| 910b-     |             |           |           |            |           |                       |                      |\n| a71c48363 |             |           |           |            |           |                       |                      |\n| 46f       |             |           |           |            |           |                       |                      |\n+-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+\n</code></pre>"},{"location":"howto/openstack/neutron/delete-network/","title":"Deleting networks","text":"<p>Deleting a network in Cleura\u00a0Cloud may sound like a pretty straightforward task \u2014 and it is. It\u2019s just that before deleting a network, there are some steps we almost always need to take. In what follows we show, step by step and through specific examples, how we delete networks using either the Cleura\u00a0Cloud Management\u00a0Panel or the OpenStack CLI.</p>"},{"location":"howto/openstack/neutron/delete-network/#prerequisites","title":"Prerequisites","text":"<p>Whether you choose to work from the Cleura\u00a0Cloud Management\u00a0Panel or with the OpenStack CLI, you need to have an account in Cleura\u00a0Cloud. Additionally, to use the OpenStack CLI, make sure to enable it for the region you will be working in.</p>"},{"location":"howto/openstack/neutron/delete-network/#selecting-a-network","title":"Selecting a network","text":"<p>Unless you already have the ID or know the name of the network you wish to delete, you may first list all available networks.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Fire up your favorite web browser, navigate to the Cleura\u00a0Cloud Management\u00a0Panel start page, and log into your Cleura\u00a0Cloud account.</p> <p>In the vertical pane on the left-hand side of the dashboard, expand the Networking section and click Networks. In the central pane of the page, you will see all networks in all regions you have access to. For the purposes of this guide, let us assume you no longer need network <code>carmacks</code>, so now you want to delete it.</p> <p></p> <p>To list all available networks in the region you are currently in, type the following:</p> <p><pre><code>openstack network list --internal\n</code></pre> <pre><code>+--------------------------------------+--------------+--------------------------------------+\n| ID                                   | Name         | Subnets                              |\n+--------------------------------------+--------------+--------------------------------------+\n| 1f94d315-7ca1-4d44-acc1-09c6c650df74 | mayo         |                                      |\n| 9b127d2c-01d7-4803-994f-f88292870c1d | teslin       | bd1d0ff2-7270-4a9a-a7ad-fff47e997e7b |\n| cb0a298a-bbb6-4ad6-832a-1456dafe45db | carmacks     | 7fa9e5a2-7d5a-466e-b120-7d2bffb99ce5 |\n| e0c4ce17-2722-4777-8140-d6c87479e190 | network-kna1 | 421d8fd2-dd7f-4f7c-9a51-42ef4a866dd9 |\n+--------------------------------------+--------------+--------------------------------------+\n</code></pre></p> <p>Let us assume you wish to delete the network named <code>carmacks</code>.</p>"},{"location":"howto/openstack/neutron/delete-network/#determining-component-dependencies","title":"Determining component dependencies","text":"<p>If the network to be deleted has a subnet component \u2014 and most likely it will have \u2014, you will first have to delete the subnet before deleting the network. If, in addition, the network is behind a router (figurately speaking), then before deleting the subnet, you will have to disconnect it from the router. Finally, you will have the option to delete the router also. Let us see what the situation is with network <code>carmacks</code>.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>For more information on <code>carmacks</code>, click the three-dot icon (right-hand side of the network row) and select View details. Four tabs immediately appear below; Details, Ports, Subnets, and Routers. Looking at the Details tab, it is clear that network <code>carmacks</code> has a subnet and is behind a router. You may click on tabs Subnets and Routers, to see more information regarding the network subnet and the router in front of the network.</p> <p></p> <p>To quickly check whether network <code>carmacks</code> has a subnet or not, type:</p> <p><pre><code>openstack network show carmacks -c subnets\n</code></pre> <pre><code>+---------+--------------------------------------+\n| Field   | Value                                |\n+---------+--------------------------------------+\n| subnets | 7fa9e5a2-7d5a-466e-b120-7d2bffb99ce5 |\n+---------+--------------------------------------+\n</code></pre></p> <p>If the value for the field <code>subnets</code> is non-empty, like in the example output above, that means the network has a subnet indeed, and the value is the ID of that subnet. At this point, it helps to assign the subnet ID to an environment variable, like so:</p> <pre><code>SUBNET_ID=\"7fa9e5a2-7d5a-466e-b120-7d2bffb99ce5\"\n</code></pre> <p>What about a router in front of <code>carmacks</code>? You might try checking the output of this command:</p> <p><pre><code>openstack network show carmacks\n</code></pre> <pre><code>+---------------------------+--------------------------------------+\n| Field                     | Value                                |\n+---------------------------+--------------------------------------+\n| admin_state_up            | UP                                   |\n| availability_zone_hints   |                                      |\n| availability_zones        | nova                                 |\n| created_at                | 2022-12-09T18:52:09Z                 |\n| description               |                                      |\n| dns_domain                | None                                 |\n| id                        | cb0a298a-bbb6-4ad6-832a-1456dafe45db |\n| ipv4_address_scope        | None                                 |\n| ipv6_address_scope        | None                                 |\n| is_default                | None                                 |\n| is_vlan_transparent       | None                                 |\n| mtu                       | 1500                                 |\n| name                      | carmacks                             |\n| port_security_enabled     | True                                 |\n| project_id                | 94109c764a754e24ac0f6b01aef82359     |\n| provider:network_type     | None                                 |\n| provider:physical_network | None                                 |\n| provider:segmentation_id  | None                                 |\n| qos_policy_id             | None                                 |\n| revision_number           | 2                                    |\n| router:external           | Internal                             |\n| segments                  | None                                 |\n| shared                    | False                                |\n| status                    | ACTIVE                               |\n| subnets                   | 7fa9e5a2-7d5a-466e-b120-7d2bffb99ce5 |\n| tags                      |                                      |\n| tenant_id                 | 94109c764a754e24ac0f6b01aef82359     |\n| updated_at                | 2022-12-09T18:55:18Z                 |\n+---------------------------+--------------------------------------+\n</code></pre></p> <p>While it usually pays off to use <code>openstack</code> commands with the verb <code>show</code> on various objects, in this case, you don\u2019t get what you\u2019re looking for \u2014 which is an indication of the presence or absence of a router in front of <code>carmacks</code>. In cases like this, try looking at things from a different vantage point. Try, in particular, to list all routers:</p> <p><pre><code>openstack router list\n</code></pre> <pre><code>+------------------------+-----------------+--------+-------+------------------------+------+\n| ID                     | Name            | Status | State | Project                | HA   |\n+------------------------+-----------------+--------+-------+------------------------+------+\n| 5ac45739-a379-4936-    | router-kna1     | ACTIVE | UP    | 94109c764a754e24ac0f6b | True |\n| 8b1b-67d10e017f4d      |                 |        |       | 01aef82359             |      |\n| 79ff91ae-91b5-4991-    | carmacks-router | ACTIVE | UP    | 94109c764a754e24ac0f6b | True |\n| af61-91e923fac87b      |                 |        |       | 01aef82359             |      |\n+------------------------+-----------------+--------+-------+------------------------+------+\n</code></pre></p> <p>The name of the second router says it all, but since it is just a name, it doesn\u2019t hurt to verify the role of this router:</p> <p><pre><code>openstack router show carmacks-router -c interfaces_info\n</code></pre> <pre><code>+-----------------+--------------------------------------------------------------------------------+\n| Field           | Value                                                                          |\n+-----------------+--------------------------------------------------------------------------------+\n| interfaces_info | [{\"port_id\": \"439bc9d5-c8a9-4de1-93b9-b01e69258a56\", \"ip_address\": \"10.1.0.1\", |\n|                 | \"subnet_id\": \"7fa9e5a2-7d5a-466e-b120-7d2bffb99ce5\"}]                          |\n+-----------------+--------------------------------------------------------------------------------+\n</code></pre></p> <p>Looking at the value of <code>interfaces_info</code>, it is easy to see that <code>subnet_id</code> has the value of the variable <code>SUBNET_ID</code> you just instantiated. In other words, router <code>carmacks-router</code> is indeed in front of network <code>carmacks</code>.</p> <p>There will be times when router names won\u2019t help much. Then, try a more exhaustive search approach:</p> <p><pre><code>for i in $(openstack router list -f value -c Name); \\\n    do echo Checking router \"$i\"; \\\n    openstack router show \"$i\" -f json -c interfaces_info \\\n    | grep \"$SUBNET_ID\"; \\\ndone\n</code></pre> <pre><code>Checking router carmacks-router\n      \"subnet_id\": \"7fa9e5a2-7d5a-466e-b120-7d2bffb99ce5\"\nChecking router router-kna1\n</code></pre></p>"},{"location":"howto/openstack/neutron/delete-network/#tearing-down-networks","title":"Tearing down networks","text":"<p>Now that you know you\u2019re dealing with a full-blown network and a router, you start by disconnecting the subnet from the router. Then, you will move on to deleting the subnet and the network, and after that, you can finish up with deleting the router.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Go to the Subnets tab of the <code>carmacks</code> network, and click the gray notepad-and-pen icon (at the left of the red circle-with-trashcan icon).</p> <p></p> <p>A vertical pane titled Modify Subnet will slide over from the right-hand side of the page. Pay attention to the Router Connections section. You will notice an active connection to the router. Click the red circle-with-line-over-chainlink icon to deactivate the connection, effectively disconnecting the subnet from the router.</p> <p></p> <p>A pop-up window will appear, asking if you really want to go ahead with the disconnection. Just click the red Yes, Remove interface button.</p> <p></p> <p>After disconnecting the subnet, click the red circle-and-trashcan icon to delete it. Once more, a pop-up will appear asking for confirmation. Click the red Yes, Delete button.</p> <p></p> <p>As soon as you delete the subnet, in the Subnets tab you will see the message No subnets found.</p> <p></p> <p>You can now delete the network. Click the three-dot icon (right-hand side of the network row) and select Delete Network.</p> <p></p> <p>Of course, you will have to confirm this action. Clicking the red Yes, Delete button is enough.</p> <p></p> <p>After deleting the network, it will not be on the list of all available networks.</p> <p></p> <p>There\u2019s still that router lying around, and if you have no use for it, go to the Routers page to delete it. In the vertical pane on the left, expand the Networking section and click on Routers. In the central pane of the page, you will see all routers in all regions you have access to.</p> <p></p> <p>Click the red three-dot icon of the router you wish to delete and select Delete Router. A pop-up will appear asking for confirmation, so click the red Yes, Delete button.</p> <p></p> <p>After successfully deleting the router, there will be no trace of it in the list of all routers.</p> <p></p> <p>First, take a look at all available subnets:</p> <p><pre><code>openstack subnet list\n</code></pre> <pre><code>+-------------------------------+-----------------+--------------------------------+---------------+\n| ID                            | Name            | Network                        | Subnet        |\n+-------------------------------+-----------------+--------------------------------+---------------+\n| 421d8fd2-dd7f-4f7c-9a51-      | subnet-kna1     | e0c4ce17-2722-4777-8140-       | 10.15.20.0/24 |\n| 42ef4a866dd9                  |                 | d6c87479e190                   |               |\n| 7fa9e5a2-7d5a-466e-b120-      | carmacks-subnet | cb0a298a-bbb6-4ad6-832a-       | 10.1.0.0/24   |\n| 7d2bffb99ce5                  |                 | 1456dafe45db                   |               |\n| bd1d0ff2-7270-4a9a-a7ad-      | teslin-subnet   | 9b127d2c-01d7-4803-994f-       | 10.254.0.0/24 |\n| fff47e997e7b                  |                 | f88292870c1d                   |               |\n+-------------------------------+-----------------+--------------------------------+---------------+\n</code></pre></p> <p>As you would expect, included on the list is subnet <code>carmacks-subnet</code>, which you are about to delete. That\u2019s easier said than done, though:</p> <p><pre><code>openstack subnet delete $SUBNET_ID\n</code></pre> <pre><code>Failed to delete subnet with name or ID '7fa9e5a2-7d5a-466e-b120-7d2bffb99ce5': ConflictException: 409:\nClient Error for url: kna1.citycloud.com:9696/v2.0/subnets/7fa9e5a2-7d5a-466e-b120-7d2bffb99ce5,\nUnable to complete operation on subnet 7fa9e5a2-7d5a-466e-b120-7d2bffb99ce5:\nOne or more ports have an IP allocation from this subnet.\n1 of 1 subnets failed to delete.\n</code></pre></p> <p>The trick here is to first disconnect the subnet from the corresponding router, which is perfectly doable from the side of the router. As we discovered a bit earlier, the router we are talking about is <code>carmacks-router</code>:</p> <pre><code>openstack router remove subnet carmacks-router $SUBNET_ID\n</code></pre> <p>If the command above is successful, you will see no output on your terminal. Now, an attempt to delete <code>carmacks-subnet</code> should go through with flying colors:</p> <pre><code>openstack subnet delete $SUBNET_ID\n</code></pre> <p>Again, no command output means success, but we suggest you check yourself:</p> <p><pre><code>openstack subnet list\n</code></pre> <pre><code>+--------------------------------+---------------+---------------------------------+---------------+\n| ID                             | Name          | Network                         | Subnet        |\n+--------------------------------+---------------+---------------------------------+---------------+\n| 421d8fd2-dd7f-4f7c-9a51-       | subnet-kna1   | e0c4ce17-2722-4777-8140-        | 10.15.20.0/24 |\n| 42ef4a866dd9                   |               | d6c87479e190                    |               |\n| bd1d0ff2-7270-4a9a-a7ad-       | teslin-subnet | 9b127d2c-01d7-4803-994f-        | 10.254.0.0/24 |\n| fff47e997e7b                   |               | f88292870c1d                    |               |\n+--------------------------------+---------------+---------------------------------+---------------+\n</code></pre></p> <p>The subnet <code>carmacks-subnet</code> is not on the list, which is what you wanted exactly. Next is network <code>carmacks</code>, which you should be able to delete by now. First, take a look at all available networks:</p> <p><pre><code>openstack network list --internal\n</code></pre> <pre><code>+--------------------------------------+--------------+--------------------------------------+\n| ID                                   | Name         | Subnets                              |\n+--------------------------------------+--------------+--------------------------------------+\n| 1f94d315-7ca1-4d44-acc1-09c6c650df74 | mayo         |                                      |\n| 9b127d2c-01d7-4803-994f-f88292870c1d | teslin       | bd1d0ff2-7270-4a9a-a7ad-fff47e997e7b |\n| cb0a298a-bbb6-4ad6-832a-1456dafe45db | carmacks     |                                      |\n| e0c4ce17-2722-4777-8140-d6c87479e190 | network-kna1 | 421d8fd2-dd7f-4f7c-9a51-42ef4a866dd9 |\n+--------------------------------------+--------------+--------------------------------------+\n</code></pre></p> <p>Network <code>carmacks</code> is on the list, and by looking at the <code>Subnets</code> column, you see that it has no subnet. That\u2019s expected, so go ahead and delete the network:</p> <pre><code>openstack network delete carmacks\n</code></pre> <p>No command output signals success, but it never hurts to verify yourself:</p> <p><pre><code>openstack network list --internal\n</code></pre> <pre><code>+--------------------------------------+--------------+--------------------------------------+\n| ID                                   | Name         | Subnets                              |\n+--------------------------------------+--------------+--------------------------------------+\n| 1f94d315-7ca1-4d44-acc1-09c6c650df74 | mayo         |                                      |\n| 9b127d2c-01d7-4803-994f-f88292870c1d | teslin       | bd1d0ff2-7270-4a9a-a7ad-fff47e997e7b |\n| e0c4ce17-2722-4777-8140-d6c87479e190 | network-kna1 | 421d8fd2-dd7f-4f7c-9a51-42ef4a866dd9 |\n+--------------------------------------+--------------+--------------------------------------+\n</code></pre></p> <p>Network <code>carmacks</code> is gone, and if you have no use of <code>carmacks-router</code>, go ahead and delete it:</p> <pre><code>openstack router delete carmacks-router\n</code></pre> <p>There is no output on the terminal, and yet the router is gone:</p> <p><pre><code>openstack router list\n</code></pre> <pre><code>+-----------------------------+-------------+--------+-------+------------------------------+------+\n| ID                          | Name        | Status | State | Project                      | HA   |\n+-----------------------------+-------------+--------+-------+------------------------------+------+\n| 5ac45739-a379-4936-8b1b-    | router-kna1 | ACTIVE | UP    | 94109c764a754e24ac0f6b01aef8 | True |\n| 67d10e017f4d                |             |        |       | 2359                         |      |\n+-----------------------------+-------------+--------+-------+------------------------------+------+\n</code></pre></p>"},{"location":"howto/openstack/neutron/delete-network/#networks-with-a-subnet-but-no-router","title":"Networks with a subnet but no router","text":"<p>These are faster to delete, for there is no router to disconnect the subnet from. For our demonstration, we created network <code>teslin</code>, with subnet <code>teslin-subnet</code> and no router in front of it.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>In the vertical pane on the left-hand side of the dashboard, expand the Networking section and click Networks. In the central pane of the page, you will see all networks in all regions you have access to. Select a network with a subnet and no router \u2014 like <code>teslin</code> in our example.</p> <p>Looking at the network details, it is immediately apparent that there\u2019s no router in front of it.</p> <p></p> <p>Go to the Subnets tab, and click the red circle-with-trashcan icon to delete the subnet.</p> <p></p> <p>Then, click the red three-dot icon at the right-hand side of the <code>teslin</code> row, and select Delete Network.</p> <p></p> <p>Let us first take a look at all available networks\u2026</p> <p><pre><code>openstack network list --internal\n</code></pre> <pre><code>+--------------------------------------+--------------+--------------------------------------+\n| ID                                   | Name         | Subnets                              |\n+--------------------------------------+--------------+--------------------------------------+\n| 1f94d315-7ca1-4d44-acc1-09c6c650df74 | mayo         |                                      |\n| 9b127d2c-01d7-4803-994f-f88292870c1d | teslin       | bd1d0ff2-7270-4a9a-a7ad-fff47e997e7b |\n| e0c4ce17-2722-4777-8140-d6c87479e190 | network-kna1 | 421d8fd2-dd7f-4f7c-9a51-42ef4a866dd9 |\n+--------------------------------------+--------------+--------------------------------------+\n</code></pre></p> <p>\u2026and at all available subnets:</p> <p><pre><code>openstack subnet list\n</code></pre> <pre><code>+--------------------------------+---------------+---------------------------------+---------------+\n| ID                             | Name          | Network                         | Subnet        |\n+--------------------------------+---------------+---------------------------------+---------------+\n| 421d8fd2-dd7f-4f7c-9a51-       | subnet-kna1   | e0c4ce17-2722-4777-8140-        | 10.15.20.0/24 |\n| 42ef4a866dd9                   |               | d6c87479e190                    |               |\n| bd1d0ff2-7270-4a9a-a7ad-       | teslin-subnet | 9b127d2c-01d7-4803-994f-        | 10.254.0.0/24 |\n| fff47e997e7b                   |               | f88292870c1d                    |               |\n+--------------------------------+---------------+---------------------------------+---------------+\n</code></pre></p> <p>Since there is nothing to disconnect the <code>teslin-subnet</code> from, you may go ahead and delete the subnet:</p> <pre><code>openstack subnet delete teslin-subnet\n</code></pre> <p>There is no command output. This is expected, but why not check yourself?</p> <p><pre><code>openstack subnet list\n</code></pre> <pre><code>+---------------------------------+-------------+----------------------------------+---------------+\n| ID                              | Name        | Network                          | Subnet        |\n+---------------------------------+-------------+----------------------------------+---------------+\n| 421d8fd2-dd7f-4f7c-9a51-        | subnet-kna1 | e0c4ce17-2722-4777-8140-         | 10.15.20.0/24 |\n| 42ef4a866dd9                    |             | d6c87479e190                     |               |\n+---------------------------------+-------------+----------------------------------+---------------+\n</code></pre></p> <p>Finally, network <code>teslin</code> can go away with a single command:</p> <pre><code>openstack network delete teslin\n</code></pre> <p>The absence of any output means the command was successful. Take a look yourself:</p> <p><pre><code>openstack network list --internal\n</code></pre> <pre><code>+--------------------------------------+--------------+--------------------------------------+\n| ID                                   | Name         | Subnets                              |\n+--------------------------------------+--------------+--------------------------------------+\n| 1f94d315-7ca1-4d44-acc1-09c6c650df74 | mayo         |                                      |\n| e0c4ce17-2722-4777-8140-d6c87479e190 | network-kna1 | 421d8fd2-dd7f-4f7c-9a51-42ef4a866dd9 |\n+--------------------------------------+--------------+--------------------------------------+\n</code></pre></p>"},{"location":"howto/openstack/neutron/delete-network/#networks-with-no-subnet-and-no-router","title":"Networks with no subnet and no router","text":"<p>You may directly, without the slightest preparation, delete networks like these. For our demonstration, we created a network named <code>mayo</code>, with no subnet and no router in front of it.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>While viewing all available networks, click the red three-dot icon at the right-hand side of the <code>mayo</code> row and select Delete Network. You will have to confirm the action, and the network will be gone as soon as you do.</p> <p></p> <p>Once more, take a look at all remaining networks:</p> <p><pre><code>openstack network list --internal\n</code></pre> <pre><code>+--------------------------------------+--------------+--------------------------------------+\n| ID                                   | Name         | Subnets                              |\n+--------------------------------------+--------------+--------------------------------------+\n| 1f94d315-7ca1-4d44-acc1-09c6c650df74 | mayo         |                                      |\n| e0c4ce17-2722-4777-8140-d6c87479e190 | network-kna1 | 421d8fd2-dd7f-4f7c-9a51-42ef4a866dd9 |\n+--------------------------------------+--------------+--------------------------------------+\n</code></pre></p> <p>Since <code>mayo</code> has no subnet, issue a single command to delete it:</p> <pre><code>openstack network delete mayo\n</code></pre> <p>And, yes, it is still a good idea to check yourself:</p> <p><pre><code>openstack network list --internal\n</code></pre> <pre><code>+--------------------------------------+--------------+--------------------------------------+\n| ID                                   | Name         | Subnets                              |\n+--------------------------------------+--------------+--------------------------------------+\n| e0c4ce17-2722-4777-8140-d6c87479e190 | network-kna1 | 421d8fd2-dd7f-4f7c-9a51-42ef4a866dd9 |\n+--------------------------------------+--------------+--------------------------------------+\n</code></pre></p>"},{"location":"howto/openstack/neutron/delete-network/#recap-of-networks-and-towns","title":"Recap: Of networks and towns","text":"<p>Depending on the features of a Neutron network, deleting it may require some preparation work. For the purposes of this guide, we created three different networks with different characteristics; <code>carmacks</code>, <code>teslin</code>, and <code>mayo</code>. Then, either from the Cleura\u00a0Cloud Management\u00a0Panel or with the help of OpenStack CLI, we showed how we discover any component dependencies and how we work towards deletion. Eventually, all three test networks were gone. We should point out, though, that all three namesake towns in Yukon are still there.</p>"},{"location":"howto/openstack/neutron/multiple-public-ips/","title":"Assigning multiple public (floating) IPs to a server","text":"<p>In Cleura\u00a0Cloud, we do not pass external networks to the compute nodes. This means that you, as a user, can not directly attach a server to the public network.</p> <p>In order to provide connectivity to the public network (for IPv4), you need to use floating IPs. A floating IP is created in the public subnet, and is mapped to the specific network port. All traffic comes through a virtual router.</p> <p>For some scenarios, you might need to have more than one public IP assigned to a server. But in case of 1-to-1 NAT (which is how the floating IP is implemented under the hood) you can not assign more than one external IP to the internal one. And adding a new port to the VM is also not an option, since this would result in asymmetric routing, as replies will go through the first interface for which a default route is set.</p> <p>Instead, you must first configure an additional private (\u201cfixed\u201d) IP address for your port, then associate a public (\u201cfloating\u201d) IP address to map to it.</p>"},{"location":"howto/openstack/neutron/multiple-public-ips/#add-an-extra-ip-to-the-port","title":"Add an extra IP to the port","text":"<p>Assume you already have a network port inside your private network:</p> <pre><code>$ openstack port show 51dae637-ad79-4ba9-9e41-78e5e0f3332c -c fixed_ips\n+-----------+--------------------------------------------------------------------------+\n| Field     | Value                                                                    |\n+-----------+--------------------------------------------------------------------------+\n| fixed_ips | ip_address='10.2.0.58', subnet_id='5efeae9f-06b8-41a5-987f-085e8c7113a6' |\n+-----------+--------------------------------------------------------------------------+\n</code></pre> <p>And you also have a floating IP associated with it:</p> <pre><code>$ openstack floating ip list -c ID -c \"Floating IP Address\" -c \"Fixed IP Address\" -c Port\n+--------------------------------------+---------------------+------------------+--------------------------------------+\n| ID                                   | Floating IP Address | Fixed IP Address | Port                                 |\n+--------------------------------------+---------------------+------------------+--------------------------------------+\n| 989f8a96-4ab4-4190-83e4-25b71d309ea9 | 192.0.2.169         | 10.2.0.58        | 51dae637-ad79-4ba9-9e41-78e5e0f3332c |\n| c45a5eaf-2f3a-4679-89fe-266a5cbe840a | 198.51.100.12       | None             | None                                 |\n+--------------------------------------+---------------------+------------------+--------------------------------------+\n</code></pre> <p>Then what you need to do, is to add extra IP address to your existing port:</p> <pre><code>openstack port set --fixed-ip subnet=5efeae9f-06b8-41a5-987f-085e8c7113a6 51dae637-ad79-4ba9-9e41-78e5e0f3332c\n</code></pre> <p>You can then confirm that the port does have two entries in its <code>fixed_ips</code> list:</p> <pre><code>$ openstack port show 51dae637-ad79-4ba9-9e41-78e5e0f3332c -c fixed_ips\n+-----------+---------------------------------------------------------------------------+\n| Field     | Value                                                                     |\n+-----------+---------------------------------------------------------------------------+\n| fixed_ips | ip_address='10.2.0.228', subnet_id='5efeae9f-06b8-41a5-987f-085e8c7113a6' |\n|           | ip_address='10.2.0.58', subnet_id='5efeae9f-06b8-41a5-987f-085e8c7113a6'  |\n+-----------+---------------------------------------------------------------------------+\n</code></pre> <p>Don\u2019t forget to configure new IP as an alias to the interface inside your VM!</p> <p>When you have an IP address on your port that is not yet assigned to any floating IP, you can assign it to the new floating IP. Proceed with:</p> <pre><code>openstack floating ip set c45a5eaf-2f3a-4679-89fe-266a5cbe840a \\\n  --port 51dae637-ad79-4ba9-9e41-78e5e0f3332c \\\n  --fixed-ip-address 10.2.0.228\n</code></pre> <p>Then, list the floating (public) IP addresses, together with their fixed (private) counterparts:</p> <pre><code>$ openstack floating ip list -c ID -c \"Floating IP Address\" -c \"Fixed IP Address\" -c Port\n+--------------------------------------+---------------------+------------------+--------------------------------------+\n| ID                                   | Floating IP Address | Fixed IP Address | Port                                 |\n+--------------------------------------+---------------------+------------------+--------------------------------------+\n| 989f8a96-4ab4-4190-83e4-25b71d309ea9 | 192.0.2.169         | 10.2.0.58        | 51dae637-ad79-4ba9-9e41-78e5e0f3332c |\n| c45a5eaf-2f3a-4679-89fe-266a5cbe840a | 198.51.100.12       | 10.2.0.228       | 51dae637-ad79-4ba9-9e41-78e5e0f3332c |\n+--------------------------------------+---------------------+------------------+--------------------------------------+\n</code></pre> <p>Now your server is accessible through two different public IP addresses.</p>"},{"location":"howto/openstack/neutron/new-network/","title":"Creating new networks","text":"<p>Before creating a server in Cleura\u00a0Cloud, you need at least one network to make the new server a member of. Since you may have more than one network per region, let us now walk through creating a new network using the Cleura\u00a0Cloud Management\u00a0Panel or the OpenStack CLI.</p>"},{"location":"howto/openstack/neutron/new-network/#prerequisites","title":"Prerequisites","text":"<p>Whether you choose to work from the Cleura\u00a0Cloud Management\u00a0Panel or with the OpenStack CLI, you need to have an account in Cleura\u00a0Cloud. Additionally, to use the OpenStack CLI make sure to enable it first.</p>"},{"location":"howto/openstack/neutron/new-network/#creating-a-network","title":"Creating a network","text":"<p>To create a network from the Cleura\u00a0Cloud Management\u00a0Panel, fire up your favorite web browser, navigate to the Cleura Cloud start page, and login into your Cleura\u00a0Cloud account. On the other hand, if you prefer to work with OpenStack CLI, please do not forget to source the RC file first.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>On the top right-hand side of the Cleura\u00a0Cloud Management\u00a0Panel, click the Create button. A new pane will slide into view from the right-hand side of the browser window, titled Create.</p> <p></p> <p>You will notice several rounded boxes prominently displayed on that pane, each for defining, configuring, and instantiating a different Cleura\u00a0Cloud object. Go ahead and click the Network box. A new pane titled Create Network will slide over. At the top, type in a name and select one of the available regions for the new network.</p> <p></p> <p>Start by creating a new network. In our example, let\u2019s name it <code>nordostbahnhof</code>:</p> <pre><code>openstack network create nordostbahnhof\n</code></pre> <p>After issuing the command above, you immediately get information regarding the new network:</p> <pre><code>+---------------------------+------------------------------------------------------------+\n| Field                     | Value                                                      |\n+---------------------------+------------------------------------------------------------+\n| admin_state_up            | UP                                                         |\n| availability_zone_hints   |                                                            |\n| availability_zones        |                                                            |\n| created_at                | 2025-05-07T17:38:31Z                                       |\n| description               |                                                            |\n| dns_domain                |                                                            |\n| id                        | f1f1b1f6-2b65-11f0-9769-080027af5a23                       |\n| ipv4_address_scope        | None                                                       |\n| ipv6_address_scope        | None                                                       |\n| is_default                | False                                                      |\n| is_vlan_transparent       | None                                                       |\n| location                  | cloud='', project.domain_id=,                              |\n|                           | project.domain_name='CCP_Domain_pqrxy',                    |\n|                           | project.id='hdzqyxmxnixcdtgldkhhwdewfacyjdbub',            |\n|                           | project.name='Nuuk-to-Iqaluit', region_name='Fra1',        |\n|                           | zone=                                                      |\n| mtu                       | 1500                                                       |\n| name                      | nordostbahnhof                                             |\n| port_security_enabled     | True                                                       |\n| project_id                | hdzqyxmxnixcdtgldkhhwdewfacyjdbub                          |\n| provider:network_type     | None                                                       |\n| provider:physical_network | None                                                       |\n| provider:segmentation_id  | None                                                       |\n| qos_policy_id             | None                                                       |\n| revision_number           | 1                                                          |\n| router:external           | Internal                                                   |\n| segments                  | None                                                       |\n| shared                    | False                                                      |\n| status                    | ACTIVE                                                     |\n| subnets                   |                                                            |\n| tags                      |                                                            |\n| updated_at                | 2025-05-07T17:38:31Z                                       |\n+---------------------------+------------------------------------------------------------+\n</code></pre>"},{"location":"howto/openstack/neutron/new-network/#adding-a-subnet-and-a-router","title":"Adding a subnet and a router","text":"<p>Creating a new network does not necessarily mean it has all the features you most likely would expect. Unless you work from the Cleura\u00a0Cloud Management\u00a0Panel, where almost every component is activated for you with a few clicks here and there, when you use the OpenStack CLI there is some extra work you need to do before you get a network you would characterize as useful.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Expand the Advanced Options section below, make sure Port Security is enabled, and leave the MTU parameter blank.</p> <p></p> <p>Assuming you are going for a full-featured network, activate the Create a complete network, containing a subnet and a router option.</p> <p>Notice that, by default, this sets things up for the the creation of an IPv4-based subnet. In particular, by looking at the new Subnet Name (#1) section, it is immediately apparent that you get the following:</p> <ul> <li>A subnet expressed in CIDR notation,</li> <li>two DNS servers,</li> <li>a Gateway, and</li> <li>a DHCP server.</li> </ul> <p>For the Gateway IP, accept the suggested address, or type in one chosen from the subnet above. As for the DHCP, either accept the defaults or type in Start and End addresses \u2013 again chosen from the subnet above.</p> <p></p> <p>Alongside your IPv4-based subnet, you may also have an IPv6-based subnet; both subnets will be members of the network you are now creating.</p> <p>To create a second, IPv6-based subnet, click the orange  icon at the top of the Subnet Name (#1) section. In the new Subnet Name (#2) section that appears, be sure to set IP Version to IPv6 abd you will get the following:</p> <ul> <li>One 64bit-prefixed subnet with</li> <li>SLAAC addressing, and</li> <li>two DNS servers with IPv6 addresses.</li> </ul> <p></p> <p>Scroll down a bit if you have to. Assuming you want your cloud servers to reach hosts on the Internet, for the External network parameter make sure you select ext-net. Then, click the green Create button. In a few seconds, the new network will be readily available.</p> <p></p> <p>You now have to create a subnet for the new network. Since Neutron networks can have more than one subnets, and subnets can be either IPv4-based or IPv6-based, let us now create two; one IPv4-based (<code>nordostbahnhof-subnet-ipv4</code>), and one IPv4-based (<code>nordostbahnhof-subnet-ipv6</code>).</p> <p>For the IPv4-based subnet, type the following:</p> <pre><code>openstack subnet create nordostbahnhof-subnet-ipv4 \\\n    --network nordostbahnhof --subnet-range 10.20.30.0/24\n</code></pre> <p>You will get detailed information regarding subnet <code>nordostbahnhof-subnet-ipv4</code>:</p> <pre><code>+----------------------+-----------------------------------------------------------------+\n| Field                | Value                                                           |\n+----------------------+-----------------------------------------------------------------+\n| allocation_pools     | 10.20.30.2-10.20.30.254                                         |\n| cidr                 | 10.20.30.0/24                                                   |\n| created_at           | 2025-05-07T17:45:06Z                                            |\n| description          |                                                                 |\n| dns_nameservers      |                                                                 |\n| dns_publish_fixed_ip | None                                                            |\n| enable_dhcp          | True                                                            |\n| gateway_ip           | 10.20.30.1                                                      |\n| host_routes          |                                                                 |\n| id                   | 4231daa6-a619-468e-9283-520054810eca                            |\n| ip_version           | 4                                                               |\n| ipv6_address_mode    | None                                                            |\n| ipv6_ra_mode         | None                                                            |\n| location             | cloud='', project.domain_id=,                                   |\n|                      | project.domain_name='CCP_Domain_pqrxy',                         |\n|                      | project.id='hdzqyxmxnixcdtgldkhhwdewfacyjdbub',                 |\n|                      | project.name='Nuuk-to-Iqaluit', region_name='Fra1', zone=       |\n| name                 | nordostbahnhof-subnet-ipv4                                      |\n| network_id           | f1f1b1f6-2b65-11f0-9769-080027af5a23                            |\n| prefix_length        | None                                                            |\n| project_id           | hdzqyxmxnixcdtgldkhhwdewfacyjdbub                               |\n| revision_number      | 0                                                               |\n| segment_id           | None                                                            |\n| service_types        |                                                                 |\n| subnetpool_id        | None                                                            |\n| tags                 |                                                                 |\n| updated_at           | 2025-05-07T17:45:06Z                                            |\n+----------------------+-----------------------------------------------------------------+\n</code></pre> <p>For the IPv6-based subnet, type the following:</p> <pre><code>openstack subnet create nordostbahnhof-subnet-ipv6 --network nordostbahnhof \\\n    --ip-version 6 \\\n    --ipv6-ra-mode slaac \\\n    --ipv6-address-mode slaac \\\n    --use-default-subnet-pool\n</code></pre> <p>Again, you will get detailed information regarding subnet <code>nordostbahnhof-subnet-ipv6</code>:</p> <pre><code>+----------------------+-----------------------------------------------------------------+\n| Field                | Value                                                           |\n+----------------------+-----------------------------------------------------------------+\n| allocation_pools     | 2a03:b000:702:13::2-2a03:b000:702:13:ffff:ffff:ffff:ffff        |\n| cidr                 | 2a03:b000:702:13::/64                                           |\n| created_at           | 2025-05-07T17:48:33Z                                            |\n| description          |                                                                 |\n| dns_nameservers      |                                                                 |\n| dns_publish_fixed_ip | None                                                            |\n| enable_dhcp          | True                                                            |\n| gateway_ip           | 2a03:b000:702:13::1                                             |\n| host_routes          |                                                                 |\n| id                   | c787c7fc-9405-443c-99bf-fefd8df0ec7f                            |\n| ip_version           | 6                                                               |\n| ipv6_address_mode    | slaac                                                           |\n| ipv6_ra_mode         | slaac                                                           |\n| location             | cloud='', project.domain_id=,                                   |\n|                      | project.domain_name='CCP_Domain_pqrxy',                         |\n|                      | project.id='hdzqyxmxnixcdtgldkhhwdewfacyjdbub',                 |\n|                      | project.name='Nuuk-to-Iqaluit', region_name='Fra1', zone=       |\n| name                 | nordostbahnhof-subnet-ipv6                                      |\n| network_id           | f1f1b1f6-2b65-11f0-9769-080027af5a23                            |\n| prefix_length        | None                                                            |\n| project_id           | hdzqyxmxnixcdtgldkhhwdewfacyjdbub                               |\n| revision_number      | 0                                                               |\n| segment_id           | None                                                            |\n| service_types        |                                                                 |\n| subnetpool_id        | d21fa4e4-2b67-11f0-b570-080027af5a23                            |\n| tags                 |                                                                 |\n| updated_at           | 2025-05-07T17:48:33Z                                            |\n+----------------------+-----------------------------------------------------------------+\n</code></pre> <p>If you want servers connected to the <code>nordostbahnhof</code> network to have Internet access, you need a router in front of the network. Following our unofficial naming convention, go ahead and create a new router called <code>nordostbahnhof-router</code>:</p> <pre><code>openstack router create nordostbahnhof-router \n</code></pre> <p>As expected, you will see lots of information regarding the new router:</p> <pre><code>+-------------------------+--------------------------------------------------------------+\n| Field                   | Value                                                        |\n+-------------------------+--------------------------------------------------------------+\n| admin_state_up          | UP                                                           |\n| availability_zone_hints |                                                              |\n| availability_zones      |                                                              |\n| created_at              | 2025-05-07T17:52:39Z                                         |\n| description             |                                                              |\n| external_gateway_info   | null                                                         |\n| flavor_id               | None                                                         |\n| ha                      | True                                                         |\n| id                      | 1ca6aaf8-2b68-11f0-833a-080027af5a23                         |\n| location                | cloud='', project.domain_id=,                                |\n|                         | project.domain_name='CCP_Domain_pqrxy',                      |\n|                         | project.id='hdzqyxmxnixcdtgldkhhwdewfacyjdbub',              |\n|                         | project.name='Nuuk-to-Iqaluit', region_name='Fra1', zone=    |\n| name                    | nordostbahnhof-router                                        |\n| project_id              | hdzqyxmxnixcdtgldkhhwdewfacyjdbub                            |\n| revision_number         | 1                                                            |\n| routes                  |                                                              |\n| status                  | ACTIVE                                                       |\n| tags                    |                                                              |\n| updated_at              | 2025-05-07T17:52:39Z                                         |\n+-------------------------+--------------------------------------------------------------+\n</code></pre> <p>You want the <code>nordostbahnhof-router</code> connected to the external network. The name of this network is <code>ext-net</code>:</p> <pre><code>openstack router set nordostbahnhof-router --external-gateway ext-net\n</code></pre> <p>Please note that if the command above is successful, you will get no output on your terminal. There is one last step to take, and that is to connect router <code>nordostbahnhof-router</code> to the subnet <code>nordostbahnhof-subnet-ipv4</code>\u2026</p> <pre><code>openstack router add subnet nordostbahnhof-router nordostbahnhof-subnet-ipv4\n</code></pre> <p>\u2026and to the subnet <code>nordostbahnhof-subnet-ipv6</code> of network <code>nordostbahnhof</code>:</p> <pre><code>openstack router add subnet nordostbahnhof-router nordostbahnhof-subnet-ipv6\n</code></pre> <p>Again, if the commands above are successful, you will get no output.</p>"},{"location":"howto/openstack/neutron/new-network/#listing-networks-and-getting-information","title":"Listing networks and getting information","text":"<p>At any time, you may connect to the Cleura\u00a0Cloud Management\u00a0Panel, list all networks you have already created, and get detailed information for any of these networks. Alternatively, you may get all that information using the OpenStack CLI.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>You may see all defined networks, in all supported regions, by selecting Networking &gt; Networks (see the left-hand side pane on the Cleura\u00a0Cloud Management\u00a0Panel).</p> <p></p> <p>For more information regarding a specific network, select View details. Then, you can glance over all the details regarding the selected network\u2019s ports, subnets, and routers. For more information on the network subnets specifically, in the left-hand side pane on the Cleura\u00a0Cloud Management\u00a0Panel select Networking &gt; Subnets.</p> <p></p> <p>To list all available networks in a specific region, just type:</p> <pre><code>openstack network list\n</code></pre> <p>You can always ask for more specific results. For instance, to see all internal networks only, type the following:</p> <pre><code>openstack network list --internal\n</code></pre> <p>You can also get detailed information about a specific network:</p> <pre><code>openstack network show nordostbahnhof\n</code></pre> <p>At any time, type <code>openstack network list --help</code> or <code>openstack network show --help</code> to see how to get information regarding networks, and what specific pieces of information you can have.</p>"},{"location":"howto/openstack/neutron/vpnaas/","title":"Creating a VPN connection between regions","text":"<p>Thanks to the Openstack Neutron VPN as a Service (VPNaaS) feature, you can bridge two different regions via a site-to-site IPSec VPN connection. This is made possible without setting up and configuring a virtual machine in any one of the regions. On the contrary, you can quickly establish such a connection using the Cleura\u00a0Cloud Management\u00a0Panel or the OpenStack CLI. Let us demonstrate the process following both approaches.</p>"},{"location":"howto/openstack/neutron/vpnaas/#prerequisites","title":"Prerequisites","text":"<p>Whether you choose to work from the Cleura\u00a0Cloud Management\u00a0Panel or with the OpenStack CLI, you need to have an account in Cleura\u00a0Cloud. If you prefer to work with the OpenStack CLI, then in addition to the Python <code>openstackclient</code> module, you need to install the Python <code>neutronclient</code> module also. Use either the package manager of your operating system or <code>pip</code>:</p> Debian/UbuntuMac OS X with HomebrewPython Package <pre><code>apt install python3-neutronclient\n</code></pre> <p>This Python module is unavailable via <code>brew</code>, but you can install it via <code>pip</code>.</p> <pre><code>pip install python-neutronclient\n</code></pre>"},{"location":"howto/openstack/neutron/vpnaas/#creating-a-vpn-connection-between-two-regions","title":"Creating a VPN connection between two regions","text":"<p>To create and establish such a connection from the Cleura\u00a0Cloud Management\u00a0Panel, fire up your favorite web browser, navigate to the Cleura\u00a0Cloud Management\u00a0Panel start page, and log into your Cleura\u00a0Cloud account. Should you decide to follow the OpenStack CLI route instead, please make sure you have the appropriate RC file for each region involved.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>On the top right-hand side of the Cleura\u00a0Cloud Management\u00a0Panel, click the Create button. A vertical pane titled Create will slide into view from the right-hand side of the browser window. You will notice several rounded boxes, each one for defining, configuring, and instantiating a different Cleura\u00a0Cloud object. Go ahead and click the VPN box.</p> <p></p> <p>A new pane titled Create a VPN Service will slide over. Between the two boxes, click the one titled Quick (Guided) Connect.</p> <p></p> <p>Type in a name for the new site-to-site VPN connection.</p> <p></p> <p>Select a region, project, and network for each of the two data centers involved.</p> <p></p> <p>Look at the pre-shared key and, optionally, expand the Advanced Options section to see all presets. You do not have to change anything there. When you are ready, click the green Create button. The VPN connection between the two regions will be established in a few seconds.</p> <p></p> <p>First, you need to have the RC files of the two regions you will be connecting. In the example that follows, we demonstrate establishing a site-to-site connection between regions <code>fra1</code> and <code>kna1</code>. This means that, while following through, before working in <code>fra1</code> you need to source the RC file for <code>fra1</code>, and before working in <code>kna1</code> you need to source the RC file for <code>kna1</code>.</p> <p>It helps to imagine the site-to-site connection schematically, with <code>fra1</code> being on the left side and <code>kna1</code> being on the right side of the connection. That is why we interchange the terms <code>fra1</code>, left and <code>kna1</code>, right.</p> <p>You also have to decide which subnets from either side you will connect. Additionally, you need to know the respective CIDR notations and routers. In the examples that follow, on the left side we have subnet <code>subnet-fra1</code> with CIDR <code>10.15.25.0/24</code> and router <code>router-fra1</code>, and on the right side we have subnet <code>subnet-kna1</code> with CIDR <code>10.15.20.0/24</code> and router <code>router-kna1</code>. For convenience, we have set the shell variables <code>SUBNET_FRA1</code> and <code>SUBNET_KNA1</code>:</p> <pre><code>SUBNET_FRA1=\"10.15.25.0/24\"\nSUBNET_KNA1=\"10.15.20.0/24\"\n</code></pre>"},{"location":"howto/openstack/neutron/vpnaas/#prepare-the-left-side-region-fra1","title":"Prepare the left side (region <code>fra1</code>)","text":"<p>Begin by creating a new IKE policy:</p> <pre><code>openstack vpn ike policy create ike-pol-fra1\n</code></pre> <pre><code>+-------------------------------+--------------------------------------+\n| Field                         | Value                                |\n+-------------------------------+--------------------------------------+\n| Authentication Algorithm      | sha1                                 |\n| Description                   |                                      |\n| Encryption Algorithm          | aes-128                              |\n| ID                            | 5782b141-afcc-4327-9ac5-b8cd2e110c6a |\n| IKE Version                   | v1                                   |\n| Lifetime                      | {'units': 'seconds', 'value': 3600}  |\n| Name                          | ike-pol-fra1                         |\n| Perfect Forward Secrecy (PFS) | group5                               |\n| Phase1 Negotiation Mode       | main                                 |\n| Project                       | dfc700467396428bacba4376e72cc3e9     |\n| project_id                    | dfc700467396428bacba4376e72cc3e9     |\n+-------------------------------+--------------------------------------+\n</code></pre> <p>Then, create a new IPSec policy:</p> <pre><code>openstack vpn ipsec policy create ipsec-pol-fra1\n</code></pre> <pre><code>+-------------------------------+--------------------------------------+\n| Field                         | Value                                |\n+-------------------------------+--------------------------------------+\n| Authentication Algorithm      | sha1                                 |\n| Description                   |                                      |\n| Encapsulation Mode            | tunnel                               |\n| Encryption Algorithm          | aes-128                              |\n| ID                            | 15ec761f-1642-49b6-b5a2-e43624c5752d |\n| Lifetime                      | {'units': 'seconds', 'value': 3600}  |\n| Name                          | ipsec-pol-fra1                       |\n| Perfect Forward Secrecy (PFS) | group5                               |\n| Project                       | dfc700467396428bacba4376e72cc3e9     |\n| Transform Protocol            | esp                                  |\n| project_id                    | dfc700467396428bacba4376e72cc3e9     |\n+-------------------------------+--------------------------------------+\n</code></pre> <p>You are ready to create a new VPN service:</p> <pre><code>openstack vpn service create --router router-fra1 vpn-service-fra1\n</code></pre> <pre><code>+----------------+--------------------------------------+\n| Field          | Value                                |\n+----------------+--------------------------------------+\n| Description    |                                      |\n| Flavor         | None                                 |\n| ID             | d74d51f0-182d-4d88-952a-1d593ce696fd |\n| Name           | vpn-service-fra1                     |\n| Project        | dfc700467396428bacba4376e72cc3e9     |\n| Router         | 62f885d8-6b13-4161-89d1-003c4fafec55 |\n| State          | True                                 |\n| Status         | PENDING_CREATE                       |\n| Subnet         | None                                 |\n| external_v4_ip | 198.51.100.50                        |\n| external_v6_ip | 2a03:b000:701:5:f816:3eff:feb5:be0e  |\n| project_id     | dfc700467396428bacba4376e72cc3e9     |\n+----------------+--------------------------------------+\n</code></pre> <p>Notice in the command output that the <code>Status</code> is <code>PENDING_CREATE</code>. This is expected. Also, jot down the value of the <code>external_v4_ip</code> parameter. Better yet, set this value to a new variable, <code>EXT_IP_FRA1</code>, for you will soon need it:</p> <pre><code>EXT_IP_FRA1=\"198.51.100.50\"\n</code></pre> <p>The site-to-site connection you are about to create needs two end-point groups on the left, and two end-point groups on the right. More specifically, on either side of the connection, there should be one end-point group for the local subnet and one end-point group for the peer (remote) subnet. You are now on the left side of the connection (region <code>fra1</code>), so begin with the left local end-point group\u2026</p> <pre><code>openstack vpn endpoint group create \\\n    --type subnet --value subnet-fra1 local-epg-fra1\n</code></pre> <pre><code>+-------------+------------------------------------------+\n| Field       | Value                                    |\n+-------------+------------------------------------------+\n| Description |                                          |\n| Endpoints   | ['df6fb6ca-4751-4b74-8b3e-5fbda0117cea'] |\n| ID          | 51895e0e-fa3a-43d3-8037-5eea073fb77f     |\n| Name        | local-epg-fra1                           |\n| Project     | dfc700467396428bacba4376e72cc3e9         |\n| Type        | subnet                                   |\n| project_id  | dfc700467396428bacba4376e72cc3e9         |\n+-------------+------------------------------------------+\n</code></pre> <p>\u2026and then move on to creating the left peer end-point group:</p> <pre><code>openstack vpn endpoint group create \\\n    --type cidr --value $SUBNET_KNA1 peer-epg-fra1\n</code></pre> <pre><code>+-------------+--------------------------------------+\n| Field       | Value                                |\n+-------------+--------------------------------------+\n| Description |                                      |\n| Endpoints   | ['10.15.20.0/24']                    |\n| ID          | a96bb9ef-d0ec-4174-93ae-a8e655910f94 |\n| Name        | peer-epg-fra1                        |\n| Project     | dfc700467396428bacba4376e72cc3e9     |\n| Type        | cidr                                 |\n| project_id  | dfc700467396428bacba4376e72cc3e9     |\n+-------------+--------------------------------------+\n</code></pre>"},{"location":"howto/openstack/neutron/vpnaas/#prepare-the-right-side-region-kna1","title":"Prepare the right side (region <code>kna1</code>)","text":"<p>Before establishing a site-to-site VPN connection between the two regions, you must make similar preparations on the right side of the connection (region <code>kna1</code>). You should adjust all commands you entered above and execute them on the right side. For your convenience, these are all the adjusted commands with the respective outputs:</p> <p>Create a new IKE policy:</p> <pre><code>openstack vpn ike policy create ike-pol-kna1\n</code></pre> <pre><code>+-------------------------------+--------------------------------------+\n| Field                         | Value                                |\n+-------------------------------+--------------------------------------+\n| Authentication Algorithm      | sha1                                 |\n| Description                   |                                      |\n| Encryption Algorithm          | aes-128                              |\n| ID                            | 6af4f52c-6522-483d-bb70-b144657489f3 |\n| IKE Version                   | v1                                   |\n| Lifetime                      | {'units': 'seconds', 'value': 3600}  |\n| Name                          | ike-pol-kna1                         |\n| Perfect Forward Secrecy (PFS) | group5                               |\n| Phase1 Negotiation Mode       | main                                 |\n| Project                       | 94109c764a754e24ac0f6b01aef82359     |\n| project_id                    | 94109c764a754e24ac0f6b01aef82359     |\n+-------------------------------+--------------------------------------+\n</code></pre> <p>Create a new IPSec policy:</p> <pre><code>openstack vpn ipsec policy create ipsec-pol-kna1\n</code></pre> <pre><code>+-------------------------------+--------------------------------------+\n| Field                         | Value                                |\n+-------------------------------+--------------------------------------+\n| Authentication Algorithm      | sha1                                 |\n| Description                   |                                      |\n| Encapsulation Mode            | tunnel                               |\n| Encryption Algorithm          | aes-128                              |\n| ID                            | 8f9c2219-a931-46eb-b3f5-22d76cbc89d0 |\n| Lifetime                      | {'units': 'seconds', 'value': 3600}  |\n| Name                          | ipsec-pol-kna1                       |\n| Perfect Forward Secrecy (PFS) | group5                               |\n| Project                       | 94109c764a754e24ac0f6b01aef82359     |\n| Transform Protocol            | esp                                  |\n| project_id                    | 94109c764a754e24ac0f6b01aef82359     |\n+-------------------------------+--------------------------------------+\n</code></pre> <p>Create a new VPN service:</p> <pre><code>openstack vpn service create --router router-kna1 vpn-service-kna1\n</code></pre> <pre><code>+----------------+--------------------------------------+\n| Field          | Value                                |\n+----------------+--------------------------------------+\n| Description    |                                      |\n| Flavor         | None                                 |\n| ID             | bb1a307d-6f8f-4a0a-83db-7c705403485d |\n| Name           | vpn-service-kna1                     |\n| Project        | 94109c764a754e24ac0f6b01aef82359     |\n| Router         | 5ac45739-a379-4936-8b1b-67d10e017f4d |\n| State          | True                                 |\n| Status         | PENDING_CREATE                       |\n| Subnet         | None                                 |\n| external_v4_ip | 203.0.113.101                        |\n| external_v6_ip | 2a00:16d8:0:3b:f816:3eff:fe0e:2074   |\n| project_id     | 94109c764a754e24ac0f6b01aef82359     |\n+----------------+--------------------------------------+\n</code></pre> <p>For convenience, set the value of parameter <code>external_v4_ip</code> to a shell variable:</p> <pre><code>EXT_IP_KNA1=\"203.0.113.101\"\n</code></pre> <p>Create a local end-point group:</p> <pre><code>openstack vpn endpoint group create \\\n    --type subnet --value subnet-kna1 local-epg-kna1\n</code></pre> <pre><code>+-------------+------------------------------------------+\n| Field       | Value                                    |\n+-------------+------------------------------------------+\n| Description |                                          |\n| Endpoints   | ['421d8fd2-dd7f-4f7c-9a51-42ef4a866dd9'] |\n| ID          | c1937c3d-77e7-4f2c-842d-70b5e10df9a8     |\n| Name        | local-epg-kna1                           |\n| Project     | 94109c764a754e24ac0f6b01aef82359         |\n| Type        | subnet                                   |\n| project_id  | 94109c764a754e24ac0f6b01aef82359         |\n+-------------+------------------------------------------+\n</code></pre> <p>Create a peer (remote) end-point group:</p> <pre><code>openstack vpn endpoint group create \\\n    --type cidr --value $SUBNET_FRA1 peer-epg-kna1\n</code></pre> <pre><code>+-------------+--------------------------------------+\n| Field       | Value                                |\n+-------------+--------------------------------------+\n| Description |                                      |\n| Endpoints   | ['10.15.25.0/24']                    |\n| ID          | 2e627315-02f0-4d68-8683-14230b166060 |\n| Name        | peer-epg-kna1                        |\n| Project     | 94109c764a754e24ac0f6b01aef82359     |\n| Type        | cidr                                 |\n| project_id  | 94109c764a754e24ac0f6b01aef82359     |\n+-------------+--------------------------------------+\n</code></pre>"},{"location":"howto/openstack/neutron/vpnaas/#instantiate-a-pre-shared-key","title":"Instantiate a pre-shared key","text":"<p>Before establishing a site-to-site IPSec VPN connection, you must have a randomly generated pre-shared key. You may use <code>openssl</code> for generating a random string and immediately set it to a shell variable:</p> <pre><code>PRE_SHARED_KEY=$(openssl rand -hex 24)\n</code></pre> <p>The above is just an example. The key should not necessarily be a hexadecimal string, nor do you have to use <code>openssl</code>. Another option would be to use the fine <code>pwgen</code> tool, for example like this:</p> <pre><code>PRE_SHARED_KEY=$(pwgen 64 1)\n</code></pre>"},{"location":"howto/openstack/neutron/vpnaas/#establish-a-left-to-right-connection-region-fra1","title":"Establish a left-to-right connection (region <code>fra1</code>)","text":"<p>To create a VPN connection from left to right, i.e., from region <code>fra1</code> to region <code>kna1</code>, issue the following command:</p> <pre><code>openstack vpn ipsec site connection create \\\n  --vpnservice vpn-service-fra1 \\\n  --ikepolicy ike-pol-fra1 \\\n  --ipsecpolicy ipsec-pol-fra1 \\\n  --local-endpoint-group local-epg-fra1 \\\n  --peer-address $EXT_IP_KNA1 \\\n  --peer-id $EXT_IP_KNA1 \\\n  --peer-endpoint-group peer-epg-fra1 \\\n  --psk $PRE_SHARED_KEY \\\n  vpn-conn-to-kna1\n</code></pre> <pre><code>+--------------------------+----------------------------------------------------+\n| Field                    | Value                                              |\n+--------------------------+----------------------------------------------------+\n| Authentication Algorithm | psk                                                |\n| Description              |                                                    |\n| ID                       | 5f44be31-0588-4f33-883d-9e2d97be55e1               |\n| IKE Policy               | 5782b141-afcc-4327-9ac5-b8cd2e110c6a               |\n| IPSec Policy             | 15ec761f-1642-49b6-b5a2-e43624c5752d               |\n| Initiator                | bi-directional                                     |\n| Local Endpoint Group ID  | 51895e0e-fa3a-43d3-8037-5eea073fb77f               |\n| Local ID                 |                                                    |\n| MTU                      | 1500                                               |\n| Name                     | vpn-conn-to-kna1                                   |\n| Peer Address             | 203.0.113.101                                      |\n| Peer CIDRs               |                                                    |\n| Peer Endpoint Group ID   | a96bb9ef-d0ec-4174-93ae-a8e655910f94               |\n| Peer ID                  | 203.0.113.101                                      |\n| Pre-shared Key           | de12db260ee1b9c0b9e624d910c9a9dbddec13dc24d60332   |\n| Project                  | dfc700467396428bacba4376e72cc3e9                   |\n| Route Mode               | static                                             |\n| State                    | True                                               |\n| Status                   | PENDING_CREATE                                     |\n| VPN Service              | d74d51f0-182d-4d88-952a-1d593ce696fd               |\n| dpd                      | {'action': 'hold', 'interval': 30, 'timeout': 120} |\n| project_id               | dfc700467396428bacba4376e72cc3e9                   |\n+--------------------------+----------------------------------------------------+\n</code></pre>"},{"location":"howto/openstack/neutron/vpnaas/#establish-a-right-to-left-connection-region-kna1","title":"Establish a right-to-left connection (region <code>kna1</code>)","text":"<p>Similarly, to create a VPN connection from right to left, i.e., from region <code>kna1</code> to region <code>fra1</code>, issue the following command:</p> <pre><code>openstack vpn ipsec site connection create \\\n  --vpnservice vpn-service-kna1 \\\n  --ikepolicy ike-pol-kna1 \\\n  --ipsecpolicy ipsec-pol-kna1 \\\n  --local-endpoint-group local-epg-kna1 \\\n  --peer-address $EXT_IP_FRA1 \\\n  --peer-id $EXT_IP_FRA1 \\\n  --peer-endpoint-group peer-epg-kna1 \\\n  --psk $PRE_SHARED_KEY \\\n  vpn-conn-to-fra1\n</code></pre> <pre><code>+--------------------------+----------------------------------------------------+\n| Field                    | Value                                              |\n+--------------------------+----------------------------------------------------+\n| Authentication Algorithm | psk                                                |\n| Description              |                                                    |\n| ID                       | 7705afc7-d0ff-444a-9474-3614e21d2399               |\n| IKE Policy               | 6af4f52c-6522-483d-bb70-b144657489f3               |\n| IPSec Policy             | 8f9c2219-a931-46eb-b3f5-22d76cbc89d0               |\n| Initiator                | bi-directional                                     |\n| Local Endpoint Group ID  | c1937c3d-77e7-4f2c-842d-70b5e10df9a8               |\n| Local ID                 |                                                    |\n| MTU                      | 1500                                               |\n| Name                     | vpn-conn-to-fra1                                   |\n| Peer Address             | 198.51.100.50                                      |\n| Peer CIDRs               |                                                    |\n| Peer Endpoint Group ID   | 2e627315-02f0-4d68-8683-14230b166060               |\n| Peer ID                  | 198.51.100.50                                      |\n| Pre-shared Key           | de12db260ee1b9c0b9e624d910c9a9dbddec13dc24d60332   |\n| Project                  | 94109c764a754e24ac0f6b01aef82359                   |\n| Route Mode               | static                                             |\n| State                    | True                                               |\n| Status                   | PENDING_CREATE                                     |\n| VPN Service              | bb1a307d-6f8f-4a0a-83db-7c705403485d               |\n| dpd                      | {'action': 'hold', 'interval': 30, 'timeout': 120} |\n| project_id               | 94109c764a754e24ac0f6b01aef82359                   |\n+--------------------------+----------------------------------------------------+\n</code></pre>"},{"location":"howto/openstack/neutron/vpnaas/#viewing-vpn-connections-and-getting-details","title":"Viewing VPN connections and getting details","text":"<p>No matter if you use the Cleura\u00a0Cloud Management\u00a0Panel or the OpenStack CLI, you may at any time list all VPN connections and get relevant details.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>In the vertical pane on the left-hand side of the Cleura\u00a0Cloud Management\u00a0Panel, expand the Networking section and then the VPN Services subsection. From the available options, click VPN Services again. You will see two VPN connections in the main pane, each from one region to the other.</p> <p></p> <p>For more information regarding a specific connection, click the corresponding three-dot icon (right-hand side) and select View details. Then, you can glance over all the details regarding, for example, the connection status and public IP address.</p> <p></p> <p>You can list all IPSec VPN connections working from any of the two regions involved. See, for example, the view from <code>fra1</code>:</p> <pre><code>openstack vpn ipsec site connection list\n</code></pre> <pre><code>+--------------------------+------------------+---------------+--------------------------+--------+\n| ID                       | Name             | Peer Address  | Authentication Algorithm | Status |\n+--------------------------+------------------+---------------+--------------------------+--------+\n| 5f44be31-0588-4f33-883d- | vpn-conn-to-kna1 | 203.0.113.101 | psk                      | ACTIVE |\n| 9e2d97be55e1             |                  |               |                          |        |\n+--------------------------+------------------+---------------+--------------------------+--------+\n</code></pre> <p>If you want more information regarding a specific connection, type something like this:</p> <pre><code>openstack vpn ipsec site connection show vpn-conn-to-kna1\n</code></pre> <pre><code>+--------------------------+----------------------------------------------------+\n| Field                    | Value                                              |\n+--------------------------+----------------------------------------------------+\n| Authentication Algorithm | psk                                                |\n| Description              |                                                    |\n| ID                       | 5f44be31-0588-4f33-883d-9e2d97be55e1               |\n| IKE Policy               | 5782b141-afcc-4327-9ac5-b8cd2e110c6a               |\n| IPSec Policy             | 15ec761f-1642-49b6-b5a2-e43624c5752d               |\n| Initiator                | bi-directional                                     |\n| Local Endpoint Group ID  | 51895e0e-fa3a-43d3-8037-5eea073fb77f               |\n| Local ID                 |                                                    |\n| MTU                      | 1500                                               |\n| Name                     | vpn-conn-to-kna1                                   |\n| Peer Address             | 203.0.113.101                                      |\n| Peer CIDRs               |                                                    |\n| Peer Endpoint Group ID   | a96bb9ef-d0ec-4174-93ae-a8e655910f94               |\n| Peer ID                  | 203.0.113.101                                      |\n| Pre-shared Key           | de12db260ee1b9c0b9e624d910c9a9dbddec13dc24d60332   |\n| Project                  | dfc700467396428bacba4376e72cc3e9                   |\n| Route Mode               | static                                             |\n| State                    | True                                               |\n| Status                   | ACTIVE                                             |\n| VPN Service              | d74d51f0-182d-4d88-952a-1d593ce696fd               |\n| dpd                      | {'action': 'hold', 'interval': 30, 'timeout': 120} |\n| project_id               | dfc700467396428bacba4376e72cc3e9                   |\n+--------------------------+----------------------------------------------------+\n</code></pre>"},{"location":"howto/openstack/neutron/vpnaas/#testing-the-site-to-site-vpn-connection","title":"Testing the site-to-site VPN connection","text":"<p>One way to test the VPN connection is to have two servers (e.g., <code>server-fra1</code> and <code>server-kna1</code>), each on a different region (e.g., <code>fra1</code> and <code>kna1</code> respectively), ping each other using private IP addresses. With no VPN connection between the two regions, pinging should not be possible:</p> <pre><code>ubuntu@server-fra1:~$ ping -c 3 10.15.20.148\nPING 10.15.20.148 (10.15.20.148) 56(84) bytes of data.\n\n--- 10.15.20.148 ping statistics ---\n3 packets transmitted, 0 received, 100% packet loss, time 2050ms\n</code></pre> <pre><code>ubuntu@server-kna1:~$ ping -c 3 10.15.25.58\nPING 10.15.25.58 (10.15.25.58) 56(84) bytes of data.\n\n--- 10.15.25.58 ping statistics ---\n3 packets transmitted, 0 received, 100% packet loss, time 2045ms\n</code></pre> <p>On the other hand, with a VPN connection established between the two regions, pinging should be all possible:</p> <pre><code>ubuntu@server-fra1:~$ ping -c 3 10.15.20.148\nPING 10.15.20.148 (10.15.20.148) 56(84) bytes of data.\n64 bytes from 10.15.20.148: icmp_seq=1 ttl=62 time=32.8 ms\n64 bytes from 10.15.20.148: icmp_seq=2 ttl=62 time=32.5 ms\n64 bytes from 10.15.20.148: icmp_seq=3 ttl=62 time=32.6 ms\n\n--- 10.15.20.148 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2003ms\nrtt min/avg/max/mdev = 32.544/32.624/32.761/0.096 ms\n</code></pre> <pre><code>ubuntu@server-kna1:~$ ping -c 3 10.15.25.58\nPING 10.15.25.58 (10.15.25.58) 56(84) bytes of data.\n64 bytes from 10.15.25.58: icmp_seq=1 ttl=62 time=33.3 ms\n64 bytes from 10.15.25.58: icmp_seq=2 ttl=62 time=32.5 ms\n64 bytes from 10.15.25.58: icmp_seq=3 ttl=62 time=32.6 ms\n\n--- 10.15.25.58 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2003ms\nrtt min/avg/max/mdev = 32.533/32.832/33.336/0.358 ms\n</code></pre>"},{"location":"howto/openstack/neutron/vpnaas/#disabling-an-active-connection","title":"Disabling an active connection","text":"<p>You may, at any time, disable an active site-to-site VPN connection.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Currently, there is no way to disable an active connection from the Cleura\u00a0Cloud Management\u00a0Panel. If you want to disable an active connection, please use the OpenStack CLI.</p> <p>All you have to do is get on either side of the connection and disable the VPN connection across the other side. Suppose you are on the left side of the connection (region <code>fra1</code>), and for whatever reason, you want to disable the site-to-site connection between left and right (regions <code>fra1</code> and <code>kna1</code>). First, you might want to remember the name of the VPN connection to the right:</p> <pre><code>openstack vpn ipsec site connection list\n</code></pre> <pre><code>+--------------------------+------------------+---------------+--------------------------+--------+\n| ID                       | Name             | Peer Address  | Authentication Algorithm | Status |\n+--------------------------+------------------+---------------+--------------------------+--------+\n| 5f44be31-0588-4f33-883d- | vpn-conn-to-kna1 | 203.0.113.101 | psk                      | ACTIVE |\n| 9e2d97be55e1             |                  |               |                          |        |\n+--------------------------+------------------+---------------+--------------------------+--------+\n</code></pre> <p>That would be <code>vpn-conn-to-kna1</code>, and according to the command output above, it is active. To disable it, type the following:</p> <pre><code>openstack vpn ipsec site connection set --disable vpn-conn-to-kna1\n</code></pre> <p>Check if it is really disabled:</p> <pre><code>openstack vpn ipsec site connection show vpn-conn-to-kna1 -c Status\n</code></pre> <pre><code>+--------+-------+\n| Field  | Value |\n+--------+-------+\n| Status | DOWN  |\n+--------+-------+\n</code></pre> <p>It looks like it is disabled \u2014 or <code>DOWN</code>.</p> <p>You will probably have to wait several seconds before seeing a status change. If you are impatient, try something like this: <pre><code>watch \"openstack vpn ipsec site connection show vpn-conn-to-kna1 -c Status\"\n</code></pre> Hit CTRL+C as soon as you see the status change you expect.</p> <p>Now, get on the right side of the connection (region <code>kna1</code>), optionally look for the name of the VPN connection to the left (in our example, that would be <code>vpn-conn-to-fra1</code>), and check its status:</p> <pre><code>openstack vpn ipsec site connection show vpn-conn-to-fra1 -c Status\n</code></pre> <pre><code>+--------+-------+\n| Field  | Value |\n+--------+-------+\n| Status | DOWN  |\n+--------+-------+\n</code></pre> <p>It turns out that this direction of the connection is also disabled.</p> <p>To test that a previously enabled site-to-site connection is now disabled, select a server from one region and try to ping a server in another region. Suppose, for example, that a now-disabled connection involved regions <code>fra1</code> and <code>kna1</code>, and that we have servers <code>server-fra1</code> (in <code>fra1</code>) and <code>server-kna1</code> (in <code>kna1</code>). With the VPN connection between the two regions now disabled, pinging should not be possible:</p> <pre><code>ubuntu@server-fra1:~$ ping -c 3 10.15.20.148\nPING 10.15.20.148 (10.15.20.148) 56(84) bytes of data.\n\n--- 10.15.20.148 ping statistics ---\n3 packets transmitted, 0 received, 100% packet loss, time 2055ms\n</code></pre> <pre><code>ubuntu@server-kna1:~$ ping -c 3 10.15.25.58\nPING 10.15.25.58 (10.15.25.58) 56(84) bytes of data.\n\n--- 10.15.25.58 ping statistics ---\n3 packets transmitted, 0 received, 100% packet loss, time 2050ms\n</code></pre>"},{"location":"howto/openstack/neutron/vpnaas/#enabling-an-inactive-connection","title":"Enabling an inactive connection","text":"<p>You can easily enable an inactive site-to-site VPN connection.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Currently, there is no way to enable an inactive connection from the Cleura\u00a0Cloud Management\u00a0Panel. If you want to re-enable an inactive connection, please use the OpenStack CLI.</p> <p>Make sure you hop on the side where you initially disabled the connection. According to the example scenario we described in the previous section, that would be the left side (region <code>fra1</code>), and the name of the disabled connection would be <code>vpn-conn-to-kna1</code>. Make sure the connection status is <code>DOWN</code>:</p> <pre><code>openstack vpn ipsec site connection show vpn-conn-to-kna1 -c Status\n</code></pre> <pre><code>+--------+-------+\n| Field  | Value |\n+--------+-------+\n| Status | DOWN  |\n+--------+-------+\n</code></pre> <p>Note that if you issued a similar command from the right side of the connection (region <code>kna1</code>), you would also get a <code>DOWN</code> status. Being on the left side, all you have to do to enable the inactive connection is type the following:</p> <pre><code>openstack vpn ipsec site connection set --enable vpn-conn-to-kna1\n</code></pre> <p>Check the connection status \u2014 it should be <code>ACTIVE</code>:</p> <pre><code>openstack vpn ipsec site connection show vpn-conn-to-kna1 -c Status\n</code></pre> <pre><code>+--------+--------+\n| Field  | Value  |\n+--------+--------+\n| Status | ACTIVE |\n+--------+--------+\n</code></pre> <p>You get the same status by issuing a similar command from the right side.</p> <p>Again, since you may have to wait several seconds before seeing the status change you expect, try something like this: <pre><code>watch \"openstack vpn ipsec site connection show vpn-conn-to-kna1 -c Status\"\n</code></pre> Hit CTRL+C to stop watching.</p> <p>To test that a previously disabled site-to-site connection is now enabled, select a server from one region and try to ping a server in another region. Suppose, for example, that a re-enabled connection involves regions <code>fra1</code> and <code>kna1</code>, and that we have servers <code>server-fra1</code> (in <code>fra1</code>) and <code>server-kna1</code> (in <code>kna1</code>). With the VPN connection between the two regions now re-established, pinging should be possible:</p> <pre><code>ubuntu@server-fra1:~$ ping -c 3 10.15.20.148\nPING 10.15.20.148 (10.15.20.148) 56(84) bytes of data.\n64 bytes from 10.15.20.148: icmp_seq=1 ttl=62 time=32.6 ms\n64 bytes from 10.15.20.148: icmp_seq=2 ttl=62 time=32.6 ms\n64 bytes from 10.15.20.148: icmp_seq=3 ttl=62 time=32.7 ms\n\n--- 10.15.20.148 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2003ms\nrtt min/avg/max/mdev = 32.619/32.634/32.662/0.019 ms\n</code></pre> <pre><code>ubuntu@server-kna1:~$ ping -c 3 10.15.25.58\nPING 10.15.25.58 (10.15.25.58) 56(84) bytes of data.\n64 bytes from 10.15.25.58: icmp_seq=1 ttl=62 time=32.8 ms\n64 bytes from 10.15.25.58: icmp_seq=2 ttl=62 time=32.6 ms\n64 bytes from 10.15.25.58: icmp_seq=3 ttl=62 time=33.4 ms\n\n--- 10.15.25.58 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2003ms\nrtt min/avg/max/mdev = 32.560/32.899/33.357/0.336 ms\n</code></pre>"},{"location":"howto/openstack/nova/boot-image-volume/","title":"Converting a boot-from-image server to boot-from-volume","text":"<p>You can freely move any server between Cleura\u00a0Cloud regions, provided it boots from a volume (which we generally recommend, since it affords more flexibility than booting from an image). You may still have boot-from-image servers, though. To verify that a particular server is of that type, you can go to the Cleura\u00a0Cloud Management\u00a0Panel, locate the server, and expand its detailed view. Go to the Details tab and pay attention to the Boot Target field. If this is a boot-from-image server, it says Ephemeral Disk.</p> <p></p> <p>Whenever you decide to move a boot-from-image server between regions, you will discover that you cannot do so. The solution is first to convert it to a boot-from-volume server and then move it. In the following, we show how to perform the conversion using the Cleura\u00a0Cloud Management\u00a0Panel or the OpenStack CLI.</p>"},{"location":"howto/openstack/nova/boot-image-volume/#preparation","title":"Preparation","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Fire up your favorite web browser and navigate to the Cleura\u00a0Cloud Management\u00a0Panel start page. Log into your Cleura\u00a0Cloud account if you have to.</p> <p>To work with the OpenStack CLI, make sure to properly enable it for the region your boot-from-image server resides in.</p>"},{"location":"howto/openstack/nova/boot-image-volume/#shutting-down-the-server","title":"Shutting down the server","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Make sure the left-hand side vertical pane is in full view, then select Compute \u2192 Servers. You will see all supported Cleura\u00a0Cloud regions in the main pane. Expand the one with the boot-from-image server you want to convert to boot-from-volume. Click the orange three-dots-in-a-circle icon at the right of the server row, and from the pop-up menu that appears, select Stop Server.</p> <p></p> <p>A pop-up window appears, asking if you want to stop the server. Confirm by clicking on the red button labeled Yes, Stop.</p> <p> </p> <p>Assuming your boot-from-image server is named <code>nanisivik</code>, using your favorite terminal application, shut it down by typing the following:</p> <pre><code>openstack server stop nanisivik\n</code></pre> <p>To confirm that it has finished shutting down, type:</p> <pre><code>openstack server show nanisivik -c status -f value\n</code></pre> <p>You should see this:</p> <pre><code>SHUTOFF\n</code></pre>"},{"location":"howto/openstack/nova/boot-image-volume/#taking-a-snapshot","title":"Taking a snapshot","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>When the server is shut off, click on its row to get the detailed view and go to the Snapshots tab. There, click the green button labeled Create a Snapshot.</p> <p></p> <p>A pop-up window appears, where you have to type in a name for the snapshot. You must also be explicit regarding the operation you are about to perform, by activating the switch at the left of the corresponding question. When you are ready, click the green button labeled Create.</p> <p></p> <p>To take a snapshot of the server, all you have to do is type something like the following:</p> <pre><code>openstack server image create --name nanisivik_snap nanisivik\n</code></pre> <pre><code>+------------+-------------------------------------------------------------------------------------+\n| Field      | Value                                                                               |\n+------------+-------------------------------------------------------------------------------------+\n| created_at | 2023-05-10T13:31:46Z                                                                |\n| file       | /v2/images/1a50720b-af1c-4a16-ba19-b89dbee251f0/file                                |\n| id         | 1a50720b-af1c-4a16-ba19-b89dbee251f0                                                |\n| min_disk   | 50                                                                                  |\n| min_ram    | 0                                                                                   |\n| name       | nanisivik_snap                                                                      |\n| owner      | 94109c764a754e24ac0f6b01aef82359                                                    |\n| properties | base_image_ref='d6b012ee-c6d3-4672-9399-b87d025ddb14', boot_roles='load-            |\n|            | balancer_member,_member_,swiftoperator,creator', hw_machine_type='pc',              |\n|            | hw_qemu_guest_agent='yes', image_type='snapshot',                                   |\n|            | instance_uuid='19ffeb4d-3016-481f-886e-6caecc525fc2', locations='[]',               |\n|            | os_hidden='False', owner_project_name='Meanwhile-in-Drama',                         |\n|            | owner_user_name='kolderson', user_id='c096cf99f65a4d22a6954b67d2ec11d7'             |\n| protected  | False                                                                               |\n| schema     | /v2/schemas/image                                                                   |\n| status     | queued                                                                              |\n| tags       |                                                                                     |\n| updated_at | 2023-05-10T13:31:46Z                                                                |\n| visibility | private                                                                             |\n+------------+-------------------------------------------------------------------------------------+\n</code></pre> <p>At first, the status of the snapshot is <code>queued</code>. To make sure the snapshot has been successfully created, type:</p> <pre><code>openstack image show nanisivik_snap -c status -f value\n</code></pre> <p>You should see this:</p> <pre><code>active\n</code></pre>"},{"location":"howto/openstack/nova/boot-image-volume/#creating-a-volume-from-the-snapshot","title":"Creating a volume from the snapshot","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>After a few seconds, the snapshot will be created and listed in the Snapshots tab. Before you proceed, take note of its size. At the right-hand side of the snapshot row, click the  icon to create a new volume off of the snapshot you just created.</p> <p></p> <p>A new vertical pane will slide over from the right-hand side of the Cleura\u00a0Cloud Management\u00a0Panel. Type in a name for the new volume, and choose a volume size bigger than the snapshot size. Then, you need to type in a description regarding the new volume.</p> <p></p> <p>Scroll down a bit if you have to, and click the green Create button.</p> <p></p> <p>Before creating a volume off of the snapshot you just created, jot down its size like this:</p> <pre><code>openstack image show nanisivik_snap -c min_disk -f value\n</code></pre> <p>In our example, the size of the snapshot is <code>50</code> gibibytes. Now, go ahead and create a volume slightly larger than the size of the snapshot:</p> <pre><code>openstack volume create --size 56 --image nanisivik_snap nanisivik_vol\n</code></pre> <pre><code>+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| attachments         | []                                   |\n| availability_zone   | nova                                 |\n| bootable            | false                                |\n| consistencygroup_id | None                                 |\n| created_at          | 2023-05-10T13:39:04.379098           |\n| description         | None                                 |\n| encrypted           | False                                |\n| id                  | e402ecda-5222-4082-93bb-c0cafce00d8d |\n| multiattach         | False                                |\n| name                | nanisivik_vol                        |\n| properties          |                                      |\n| replication_status  | None                                 |\n| size                | 56                                   |\n| snapshot_id         | None                                 |\n| source_volid        | None                                 |\n| status              | creating                             |\n| type                | ceph_hdd                             |\n| updated_at          | None                                 |\n| user_id             | c096cf99f65a4d22a6954b67d2ec11d7     |\n+---------------------+--------------------------------------+\n</code></pre> <p>As you can see in the example above, we named our volume <code>nanisivik_vol</code>, and its <code>status</code> was at first <code>creating</code>. You may check the progress of this operation by typing this:</p> <pre><code>openstack volume show nanisivik_vol -c status -f value\n</code></pre> <p>As soon as the new volume is ready, the <code>status</code> becomes <code>available</code>.</p>"},{"location":"howto/openstack/nova/boot-image-volume/#viewing-the-new-volume","title":"Viewing the new volume","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>In the left-hand side vertical pane, select Storage \u2192 Volumes. In the main pane, select the region your boot-from-image server resides in. You will see the volume you created in the previous step. In the intersection of the volume row and the column labeled Attached To, it will say -No attachments-. That is expected.</p> <p></p> <p>To view all details regarding the volume you just created off of the boot-from-image server snapshot, type the following:</p> <pre><code>openstack volume show nanisivik_vol\n</code></pre> <pre><code>+------------------------------+-------------------------------------------------------------------+\n| Field                        | Value                                                             |\n+------------------------------+-------------------------------------------------------------------+\n| attachments                  | []                                                                |\n| availability_zone            | nova                                                              |\n| bootable                     | true                                                              |\n| consistencygroup_id          | None                                                              |\n| created_at                   | 2023-05-10T13:39:04.000000                                        |\n| description                  | None                                                              |\n| encrypted                    | False                                                             |\n| id                           | e402ecda-5222-4082-93bb-c0cafce00d8d                              |\n| multiattach                  | False                                                             |\n| name                         | nanisivik_vol                                                     |\n| os-vol-tenant-attr:tenant_id | 94109c764a754e24ac0f6b01aef82359                                  |\n| properties                   |                                                                   |\n| replication_status           | None                                                              |\n| size                         | 56                                                                |\n| snapshot_id                  | None                                                              |\n| source_volid                 | None                                                              |\n| status                       | available                                                         |\n| type                         | ceph_hdd                                                          |\n| updated_at                   | 2023-05-10T13:39:05.000000                                        |\n| user_id                      | c096cf99f65a4d22a6954b67d2ec11d7                                  |\n| volume_image_metadata        | {'hw_qemu_guest_agent': 'yes', 'base_image_ref':                  |\n|                              | 'd6b012ee-c6d3-4672-9399-b87d025ddb14', 'owner_user_name':        |\n|                              | 'kolderson', 'owner_project_name': 'Meanwhile-in-Drama',          |\n|                              | 'boot_roles': 'load-                                              |\n|                              | balancer_member,_member_,swiftoperator,creator',                  |\n|                              | 'hw_machine_type': 'pc', 'instance_uuid':                         |\n|                              | '19ffeb4d-3016-481f-886e-6caecc525fc2', 'user_id':                |\n|                              | 'c096cf99f65a4d22a6954b67d2ec11d7', 'image_type': 'snapshot',     |\n|                              | 'owner_id': '94109c764a754e24ac0f6b01aef82359', 'image_state':    |\n|                              | 'available', 'image_location': 'snapshot', 'image_id':            |\n|                              | '1a50720b-af1c-4a16-ba19-b89dbee251f0', 'image_name':             |\n|                              | 'nanisivik_snap', 'container_format': 'bare', 'disk_format':      |\n|                              | 'raw', 'min_disk': '50', 'min_ram': '0', 'size': '53687091200'}   |\n+------------------------------+-------------------------------------------------------------------+\n</code></pre>"},{"location":"howto/openstack/nova/boot-image-volume/#deleting-the-boot-from-image-server","title":"Deleting the boot-from-image server","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Go to the servers view by selecting Compute \u2192 Servers, and locate the boot-from-image server. It is time to delete it, so click the orange three-dots-in-a-circle icon at the right of the server row, and from the pop-up menu that appears, select Delete Server.</p> <p></p> <p>A window titled About to delete a server will appear, asking you if you want to proceed with the deletion. Click the red button labeled Yes, Delete.</p> <p></p> <p>Since you have a snapshot and a volume of your server, you can now delete it:</p> <pre><code>openstack server delete nanisivik\n</code></pre> <p>If the operation succeeds, you should see no output on your terminal.</p>"},{"location":"howto/openstack/nova/boot-image-volume/#creating-a-new-boot-from-volume-server","title":"Creating a new boot-from-volume server","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Now, go back to your volumes by selecting Storage \u2192 Volumes. Tick the volume you created a bit earlier, click the orange three-dots-in-a-circle icon at the right of the volume row, and from the pop-up menu that appears, select Create Server. A vertical pane will slide over from the right-hand side of the Cleura\u00a0Cloud Management\u00a0Panel, titled Create a Server.</p> <p></p> <p>Type in a name for the new server. That could as well be the name your boot-from-image server had. Notice that the server region is pre-selected and the same as the one in which the volume resides. The boot source of the server is also pre-selected and is the volume itself.</p> <p></p> <p>Scroll down a bit if you have to. Take notice of the Boot Target, which should be Volume (Recommended). Select a flavor for the new boot-from-volume server, similar to or even the same as the flavor the boot-from-image server had.</p> <p></p> <p>Consider leaving the Disaster recovery option enabled, and see if you want an external IP address for the server.</p> <p></p> <p>Select the default security group and choose a keypair for the server.</p> <p></p> <p>To create it, click the green button labeled Create.</p> <p></p> <p>To create your new boot-from-volume server, which will be using the volume you got starting from the snapshot of the old boot-from-image server, type something like the following:</p> <pre><code>openstack server create \\\n    --flavor b.2c4gb \\\n    --volume nanisivik_vol \\\n    --network network-kna1 \\\n    --security-group default \\\n    --key-name karlskrona \\\n    --wait \\\n    nanisivik\n</code></pre> <p>The most important parameter in the command above is <code>--volume</code>, which is used to specify the boot volume of the server. Regarding server creation in general, you might want to check the corresponding guide.</p>"},{"location":"howto/openstack/nova/boot-image-volume/#viewing-the-new-server","title":"Viewing the new server","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>In the left-hand side vertical pane, select Compute \u2192 Servers. In the main pane of the Cleura\u00a0Cloud Management\u00a0Panel, go to the region the new server resides, select it and click on its row to see all its characteristics. In the Details tab, you will notice, among other things, that it has one attached volume; that would be the one you created from the snapshot you created from the old boot-from-image server.</p> <p></p> <p>You may see all details regarding the new boot-from-volume server, by typing something like the following:</p> <pre><code>openstack server show nanisivik\n</code></pre> <pre><code>+-----------------------------+----------------------------------------------------------+\n| Field                       | Value                                                    |\n+-----------------------------+----------------------------------------------------------+\n| OS-DCF:diskConfig           | MANUAL                                                   |\n| OS-EXT-AZ:availability_zone | nova                                                     |\n| OS-EXT-STS:power_state      | Running                                                  |\n| OS-EXT-STS:task_state       | None                                                     |\n| OS-EXT-STS:vm_state         | active                                                   |\n| OS-SRV-USG:launched_at      | 2023-05-10T15:06:28.000000                               |\n| OS-SRV-USG:terminated_at    | None                                                     |\n| accessIPv4                  |                                                          |\n| accessIPv6                  |                                                          |\n| addresses                   | network-kna1=10.15.20.165                                |\n| config_drive                |                                                          |\n| created                     | 2023-05-10T15:06:20Z                                     |\n| flavor                      | b.2c4gb (2d49822b-a1d8-4f9c-a12c-cba8150611d1)           |\n| hostId                      | 975f93445c8d4e1d7df89729f7712b704fafca8c35c8eeb86150a68a |\n| id                          | 8249b090-d5ea-403c-a636-5e27925c83bb                     |\n| image                       | N/A (booted from volume)                                 |\n| key_name                    | karlskrona                                               |\n| name                        | nanisivik                                                |\n| progress                    | 0                                                        |\n| project_id                  | 94109c764a754e24ac0f6b01aef82359                         |\n| properties                  |                                                          |\n| security_groups             | name='default'                                           |\n| status                      | ACTIVE                                                   |\n| updated                     | 2023-05-10T15:06:28Z                                     |\n| user_id                     | c096cf99f65a4d22a6954b67d2ec11d7                         |\n| volumes_attached            | id='e402ecda-5222-4082-93bb-c0cafce00d8d'                |\n+-----------------------------+----------------------------------------------------------+\n</code></pre> <p>Pay attention to the value of the <code>image</code> field, which confirms that this is indeed a boot-from-volume server.</p>"},{"location":"howto/openstack/nova/config-drive/","title":"Launching a server with a configuration drive","text":""},{"location":"howto/openstack/nova/config-drive/#background-openstack-metadata-discovery","title":"Background: OpenStack metadata discovery","text":"<p>OpenStack Compute uses metadata to inject custom configurations to servers on boot. You can add custom scripts, install packages, and add SSH keys to the servers using metadata.</p> <p>By default, metadata discovery in Cleura\u00a0Cloud uses an HTTP data source that booting servers connect to. Sometimes, this is undesirable or \u2014 for specific server/networking configurations \u2014 unreliable. Under those circumstances, you can use an alternate configuration source.</p>"},{"location":"howto/openstack/nova/config-drive/#store-metadata-on-a-configuration-drive","title":"Store metadata on a configuration drive","text":"<p>A configuration drive (config drive) is a read-only virtual drive that gets attached to a server during boot. The server can then mount the drive and read files from it. Configuration drives are used as a data source for cloud-init.</p>"},{"location":"howto/openstack/nova/config-drive/#enable-the-configuration-drive-on-server-creation","title":"Enable the configuration drive on server creation","text":"<p>Follow our How-To guide to create your new cloud server using either the Cleura\u00a0Cloud Management\u00a0Panel or the OpenStack CLI.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>When you are done configuring the server, right before creating it notice the icon on the left-hand side of Advanced Options. Click on it to expand all related options.</p> <p></p> <p>In the User-Data area, you see everything that will be executed upon server boot.</p> <p>Right below, in the User-data propagation method area, the Use metadata service (Default) option is preselected for you. To opt for the alternative metadata discovery method, select Use configuration drive.</p> <p></p> <p>Then, instantiate the server by clicking the green Create button.</p> <p>Once the server is ready, you may see what happened during configuration by examining the console log. In the Cleura\u00a0Cloud Management\u00a0Panel, click on the server line to get an extended view of its characteristics, and then click once more on the Console Output tab.</p> <p></p> <p>There might be quite a lot of text to sift through, but you can easily do so by pressing the <code>Up</code> or <code>Down</code> arrow keys.</p> <p>To enable the configuration drive, you need to pass the parameter <code>--use-config-drive</code> to the <code>openstack server create</code> command.</p> <p>In the following example, replace the image, flavor, keypair, and network reference, as well as the server name, to match your desired configuration.</p> <pre><code>openstack server create \\\n  --use-config-drive \\\n  --image \"Ubuntu 22.04 Jammy Jellyfish x86_64\" \\\n  --flavor b.1c2gb \\\n  --keypair mykey\n  --nic net-id=3a747038-ee59-404c-973d-5f795e8ebb73 \\\n  myserver\n</code></pre> <p>Once the server launches, you can monitor its configuration process by watching the server console log:</p> <pre><code>openstack console log show myserver\n</code></pre>"},{"location":"howto/openstack/nova/move-server-between-regions/","title":"Moving a server from one region to another","text":"<p>This guide will show you how to move a server to a different region in Cleura\u00a0Cloud.</p>"},{"location":"howto/openstack/nova/move-server-between-regions/#prerequisites","title":"Prerequisites","text":"<p>In order to move a server from one region to another, you will need</p> <ul> <li>a correctly installed and configured OpenStack CLI   client,</li> <li>access to the RC files with credentials for both the source and   target region,</li> <li>enough space on the local machine to be able to download an image of   your server,</li> <li>a sufficiently configured target region including a virtual   network and necessary security   groups.</li> </ul>"},{"location":"howto/openstack/nova/move-server-between-regions/#finding-a-volumes-id","title":"Finding a volume\u2019s ID","text":"<p>To work with the OpenStack CLI, please do not forget to source the RC file first.</p> <p>Use the ID of the server instead of using the server name. This will make sure that you are using the correct server.</p> <p>Find the ID of your server by matching the name, using the following command:</p> <pre><code>$ openstack server list --name 'name'\n+-----------------+-----------------+---------+-----------------+--------------------+-------------+\n| ID              | Name            | Status  | Networks        | Image              | Flavor      |\n+-----------------+-----------------+---------+-----------------+--------------------+-------------+\n| 4e88215e-2df9-4 | Name2           | SHUTOFF | STO2_test_netwo | N/A (booted from v | b.1c1gb     |\n| 9ca-8d39-31c01d |                 |         | rk=10.x.y.z     | olume)             |             |\n| 5b7eae          |                 |         |                 |                    |             |\n| fad16a17-a4f1-4 | Name1           | ACTIVE  | STO2_test_netwo | N/A (booted from v | b.1c1gb     |\n| 74c-9b90-de14c8 |                 |         | rk=10.x.y.z     | olume)             |             |\n| ab8100          |                 |         |                 |                    |             |\n+-----------------+-----------------+---------+-----------------+--------------------+-------------+\n</code></pre> <p>This guide is only applicable to servers that are using boot from volume. To verify this, make sure your server\u2019s <code>Image</code> value is <code>N/A (booted from volume)</code>.</p> <p>To get the ID of your server\u2019s boot volume, use the following command:</p> <pre><code>$ openstack server show -c volumes_attached &lt;server_id&gt;\n+------------------+-------------------------------------------+\n| Field            | Value                                     |\n+------------------+-------------------------------------------+\n| volumes_attached | id='14942b23-25ef-4181-9c54-e5c100e4eeb8' |\n|                  | id='34e2ca96-3c34-464f-ac66-9fef7a6810b8' |\n|                  | id='68756558-dbf0-44db-9475-1bc24822371c' |\n+------------------+-------------------------------------------+\n</code></pre> <p>If there are multiple volumes attached, the first volume in the list is the server system volume. Copy this ID.</p> <p>If you want to move any other attached volumes along with your server\u2019s system volume, you also need to follow the same steps for each one of these volumes.</p>"},{"location":"howto/openstack/nova/move-server-between-regions/#stopping-a-running-server","title":"Stopping a running server","text":"<p>In the next step you are instructed to make a copy of the server\u2019s system volume. Some operating systems or applications might experience issues being copied at the same time it might be performing operations.</p> <p>While this step is not strictly required, it is recommended to first power off your server.</p> <p>Stop the running server with the following command:</p> <pre><code>openstack server stop &lt;server_id&gt;\n</code></pre>"},{"location":"howto/openstack/nova/move-server-between-regions/#creating-a-copy-of-a-volume","title":"Creating a copy of a volume","text":"<p>Begin by making a copy of the volume, using the following command:</p> <pre><code>openstack volume create --source &lt;source_volume_id&gt; &lt;copy_volume_name&gt;\n</code></pre> <p>You will get a printout showing you information about the created volume, such as <code>source_volid</code> which is the ID of volume you just copied, and <code>id</code> of this new volume, that you will use in the next step to create an image.</p> <pre><code>+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| attachments         | []                                   |\n| availability_zone   | nova                                 |\n| bootable            | true                                 |\n| consistencygroup_id | None                                 |\n| created_at          | 2022-11-10T15:12:11.821647           |\n| description         | None                                 |\n| encrypted           | False                                |\n| id                  | 2fde1e6d-2d72-4d70-b12d-1a2d314dba72 |\n| multiattach         | False                                |\n| name                | &lt;copy_volume_name&gt;                   |\n| properties          |                                      |\n| replication_status  | None                                 |\n| size                | 20                                   |\n| snapshot_id         | None                                 |\n| source_volid        | 14942b23-25ef-4181-9c54-e5c100e4eeb8 |\n| status              | creating                             |\n| type                | default                              |\n| updated_at          | None                                 |\n| user_id             | 9e19-9424-42c4-b70f-a371ec0db5d3     |\n+---------------------+--------------------------------------+\n</code></pre>"},{"location":"howto/openstack/nova/move-server-between-regions/#creating-an-image-of-a-volume","title":"Creating an image of a volume","text":"<p>Then create an image of the copied volume, by using the following command:</p> <pre><code>openstack image create --volume &lt;copy_volume_id&gt; &lt;new_image_name&gt;\n</code></pre> <p>Substitute <code>&lt;copy_volume_id&gt;</code> with the ID from the newly created volume in the previous step.</p> <p>After a while you will get a printout showing you information of the new image, such as the image disk format <code>disk_format</code> and the image ID <code>image_id</code>, you need these two values in an upcoming step.</p> <pre><code>+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| container_format    | bare                                 |\n| disk_format         | raw                                  |\n| display_description | None                                 |\n| id                  | 2fde1e6d-2d72-4d70-b12d-1a2d314dba72 |\n| image_id            | cfa8620a-52d7-4d84-91ef-b25bb24e6e33 |\n| image_name          | &lt;new_image_name&gt;                     |\n| protected           | False                                |\n| size                | 20                                   |\n| status              | uploading                            |\n| updated_at          | 2022-11-10T15:22:12.000000           |\n| visibility          | shared                               |\n| volume_type         | default                              |\n+---------------------+--------------------------------------+\n</code></pre> <p>Depending on the size of the volume it might take some time to upload and while it is, the image <code>status</code> will be <code>uploading</code>. Before you continue to the next step, make sure the image <code>status</code> is <code>active</code>, otherwise wait a bit and then check again with:</p> <pre><code>openstack image show -c status &lt;image_id&gt;\n</code></pre> <p>The printout should look like this before you continue.</p> <pre><code>+--------+--------+\n| Field  | Value  |\n+--------+--------+\n| status | active |\n+--------+--------+\n</code></pre> <p>It is now safe to remove the volume you created earlier, using the command: <code>openstack volume delete &lt;copy_volume_id&gt;</code></p>"},{"location":"howto/openstack/nova/move-server-between-regions/#downloading-an-image","title":"Downloading an image","text":"<p>Download the image to your local computer, using the following command:</p> <pre><code>openstack image save --file &lt;local_image_name&gt;.&lt;disk_format&gt; &lt;image_id&gt;\n</code></pre> <p>Substitute <code>&lt;disk_format&gt;</code> with the value from the printout in the previous step.</p> <p>Substitute <code>&lt;image_id&gt;</code> with the ID from the printout in the previous step.</p> <p>When you have downloaded the file, verify that the checksum of the file is the same as the <code>checksum</code> value of the image.</p> <pre><code>md5 &lt;local_image_name&gt;.&lt;disk_format&gt;\n</code></pre> <p>This will output the checksum of your local file.</p> <pre><code>MD5 (&lt;local_image_name&gt;.&lt;disk_format&gt;) = 4b086035a943cc1676583c0cc78f0896\n</code></pre> <p>Show the checksum of the image in the cloud, using the following command:</p> <pre><code>openstack image show -c checksum &lt;image_id&gt;\n</code></pre> <p>These two checksums should be the same.</p> <pre><code>+----------+----------------------------------+\n| Field    | Value                            |\n+----------+----------------------------------+\n| checksum | 4b086035a943cc1676583c0cc78f0896 |\n+----------+----------------------------------+\n</code></pre> <p>You are now done with the steps for the source region. The following steps will be done on the target region.</p> <p>It is now safe to remove the image you created earlier, using the command: <pre><code>openstack image delete &lt;image_id&gt;\n</code></pre></p>"},{"location":"howto/openstack/nova/move-server-between-regions/#uploading-an-image","title":"Uploading an image","text":"<p>Source the RC-file for the region you want to upload to.</p> <pre><code>source &lt;target_region_openrc&gt;\n</code></pre> <p>Upload the image to the new region. Set the correct disk format, input the path to the image file and select a name for the new image.</p> <pre><code>openstack image create --disk-format &lt;disk_format&gt; --file &lt;local_image_name&gt;.&lt;disk_format&gt; &lt;new_image_name&gt;\n</code></pre> <p>The upload will take some time, depending on your internet upload speed and the size of the image. When the upload is finished you get a printout displaying information about your image.</p> <pre><code>+------------------+-------------------------------------------------------------------------------+\n| Field            | Value                                                                         |\n+------------------+-------------------------------------------------------------------------------+\n| container_format | bare                                                                          |\n| created_at       | 2022-11-17T16:26:58Z                                                          |\n| disk_format      | raw                                                                           |\n| file             | /v2/images/df4593a9-a4d4-46fe-9c82-1b8f88ecac5d/file                          |\n| id               | df4593a9-a4d4-46fe-9c82-1b8f88ecac5d                                          |\n| min_disk         | 0                                                                             |\n| min_ram          | 0                                                                             |\n| name             | &lt;new_image_name&gt;                                                              |\n| owner            | facabd68822643d19be8c9de84e27c49                                              |\n| properties       | locations='[]', os_hidden='False', owner_specified.openstack.md5='',          |\n|                  | owner_specified.openstack.object='images/&lt;new_image_name&gt;',                   |\n|                  | owner_specified.openstack.sha256=''                                           |\n| protected        | False                                                                         |\n| schema           | /v2/schemas/image                                                             |\n| status           | saving                                                                        |\n| tags             |                                                                               |\n| updated_at       | 2022-11-17T16:26:58Z                                                          |\n| visibility       | shared                                                                        |\n+------------------+-------------------------------------------------------------------------------+\n</code></pre> <p>But your image is not yet ready to use, Cleura\u00a0Cloud still needs to process the file, which shouldn\u2019t take long. To check the status of the image, use the ID of the new image with the following command:</p> <pre><code>$ openstack image show &lt;new_image_id&gt;\n+------------------+-------------------------------------------------------------------------------+\n| Field            | Value                                                                         |\n+------------------+-------------------------------------------------------------------------------+\n| checksum         | 4b086035a943cc1676583c0cc78f0896                                              |\n| container_format | bare                                                                          |\n| created_at       | 2022-11-17T16:26:58Z                                                          |\n| disk_format      | raw                                                                           |\n| file             | /v2/images/df4593a9-a4d4-46fe-9c82-1b8f88ecac5d/file                          |\n| id               | df4593a9-a4d4-46fe-9c82-1b8f88ecac5d                                          |\n| min_disk         | 0                                                                             |\n| min_ram          | 0                                                                             |\n| name             | &lt;new_image_name&gt;                                                              |\n| owner            | facabd68822643d19be8c9de84e27c49                                              |\n| properties       | locations='[]', os_hidden='False', owner_specified.openstack.md5='',          |\n|                  | owner_specified.openstack.object='images/&lt;new_image_name&gt;',                   |\n|                  | owner_specified.openstack.sha256=''                                           |\n| protected        | False                                                                         |\n| schema           | /v2/schemas/image                                                             |\n| status           | active                                                                        |\n| tags             |                                                                               |\n| updated_at       | 2022-11-17T16:26:58Z                                                          |\n| visibility       | shared                                                                        |\n+------------------+-------------------------------------------------------------------------------+\n</code></pre> <p>When the image\u2019s <code>status</code> value is <code>active</code>, the whole upload process is done.</p> <p>Verify that the checksum of the new image is the same as your local file:</p> <pre><code>openstack image show -c checksum &lt;new_image_id&gt;\n</code></pre> <p>It is now safe to remove the image file from your local computer.</p>"},{"location":"howto/openstack/nova/move-server-between-regions/#creating-a-volume-from-an-image","title":"Creating a volume from an image","text":"<p>First you must choose the image you want to create a volume from.</p> <p>List all your private images with the following command:</p> <pre><code>$ openstack image list --private\n+--------------------------------------+--------------------------+--------+\n| ID                                   | Name                     | Status |\n+--------------------------------------+--------------------------+--------+\n| f9ce95de-564e-4f6e-ad0e-789c84f30b7c | &lt;new_image_name&gt;         | active |\n+--------------------------------------+--------------------------+--------+\n</code></pre> <p>Then create the volume using the ID of the image in this command:</p> <pre><code>openstack volume create --size &lt;GB&gt; --image &lt;new_image_id&gt; &lt;new_volume_name&gt;\n</code></pre> <p>Substitute <code>&lt;GB&gt;</code> with the size in gigabytes you wish the volume to be.</p>"},{"location":"howto/openstack/nova/move-server-between-regions/#creating-a-server-from-a-volume","title":"Creating a server from a volume","text":"<p>Now you need to create the new server using the system volume. To create a new server, follow this guide.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>If you use the Cleura\u00a0Cloud Management\u00a0Panel, when choosing a boot source, select Boot from volume and then your server\u2019s system volume.</p> <p>If you use the OpenStack CLI, forgo the <code>--image</code> and <code>--boot-from-volume</code> options and instead use <code>--volume &lt;new_volume_name&gt;</code></p> <p>If you also moved other volumes, after you have created the server is the time to attach those volumes to the server.</p>"},{"location":"howto/openstack/nova/new-server-cnw/","title":"Creating servers behind a Clavister NetWall instance","text":"<p>Provided you have a Clavister NetWall instance up and running, you may use the Cleura\u00a0Cloud Management\u00a0Panel to create a new server and place it behind the firewall.</p>"},{"location":"howto/openstack/nova/new-server-cnw/#creating-a-new-server","title":"Creating a new server","text":"<p>You may follow our server creation guide using the Cleura\u00a0Cloud Management\u00a0Panel or the OpenStack CLI almost to the letter. You should only pay extra attention to the region the new server will reside in and the network it will be connected to.</p> <p>More specifically, the new server must be in the region where the Clavister NetWall instance you are interested in resides.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>After you type in the new server\u2019s name, choose its region from the drop-down menu named Region.</p> <p></p> <p>Make sure you have sourced an RC file from the same region the Clavister NetWall instance belongs to.</p> <p>In addition to residing in the correct region, the server must also be in a network behind the Clavister NetWall instance.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>While creating your new server, use the Networks drop-down menu to select a network.</p> <p></p> <p>In the <code>openstack</code> CLI client, use the <code>--network</code> parameter to specify a network to which the server will be connected. For instance, here is how to create a server named <code>gostenhof</code>, connected to a network named <code>inside-net</code>, which is behind a Clavister NetWall instance:</p> <pre><code>openstack server create \\\n    --flavor b.1c1gb \\\n    --image \"Ubuntu 24.04 Noble Numbat x86_64\" \\\n    --boot-from-volume 20 \\\n    --network inside-net \\\n    --security-group default \\\n    --key-name my-public-key  \\\n    --wait \\\n    gostenhof\n</code></pre>"},{"location":"howto/openstack/nova/new-server-cnw/#getting-network-connectivity-information","title":"Getting network connectivity information","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Make sure the vertical pane on the left-hand side of the page is expanded. From it, select Compute, then Servers. In the central pane, select the region in which your new server resides. Click on the server row to expand it, and go to the Addresses tab.</p> <p></p> <p>There, you can see the server\u2019s IP address. You might also want to jot down the corresponding MAC address.</p> <p>To get the IP of the <code>gostenhof</code> server, type:</p> <pre><code>$ openstack server show gostenhof -c addresses\n+-----------+---------------------------+\n| Field     | Value                     |\n+-----------+---------------------------+\n| addresses | inside-net=192.168.201.47 |\n+-----------+---------------------------+\n</code></pre> <p>Also, you may want to take note of the server\u2019s network adapter MAC address:</p> <pre><code>$ openstack port list --server gostenhof -c 'MAC Address' -c Status\n+-------------------+--------+\n| MAC Address       | Status |\n+-------------------+--------+\n| fa:16:3e:e8:75:7c | ACTIVE |\n+-------------------+--------+\n</code></pre>"},{"location":"howto/openstack/nova/new-server-cnw/#viewing-the-new-server-from-the-clavister-netwall-dashboard","title":"Viewing the new server from the Clavister NetWall dashboard","text":"<p>Login to the Clavister NetWall instance. From the left-hand side vertical pane, make sure you have expanded the Run-time Information category. Go to the SUB SYSTEMS sub-category and select Neighbor Devices. The firewall has two network interfaces: <code>if1</code> is the external interface, and <code>if2</code> is the internal interface, which any server behind the firewall faces.</p> <p></p> <p>You will notice the IP address of the new server you just spun up and its network adapter MAC address. Finally, in the Status column, there is a green box labeled ACTIVE, indicating that the server is accessible to the firewall.</p>"},{"location":"howto/openstack/nova/new-server/","title":"Creating new servers","text":"<p>Once you have an account in Cleura\u00a0Cloud, you can create virtual machines \u2014 henceforth simply servers \u2014 using either the Cleura\u00a0Cloud Management\u00a0Panel or the OpenStack CLI. Let us demonstrate the creation of a new server, following both approaches.</p>"},{"location":"howto/openstack/nova/new-server/#prerequisites","title":"Prerequisites","text":"<p>You need to have at least one network in the region you are interested in. Additionally, if you prefer to work with the OpenStack CLI, then make sure to properly enable it first.</p>"},{"location":"howto/openstack/nova/new-server/#creating-a-server","title":"Creating a server","text":"<p>To create a server from the Cleura\u00a0Cloud Management\u00a0Panel, fire up your favorite web browser, navigate to the Cleura\u00a0Cloud Management\u00a0Panel start page, and log into your Cleura\u00a0Cloud account. On the other hand, if you prefer to work with the OpenStack CLI, please do not forget to source the RC file first.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>On the top right-hand side of the Cleura\u00a0Cloud Management\u00a0Panel, click the Create button. A new pane titled Create slides into view.</p> <p></p> <p>Notice all these rounded boxes on that pane, each for defining, configuring, and instantiating a different Cleura\u00a0Cloud object. Go ahead and click the Server box.</p> <p>Another pane, titled Create a Server, slides over. At the top, type in a name for the new server and select one of the available regions.</p> <p></p> <p>Set the Connect to parameter to Network. Select at least one of the available networks to attach the new server to.</p> <p></p> <p>If that network is dual-stack or has one IPv4-based subnet, you may want the server to be publicly accessible via an IPv4 address. In that case, make sure to activate the Connect\u00a0a\u00a0floating\u00a0IP\u00a0to\u00a0the\u00a0server switch. Then, use the dropdown menu below to set the Create\u00a0External\u00a0IP\u00a0on parameter to the network you have already attached the server to.</p> <p>Now, pay attention to the Server profile parameter.</p> <p></p> <p>From the corresponding dropdown menu, you must select a profile. Currently, the following profiles are available:</p> <ul> <li>Generic,</li> <li>High CPU, and</li> <li>Low Latency Disk.</li> </ul> <p>Selecting one of those gives you access to a subset of all available flavors.</p> <p>As the profile names suggest, each profile but the Generic points to a subset of flavors suitable for servers intended to run specific types of applications:</p> <ul> <li>the Generic profile is for general-purpose cloud servers,</li> <li>the High CPU profile is for servers designed to host CPU-intensive applications, and</li> <li>the Low Latency Disk profile is for servers that use a local low-latency disk.</li> </ul> <p>We should point out that some profiles may not be available in all regions. Choose the Generic profile for now, which is available in any of the regions.</p> <p>In the Boot source section below, click the dropdown menu on the left and make sure you select Image, so you can choose one of the readily available OS images to boot the new server off of.</p> <p></p> <p>To pick a specific image, click on the dropdown menu on the right. In this example, we have chosen ubuntu in general and Ubuntu 24.04 Noble Numbat x86_64 in particular.</p> <p>Next, notice that the Boot Target is preselected for you. Depending on the server profile you have already selected, the boot target will be either Volume or Ephemeral. Specifically:</p> <ul> <li>for the Generic and High CPU profiles, the boot target is Volume, and</li> <li>for the Low Latency Disk profile, the boot target is Ephemeral.</li> </ul> <p></p> <p>Regarding the server\u2019s CPU core count and amount of memory, set the Flavor accordingly. We suggest selecting a flavor that specifies a server with 1 CPU core and 1 GiB of RAM.</p> <p>Note that, depending on the chosen flavor, the estimated monthly cost of the server changes. (While a server is shut off, you are still getting charged for it, but less so.) At any time, this estimated cost is displayed in the green rectangular area at the top.</p> <p>Something else that affects the cost is the size of the root device. Take a look at the Volume parameter below, and notice the default (in gibibytes). You may leave the root device size unchanged, or modify it to be a bit higher than the default.</p> <p>When, at a later time, you decide to delete the server, you can do so but keep its boot volume (you may want, for example, to attach that volume to a new server). Just disable the Delete\u00a0on\u00a0termination option if you want this kind of flexibility. On the other hand, if you want your root volume to be automatically deleted when the server is deleted, the Delete\u00a0on\u00a0termination option is already enabled for you. In any case, use this option with caution.</p> <p></p> <p>Also, notice the Storage classes (types) parameter, which actually deals with volume types. By default, this parameter is set to cbs.</p> <p>You may want to leave the Disaster\u00a0recovery option enabled. If you do, then daily server snapshots will be created, and you will have the option for easy and fast roll-ups to previous snapshots. Please be aware that leaving this option enabled increases the server\u2019s monthly estimated cost (again, it is displayed in the green rectangular area at the top).</p> <p></p> <p>To control network access to the server, use the dropdown menu to the right of Security Groups and choose one.</p> <p></p> <p>If you already have one or more key pairs in your Cleura\u00a0Cloud account, you can now select a public key to be included in the <code>~/.ssh/authorized_keys</code> file of the server\u2019s default user. (For the image you have selected, that user would be <code>ubuntu</code>.) That way, you can securely log into the remote user\u2019s account via SSH without typing a password.</p> <p></p> <p>In case there are no key pairs to choose from, activate the Set\u00a0password option and set a password for the default user account (<code>ubuntu</code>).</p> <p></p> <p>A configuration script is automatically prepared based on the choices you have already made. That script runs during system boot and performs housekeeping tasks like user account creation, enabling acceptable authentication methods, and configuring remote package repositories. Click on Advanced Options to see the default script.</p> <p></p> <p>Regarding the User-data propagation method above, notice that the Use\u00a0metadata\u00a0service is pre-selected for you. For more on what this is and why you might want to select the Use\u00a0configuration\u00a0drive method, please read our guide on launching a server with a configuration drive.</p> <p>It is now time to create your Cleura\u00a0Cloud server. Click the green Create button, and the new server will be readily available in a few seconds.</p> <p></p> <p>An <code>openstack</code> command for creating a server may look like this:</p> <pre><code>openstack server create \\\n    --flavor $FLAVOR_NAME \\\n    --image $IMAGE_NAME \\\n    --boot-from-volume $VOL_SIZE \\\n    --network $NETWORK_NAME \\\n    --security-group $SEC_GROUP_NAME \\\n    --key-name $KEY_NAME \\\n    --wait \\\n    $SERVER_NAME\n</code></pre> <p>Each variable represents a piece of information we have to look for or, in the cases of <code>KEY_NAME</code> and <code>SERVER_NAME</code>, arbitrarily define.</p> <p>Let us begin with the flavors (<code>FLAVOR_NAME</code>), which describe combinations of CPU core count and memory size. Each server has a distinct flavor, and to see all available flavors type:</p> <pre><code>openstack flavor list\n</code></pre> <p>You will get a pretty long list of flavors. For our demonstration, we suggest you go with <code>b.1c1gb</code>. A server with this particular flavor will have one CPU core and one gibibyte of RAM. Go ahead and set <code>FLAVOR_NAME</code> accordingly:</p> <pre><code>FLAVOR_NAME=\"b.1c1gb\"\n</code></pre> <p>Your server should have an image to boot off of (<code>IMAGE_NAME</code>). For a list of all available images in Cleura\u00a0Cloud, type:</p> <pre><code>openstack image list\n</code></pre> <p>This time, you get a shorter list, but you can still filter for images with the OS you prefer. For example, filter for Ubuntu:</p> <pre><code>openstack image list --tag \"os:ubuntu\"\n</code></pre> <p>Continue with the <code>Ubuntu 24.04 Noble Numbat x86_64</code> image:</p> <pre><code>IMAGE_NAME=\"Ubuntu 24.04 Noble Numbat x86_64\"\n</code></pre> <p>Before you go on, decide on the capacity (in gibibytes) of the server\u2019s boot volume (<code>VOL_SIZE</code>). We suggest you start with 10 gibibytes:</p> <pre><code>VOL_SIZE=\"10\"\n</code></pre> <p>You need at least one network in the region you\u2019re about to create your new server (<code>NETWORK_NAME</code>). To get the names of all available (internal) networks, type:</p> <pre><code>openstack network list --internal -c Name\n</code></pre> <pre><code>+----------------+\n| Name           |\n+----------------+\n| nordostbahnhof |\n+----------------+\n</code></pre> <p>Set the <code>NETWORK_NAME</code> variable accordingly:</p> <pre><code>NETWORK_NAME=\"nordostbahnhof\"\n</code></pre> <p>Regarding the security group (<code>SEC_GROUP_NAME</code>), unless you have already created one yourself, you will find only one per region:</p> <pre><code>openstack security group list -c Name -c Description\n</code></pre> <pre><code>+---------+------------------------+\n| Name    | Description            |\n+---------+------------------------+\n| default | Default security group |\n+---------+------------------------+\n</code></pre> <p>Go ahead and set <code>SEC_GROUP_NAME</code>:</p> <pre><code>SEC_GROUP_NAME=\"default\"\n</code></pre> <p>You most likely want a server you can remotely connect to via SSH without typing a password. Upload one of our public keys to your Cleura\u00a0Cloud account:</p> <pre><code>openstack keypair create --public-key ~/.ssh/id_ed25519.pub bahnhof\n</code></pre> <p>In the example above, we uploaded the public key <code>~/.ssh/id_ed25519.pub</code> to our Cleura\u00a0Cloud account and named it <code>bahnhof</code>. Follow our example, and do not forget to set the <code>KEY_NAME</code>:</p> <pre><code>KEY_NAME=\"bahnhof\"\n</code></pre> <p>By the way, check all uploaded public keys\u2026</p> <pre><code>openstack keypair list\n</code></pre> <p>\u2026and get more information regarding the one you just uploaded:</p> <pre><code>openstack keypair show $KEY_NAME\n</code></pre> <p>You are almost ready to create your new server. Decide on a name\u2026</p> <pre><code>SERVER_NAME=\"zug\" # just an example\n</code></pre> <p>\u2026and then go ahead and create it:</p> <pre><code>openstack server create \\\n    --flavor b.1c1gb \\\n    --image $IMAGE_NAME \\\n    --boot-from-volume 20 \\\n    --network nordostbahnhof \\\n    --security-group default \\\n    --key-name bahnhof \\\n    --wait \\\n    zug\n</code></pre> <p>(For clarity\u2019s sake, and with the exception of <code>$IMAGE_NAME</code>, we used the actual values and not the variables we so meticulously set.) The <code>--wait</code> parameter is optional. Whenever you choose to use it, you get back control of your terminal only after the server is readily available in Cleura\u00a0Cloud.</p> <p>The network your server is attached to might be dual-stack, or it might have an IPv4-based subnet. If this is the case and, additionally, you want the server to be reachable via a public IPv4 address, then you need to create a floating IP and assign it to your server. First, create the floating IP:</p> <pre><code>openstack floating ip create ext-net\n</code></pre> <p>See all floating IPs\u2026</p> <pre><code>openstack floating ip list\n</code></pre> <p>\u2026and assign the one you just created to your server:</p> <pre><code>openstack server add floating ip zug 198.51.100.12\n</code></pre> <p>The username of the default user account in the Ubuntu image is <code>ubuntu</code>, so now you can connect to your remote server via SSH without typing a password:</p> <pre><code>ssh ubuntu@198.51.100.12\n</code></pre> <p>Alternatively, you may also use the following command, which is more flexible though slightly longer:</p> <pre><code>openstack server ssh --public zug -- -l ubuntu\n</code></pre>"},{"location":"howto/openstack/nova/new-server/#viewing-information-about-the-newly-created-server","title":"Viewing information about the newly created server","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>At any time, from the Cleura\u00a0Cloud Management\u00a0Panel you may see all servers, and get detailed information regarding each one of them. Expand the left-hand side vertical pane, click Compute, then Servers, and, in the central pane, select the region you want.</p> <p></p> <p>To see all available servers in the region, type:</p> <pre><code>openstack server list\n</code></pre> <p>You can always get specific information on a particular server:</p> <pre><code>openstack server show zug\n</code></pre>"},{"location":"howto/openstack/nova/new-server/#connecting-to-the-server-console","title":"Connecting to the server console","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>While viewing information regarding your server, you may get its IPv6 or public IPv4 address (e.g., from the Addresses tab), and connect to it remotely via SSH. Alternatively, you may launch a web console and log in. Click on the three-dot icon on the right of the server header, and from the pop-up menu that appears, select Remote Console.</p> <p></p> <p>A new window pops up, and that\u2019s your web console to your Cleura\u00a0Cloud server. Please note that this window cannot be resized but can be opened in a new browser window or tab.</p> <p></p> <p>You may have access to the web console of your server, and you need the corresponding URL for it:</p> <pre><code>openstack console url show zug\n</code></pre> <p>Usage of the web console is discouraged, though. Instead, securely connect to your server via SSH.</p>"},{"location":"howto/openstack/nova/rescue-server/","title":"Rescuing a server","text":"<p>This guide will walk you through the required steps to access a server in rescue\u00a0mode when you need to.</p> <p>You can use rescue mode to access the server\u2019s operating system disk to fix a corrupted file system, reset access credentials on the server, and do other emergency recovery tasks.</p>"},{"location":"howto/openstack/nova/rescue-server/#prerequisites","title":"Prerequisites","text":"<p>You need to have previously created a server that you now wish to rescue. Additionally, if you prefer to work with the OpenStack CLI, then make sure to properly enable it first.</p>"},{"location":"howto/openstack/nova/rescue-server/#initiating-the-rescue","title":"Initiating the rescue","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Navigate to the server list.</p> <p></p> <p>Find the server you want to rescue in the list, and on the right-hand side, click on its menu button.</p> <p></p> <p>Click on Rescue Server.</p> <p></p> <p>Once selected, the rescue dialog appears. Leave the default option, Use System Rescue Image.</p> <p>Click No to cancel or Rescue to proceed.</p> <p></p> <p>When a server has been switched to rescue mode, its status icon appears with an exclamation mark:</p> <p></p> <p>You must first select a system rescue image from the available images:</p> <pre><code>$ openstack image list --tag system-rescue\n+--------------------------------------+---------------+--------+\n| ID                                   | Name          | Status |\n+--------------------------------------+---------------+--------+\n| cb2217f3-1ca7-4440-b10e-df7ff2d92cae | system-rescue | active |\n+--------------------------------------+---------------+--------+\n</code></pre> <p>To start the server using the system rescue image, use the following command, substituting the correct ID for the <code>system-rescue</code> image in your Cleura\u00a0Cloud region:</p> <pre><code>openstack --os-compute-api-version 2.87 \\\n  server rescue \\\n  --image cb2217f3-1ca7-4440-b10e-df7ff2d92cae &lt;server_id&gt;\n</code></pre> <p>While the rescue is ongoing, the server should have the <code>OS-EXT-STS:vm_state</code> of <code>rescued</code> and the <code>status</code> of <code>RESCUE</code>.</p> <pre><code>$ openstack server show -v OS-EXT-STS:vm_state -c status &lt;server_id&gt;\n+---------------------+---------+\n| Field               | Value   |\n+---------------------+---------+\n| OS-EXT-STS:vm_state | rescued |\n| status              | RESCUE  |\n+---------------------+---------+\n</code></pre>"},{"location":"howto/openstack/nova/rescue-server/#accessing-the-server-in-rescue-mode","title":"Accessing the server in rescue mode","text":"<p>You can now proceed to accessing the remote console of your server, as you would with any other newly launched server.</p> <p>Please refer to the System\u00a0Rescue documentation documentation for details on the available tools and features bundled with System Rescue.</p>"},{"location":"howto/openstack/nova/resize-server/","title":"Resizing a server","text":"<p>This guide will walk you through the required steps to change the number of CPU cores and the amount of memory your server has access to, this is done by changing the server\u2019s flavor.</p> <p>Resize (or Server resize) is the ability to change the flavor of a server, thus allowing it to upscale or downscale according to user needs. A resize operation is a two-step process for the user:</p> <ol> <li>Initiate the resize.</li> <li>Either confirm (verify) success and release the old server, or    declare a revert to release the new server and restart the old one.</li> </ol>"},{"location":"howto/openstack/nova/resize-server/#prerequisites","title":"Prerequisites","text":"<p>You need to have a server you wish to resize. Additionally, if you prefer to work with the OpenStack CLI, then make sure to properly enable it first.</p>"},{"location":"howto/openstack/nova/resize-server/#listing-available-flavors","title":"Listing available flavors","text":"Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Navigate to the server list.</p> <p></p> <p>Find the server you want to resize in the list, and on the right-hand side click on its menu button.</p> <p></p> <p>Click on Modify Server.</p> <p></p> <p>On this panel near the top, find the section called Flavor. This is the current flavor used by the server. Press on the dropdown menu to see all available flavors.</p> <p></p> <p>To list all available flavors you can simply run <code>openstack flavor list</code>, but that will return a very long unsorted list, instead we recommend the following command:</p> <pre><code>openstack flavor list -c Name -f value | grep '[0-9]c[0-9]' | sort -V\n</code></pre> <p>The printout is a simple and clean list, sorted by the compute type, the number of cores and then by the amount of memory.</p> <pre><code>b.1c1gb\nb.1c2gb\nb.1c4gb\nb.2c2gb\nb.2c4gb\nb.2c8gb\nb.2c16gb\nb.4c4gb\n...\n</code></pre>"},{"location":"howto/openstack/nova/resize-server/#initiating-the-resize","title":"Initiating the resize","text":"<p>Choose a new flavor that you want your server to use instead.</p> <p>A resize is only possible with flavors using the same prefix letter. Most commonly you will have a <code>b.</code> flavor, thus you must select another <code>b.</code> flavor.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Once selected, the Resize button will appear.</p> <p></p> <p>Click on it to start the resize.</p> <p>While the resize is ongoing you will see a spinning circle saying Resize is in progress.</p> <p></p> <p>To start the resize use the following command:</p> <pre><code>openstack server resize --flavor &lt;new_flavor&gt; &lt;server_id&gt;\n</code></pre> <p>While the resize is ongoing the server should have the <code>OS-EXT-STS:task_state</code> of <code>resize_migrating</code> and the <code>status</code> of <code>RESIZE</code>.</p> <pre><code>openstack server show -c OS-EXT-STS:task_state -c OS-EXT-STS:vm_state -c status &lt;server_id&gt;\n</code></pre> <pre><code>+-----------------------+------------------+\n| Field                 | Value            |\n+-----------------------+------------------+\n| OS-EXT-STS:task_state | resize_migrating |\n| OS-EXT-STS:vm_state   | active           |\n| status                | RESIZE           |\n+-----------------------+------------------+\n</code></pre> <p>You may proceed with the next step once your server status is <code>VERIFY_RESIZE</code>.</p> <pre><code>+-----------------------+---------------+\n| Field                 | Value         |\n+-----------------------+---------------+\n| OS-EXT-STS:task_state | None          |\n| OS-EXT-STS:vm_state   | resized       |\n| status                | VERIFY_RESIZE |\n+-----------------------+---------------+\n</code></pre> <p>The resize process might take a minute or more. Cleura\u00a0Cloud will now make a restore point in case the resize process fails. It would then restore your server to the state it was before the resize.</p>"},{"location":"howto/openstack/nova/resize-server/#confirming-the-resize","title":"Confirming the resize","text":"<p>Your server is now using the new flavor you selected earlier, and you need to make sure the server is working as intended after the resize.</p> <p>Once you are certain your server is working as intended, you should confirm the resize. If you do not confirm the resize, your server will automatically have the resize confirmed after 24 hours.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>This is done by clicking the Confirm button.</p> <p></p> <p>If your server is not working as intended, or you simply regret the resize, instead click Cancel.</p> <p>This is done by using the following command:</p> <pre><code>openstack server resize confirm &lt;server_id&gt;\n</code></pre> <p>Alternatively, if your server is not working as intended, revert the resize with the following command:</p> <pre><code>openstack server resize revert &lt;server_id&gt;\n</code></pre> <p>This concludes the process of resizing a server in Cleura\u00a0Cloud.</p>"},{"location":"howto/openstack/nova/restore-srv-to-snap/","title":"Restoring a server to a snapshot","text":"<p>Servers in Cleura\u00a0Cloud that have the disaster recovery feature enabled can go back in time, meaning you may restore such a server to one of the available point-in-time snapshots. Here is how you can do that.</p>"},{"location":"howto/openstack/nova/restore-srv-to-snap/#how-to-restore-step-by-step","title":"How to restore, step by step","text":"<p>To select a particular snapshot and restore your server to it, first off, navigate to the Cleura\u00a0Cloud Management\u00a0Panel. From the left-hand side vertical pane, choose Compute \u2192 Servers, then expand the detailed view of the server of interest. Select the Disaster Recovery tab and see all available snapshots listed below. For each one of those, the date and creation time are visible.</p> <p></p> <p>Click on the snapshot you want. A pop-up window will appear, asking for your permission to proceed. Make sure this is indeed the snapshot you want, then click the red button labeled Yes, Restore snapshot.</p> <p></p> <p>The restore process will begin immediately. You can tell it is progressing by checking the status.</p> <p></p> <p>As soon as the snapshot is ready to be used, you will see a message indicating that it has started. Do not forget to activate the server; click on its row to select it, then click the  button above.</p> <p></p>"},{"location":"howto/openstack/nova/server-group/","title":"Using server groups","text":"<p>In Cleura\u00a0Cloud, you can use server groups to control the scheduling of a group of servers.</p>"},{"location":"howto/openstack/nova/server-group/#prerequisites","title":"Prerequisites","text":"<p>In order to use server groups you must use the OpenStack CLI. Make sure you have enabled it.</p>"},{"location":"howto/openstack/nova/server-group/#policies","title":"Policies","text":"<p>A server group can have one of four different policies: affinity, soft-affinity, anti-affinity or soft-anti-affinity.</p>"},{"location":"howto/openstack/nova/server-group/#affinity","title":"Affinity","text":"<p>A server group with the policy of <code>affinity</code> will make sure that all the servers in that group are always placed on the same physical compute node.</p>"},{"location":"howto/openstack/nova/server-group/#soft-affinity","title":"Soft affinity","text":"<p>A policy of <code>soft-affinity</code> will try to make sure that all the servers in that group are placed on the same physical compute node, but ultimately will allow it if otherwise not possible.</p>"},{"location":"howto/openstack/nova/server-group/#anti-affinity","title":"Anti-affinity","text":"<p>A server group with the policy of <code>anti-affinity</code> will make sure that the servers in that group are never placed on the same physical compute node.</p>"},{"location":"howto/openstack/nova/server-group/#soft-anti-affinity","title":"Soft anti-affinity","text":"<p>A policy of <code>soft-anti-affinity</code> will try to make sure that all the servers in that group are not placed on the same physical compute node, but ultimately will allow it if otherwise not possible.</p>"},{"location":"howto/openstack/nova/server-group/#creating-server-groups","title":"Creating server groups","text":"<p>To create a server group, use the following command:</p> <pre><code>openstack server group create \\\n  --policy [affinity|soft-affinity|anti-affinity|soft-anti-affinity] \\\n  &lt;server_group_name&gt;\n</code></pre> <p>With an <code>anti-affinity</code> or <code>soft-anti-affinity</code> policy, you may also configure how many servers you want to allow on the same physical compute node. To do this, use the option <code>--rule\u00a0max_server_per_host=&lt;number&gt;</code>, where <code>&lt;number&gt;</code> is the amount of servers to allow on the same physical compute node.</p>"},{"location":"howto/openstack/nova/server-group/#creating-servers-using-server-groups","title":"Creating servers using server groups","text":"<p>To apply a server group policy, you must specify the group when creating a server, as a scheduling hint. To do that, use the <code>--hint</code> parameter in the following command:</p> <pre><code>openstack server create --hint group=&lt;server_group_id&gt; [...] &lt;server_name&gt;\n</code></pre> <p>If you subsequently launch more servers referencing the same server group, Cleura\u00a0Cloud concentrates or distributes them according to the server group\u2019s policy.</p>"},{"location":"howto/openstack/nova/server-group/#troubleshooting-common-issues","title":"Troubleshooting common issues","text":"<p>If you keep creating servers within a server group with a policy of <code>anti-affinity</code>, you will eventually exceed the total amount of physical compute nodes in the region. The command will still succeed, but the server will subsequently fail to be scheduled to a compute node. Instead, it will assume the <code>ERROR</code> status with the following <code>fault</code> message: No valid host was found. There are not enough hosts available.</p> <pre><code>$ openstack server show -c fault -c status &lt;server_id&gt;\n+--------+--------------------------------------------------+\n| Field  | Value                                            |\n+--------+--------------------------------------------------+\n| fault  | {'code': 500, 'created': '2022-12-23T11:21:33Z', |\n|        | 'message': 'No valid host was found.             |\n|        | There are not enough hosts available.'}          |\n| status | ERROR                                            |\n+--------+--------------------------------------------------+\n</code></pre> <p>This is normal as Cleura\u00a0Cloud cannot schedule the server on a different physical compute node because there is already a server in the server group on every node. The same scheduling error and \u201cfault\u201d message will occur when using a server group with a policy of <code>affinity</code> as well, when you create more servers than a physical compute node can host.</p> <p>However when using a soft affinity policy, such as <code>soft-affinity</code> or <code>soft-anti-affinity</code>, the scheduler is allowed to break the server group\u2019s policy if it is unable to uphold it. This means you may want to verify that your servers are on the same or on different physical compute nodes by looking at the hostId value of your servers.</p> <pre><code>$ openstack server show -c hostId &lt;server_id&gt;\n+--------+----------------------------------------------------------+\n| hostId | 8fae028139411e9e125d5f39895bef79f916aefce6003a7888de105d |\n+--------+----------------------------------------------------------+\n</code></pre> <p><code>hostId</code> is a unique identifier for each physical compute node. By comparing this value on your different servers you can be sure if your server group policy is being upheld or not.</p> <p>It\u2019s not possible to compare <code>hostId</code> between different projects because the values are unique for each project.</p>"},{"location":"howto/openstack/octavia/lbaas-l7pol/","title":"Using layer 7 redirection","text":"<p>Unlike TCP-based load balancers, which may be considered low-level, Layer 7 load balancers follow high-level application logic to redirect client requests to back-end server pools. They take their name from the OSI model, where Layer 7 is also known as the Application Layer. A Layer 7, or simply L7, load balancer decides where to redirect incoming packets based on URI, host, HTTP headers, etc.</p> <p>One common application of L7 load balancing is HTTP to HTTPS redirection. More specifically, you may have an HTTPS-terminated load balancer that distributes incoming client traffic to one or more back-end services, and you are looking for a way to automatically turn each H\u03a4TP request into an HTTPS one. To have this kind of automatic redirection, you equip your load balancer with a new listener that acknowledges incoming HTTP requests and silently forwards them to the existing HTTPS-based listener.</p> <p>The HTTP listener will apply a specific L7 policy to accomplish this. In general, an L7 policy is nothing but a set of one or more L7 rules, along with a predefined action. That action is fallowed when all L7 rules evaluate to <code>true</code>, and we should point out that an L7 rule is a logical test that evaluates to either <code>true</code> or <code>false</code>.</p> <p>In what follows, we show, step by step, how we add such a listener, policy, and set of rules to an existing HTTPS-terminated load balancer.</p>"},{"location":"howto/openstack/octavia/lbaas-l7pol/#prerequisites","title":"Prerequisites","text":"<p>The Cleura\u00a0Cloud Management\u00a0Panel does not support defining L7 policies and rules, so you will have to work with the OpenStack CLI. Enable it for the region you will be working in, and make sure you have the Python <code>octaviaclient</code> module installed. For that, use either the package manager of your operating system or <code>pip</code>:</p> Debian/UbuntuMac OS X with HomebrewPython Package <pre><code>apt install python3-octaviaclient\n</code></pre> <p>This particular Python module is unavailable via <code>brew</code>, but you can install it via <code>pip</code>.</p> <pre><code>pip install python-octaviaclient\n</code></pre>"},{"location":"howto/openstack/octavia/lbaas-l7pol/#assumptions-and-scenario","title":"Assumptions and scenario","text":"<p>We assume you already have an HTTPS-terminated load balancer that forwards client requests to a back-end server pool. In our test scenario, the load balancer was accepting HTTPS requests for <code>whoogle.example.com</code> and forwarding them to a two-member pool, with servers each running a Docker container for Whoogle Search.</p>"},{"location":"howto/openstack/octavia/lbaas-l7pol/#creating-an-http-listener","title":"Creating an HTTP listener","text":"<p>Here is <code>mylb</code>, the load balancer we used during testing\u2026</p> <pre><code>$ openstack loadbalancer list\n\n+---------------+------+---------------+--------------+---------------------+------------------+----------+\n| id            | name | project_id    | vip_address  | provisioning_status | operating_status | provider |\n+---------------+------+---------------+--------------+---------------------+------------------+----------+\n| eaf6d4f3-     | mylb | dfc7004673964 | 10.15.25.155 | ACTIVE              | ONLINE           | amphora  |\n| 8d73-4b77-    |      | 28bacba4376e7 |              |                     |                  |          |\n| 8bc4-         |      | 2cc3e9        |              |                     |                  |          |\n| 788a3b4e3916  |      |               |              |                     |                  |          |\n+---------------+------+---------------+--------------+---------------------+------------------+----------+\n</code></pre> <p>\u2026and here is <code>mylb-listener-https</code>, the HTTPS-terminated listener of <code>mylb</code>:</p> <pre><code>$ openstack loadbalancer listener list\n\n+-------------+-----------------+-------------+-------------+-------------+---------------+----------------+\n| id          | default_pool_id | name        | project_id  | protocol    | protocol_port | admin_state_up |\n+-------------+-----------------+-------------+-------------+-------------+---------------+----------------+\n| 95414884-   | 3a683bfb-6c77-  | mylb-       | dfc70046739 | TERMINATED_ |           443 | True           |\n| f9f2-4f76-  | 4ffd-befa-      | listener-   | 6428bacba43 | HTTPS       |               |                |\n| 8aa5-       | 9f5e0e168538    | https       | 76e72cc3e9  |             |               |                |\n| 56741f12825 |                 |             |             |             |               |                |\n| 2           |                 |             |             |             |               |                |\n+-------------+-----------------+-------------+-------------+-------------+---------------+----------------+\n</code></pre> <p>You may now add <code>mylb-listener-http</code>, a new HTTP-based listener for <code>mylb</code>:</p> <pre><code>openstack loadbalancer listener create \\\n    --name mylb-listener-http \\\n    --protocol HTTP \\\n    --protocol-port 80 \\\n    mylb\n</code></pre> <pre><code>+-----------------------------+--------------------------------------+\n| Field                       | Value                                |\n+-----------------------------+--------------------------------------+\n| admin_state_up              | True                                 |\n| connection_limit            | -1                                   |\n| created_at                  | 2023-01-29T16:12:24                  |\n| default_pool_id             | None                                 |\n| default_tls_container_ref   | None                                 |\n| description                 |                                      |\n| id                          | 798e9f76-300d-4769-9dc9-2ee111ed2af7 |\n| insert_headers              | None                                 |\n| l7policies                  |                                      |\n| loadbalancers               | eaf6d4f3-8d73-4b77-8bc4-788a3b4e3916 |\n| name                        | mylb-listener-http                   |\n| operating_status            | OFFLINE                              |\n| project_id                  | dfc700467396428bacba4376e72cc3e9     |\n| protocol                    | HTTP                                 |\n| protocol_port               | 80                                   |\n| provisioning_status         | PENDING_CREATE                       |\n| sni_container_refs          | []                                   |\n| timeout_client_data         | 50000                                |\n| timeout_member_connect      | 5000                                 |\n| timeout_member_data         | 50000                                |\n| timeout_tcp_inspect         | 0                                    |\n| updated_at                  | None                                 |\n| client_ca_tls_container_ref | None                                 |\n| client_authentication       | NONE                                 |\n| client_crl_container_ref    | None                                 |\n| allowed_cidrs               | None                                 |\n| tls_ciphers                 | None                                 |\n| tls_versions                | None                                 |\n| alpn_protocols              | None                                 |\n| tags                        |                                      |\n+-----------------------------+--------------------------------------+\n</code></pre> <p>To check the provisioning status of <code>mylb-listener-http</code>, type:</p> <pre><code>openstack loadbalancer listener show mylb-listener-http -c provisioning_status\n</code></pre> <pre><code>+---------------------+--------+\n| Field               | Value  |\n+---------------------+--------+\n| provisioning_status | ACTIVE |\n+---------------------+--------+\n</code></pre> <p>Have a look at both listeners of <code>mylb</code>:</p> <pre><code>openstack loadbalancer listener list\n</code></pre> <pre><code>+-------------+-----------------+-------------+-------------+-------------+---------------+----------------+\n| id          | default_pool_id | name        | project_id  | protocol    | protocol_port | admin_state_up |\n+-------------+-----------------+-------------+-------------+-------------+---------------+----------------+\n| 95414884-   | 3a683bfb-6c77-  | mylb-       | dfc70046739 | TERMINATED_ |           443 | True           |\n| f9f2-4f76-  | 4ffd-befa-      | listener-   | 6428bacba43 | HTTPS       |               |                |\n| 8aa5-       | 9f5e0e168538    | https       | 76e72cc3e9  |             |               |                |\n| 56741f12825 |                 |             |             |             |               |                |\n| 2           |                 |             |             |             |               |                |\n| 798e9f76-   | None            | mylb-       | dfc70046739 | HTTP        |            80 | True           |\n| 300d-4769-  |                 | listener-   | 6428bacba43 |             |               |                |\n| 9dc9-       |                 | http        | 76e72cc3e9  |             |               |                |\n| 2ee111ed2af |                 |             |             |             |               |                |\n| 7           |                 |             |             |             |               |                |\n+-------------+-----------------+-------------+-------------+-------------+---------------+----------------+\n</code></pre>"},{"location":"howto/openstack/octavia/lbaas-l7pol/#adding-policies-and-rules","title":"Adding policies and rules","text":"<p>Create <code>mylb-listener-http-policy</code>, a policy for <code>mylb-listener-http</code>:</p> <pre><code>openstack loadbalancer l7policy create \\\n    --action REDIRECT_PREFIX \\\n    --redirect-prefix https://whoogle.example.com \\\n    --name mylb-listener-http-policy \\\n    mylb-listener-http\n</code></pre> <pre><code>+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| listener_id         | 798e9f76-300d-4769-9dc9-2ee111ed2af7 |\n| description         |                                      |\n| admin_state_up      | True                                 |\n| rules               |                                      |\n| project_id          | dfc700467396428bacba4376e72cc3e9     |\n| created_at          | 2023-01-29T16:24:26                  |\n| provisioning_status | PENDING_CREATE                       |\n| updated_at          | None                                 |\n| redirect_pool_id    | None                                 |\n| redirect_url        | None                                 |\n| redirect_prefix     | https://whoogle.example.com          |\n| action              | REDIRECT_PREFIX                      |\n| position            | 1                                    |\n| id                  | baafb237-a7fa-4b26-b4f3-a8e5a0e5ec40 |\n| operating_status    | OFFLINE                              |\n| name                | mylb-listener-http-policy            |\n| redirect_http_code  | 302                                  |\n| tags                |                                      |\n+---------------------+--------------------------------------+\n</code></pre> <p>Make sure the policy is active:</p> <pre><code>openstack loadbalancer l7policy list -c name -c provisioning_status\n</code></pre> <pre><code>+---------------------------+---------------------+\n| name                      | provisioning_status |\n+---------------------------+---------------------+\n| mylb-listener-http-policy | ACTIVE              |\n+---------------------------+---------------------+\n</code></pre> <p>Then, add a single rule to <code>mylb-listener-http-policy</code>:</p> <pre><code>openstack loadbalancer l7rule create \\\n    --compare-type EQUAL_TO \\\n    --type HOST_NAME \\\n    --value whoogle.example.com \\\n    mylb-listener-http-policy\n</code></pre> <pre><code>+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| created_at          | 2023-01-29T16:34:21                  |\n| compare_type        | STARTS_WITH                          |\n| provisioning_status | PENDING_CREATE                       |\n| invert              | False                                |\n| admin_state_up      | True                                 |\n| updated_at          | None                                 |\n| value               | whoogle.example.com                  |\n| key                 | None                                 |\n| project_id          | dfc700467396428bacba4376e72cc3e9     |\n| type                | PATH                                 |\n| id                  | 40ebeef4-9ee2-42c9-8119-c2e478b48a21 |\n| operating_status    | OFFLINE                              |\n| tags                |                                      |\n+---------------------+--------------------------------------+\n</code></pre> <p>Check the provisioning status of the new rule:</p> <pre><code>openstack loadbalancer l7policy list -c name -c provisioning_status\n</code></pre> <pre><code>+---------------------------+---------------------+\n| name                      | provisioning_status |\n+---------------------------+---------------------+\n| mylb-listener-http-policy | ACTIVE              |\n+---------------------------+---------------------+\n</code></pre> <p>From now on, all client attempts to reach <code>http://whoogle.example.com</code> will end up at <code>https://whoogle.example.com</code>. You may confirm this is the case with any web browser or from your terminal, e.g., using <code>curl</code> like this:</p> <pre><code>curl -IL http://whoogle.example.com\n</code></pre> <pre><code>HTTP/1.1 302 Found\ncontent-length: 0\nlocation: https://whoogle.example.com/\ncache-control: no-cache\n\nHTTP/2 200\ncontent-length: 13154\ncontent-type: text/html; charset=utf-8\ndate: Tue, 31 Jan 2023 13:50:59 GMT\nserver: waitress\nset-cookie: ...\n</code></pre> <p>As you can see in the output, the first thing that happens when visiting <code>http://whoogle.example.com</code> is a redirection (<code>HTTP/1.1 302 Found</code>) to <code>https://whoogle.example.com</code>.</p>"},{"location":"howto/openstack/octavia/lbaas-tcp/","title":"Setting up a TCP load balancer","text":"<p>Octavia is a Load Balancer as a Service (LBaaS) solution designed to work with OpenStack. In this guide, we show how to instantiate and configure a simple TCP load balancer in Cleura\u00a0Cloud. All intermediate steps are presented and explained through a specific scenario, and we work using either the Cleura\u00a0Cloud Management\u00a0Panel or the OpenStack CLI.</p>"},{"location":"howto/openstack/octavia/lbaas-tcp/#prerequisites","title":"Prerequisites","text":"<p>Whether you choose the Cleura\u00a0Cloud Management\u00a0Panel or the OpenStack CLI, you need to have an account in Cleura\u00a0Cloud. Additionally, to use the OpenStack CLI, make sure to enable it for the region you will be working in. Besides the Python <code>openstackclient</code> module, you will also have to install the Python <code>octaviaclient</code> module. For that, use either the package manager of your operating system or <code>pip</code>:</p> Debian/UbuntuMac OS X with HomebrewPython Package <pre><code>apt install python3-octaviaclient\n</code></pre> <p>This particular Python module is unavailable via <code>brew</code>, but you can install it via <code>pip</code>.</p> <pre><code>pip install python-octaviaclient\n</code></pre>"},{"location":"howto/openstack/octavia/lbaas-tcp/#scenario-and-terminology","title":"Scenario and terminology","text":"<p>To configure and test our basic TCP load balancer, we have two servers in the same Cleura\u00a0Cloud region, both having <code>ncat</code> listening on port 61234/TCP and returning a greeting to clients asking to connect to that port. Both servers are in the same internal network, and our load balancer will be responsible for redirecting client connections to them in a round-robin fashion. The load balancer will have a single floating IP address, so prospective clients will have to know only that address and not any of the backend server addresses.</p> <p>Please note that while working with your load balancer, you will encounter some technical terms which go a long way toward conceptualizing the inner logic of the whole LBaaS system. This guide introduces those terms not beforehand but exactly when needed.</p>"},{"location":"howto/openstack/octavia/lbaas-tcp/#creating-a-load-balancer","title":"Creating a load balancer","text":"<p>That is only the first step towards our goal, which is to have a fully functional TCP load balancer. We will also need a listener and a pool, but first things first.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Fire up your favorite web browser, navigate to the Cleura\u00a0Cloud Management\u00a0Panel start page, and log into your Cleura\u00a0Cloud account. On the top right-hand side of the Cleura\u00a0Cloud Management\u00a0Panel, click the Create button. A new pane titled Create will slide into view from the right-hand side of the browser window. You will notice several rounded boxes on that pane, each for defining, configuring, and instantiating a different Cleura\u00a0Cloud object. Go ahead and click the Load Balancer box.</p> <p></p> <p>A new pane titled Create a Load Balancer will slide over. At the top, type in a name for the new load balancer and select one of the available regions. Optionally, type in a description.</p> <p></p> <p>In the same pane, scroll down a bit if you have to and activate the Subnet radio button. Then, from the Subnet dropdown menu below, select an appropriate subnet for the new load balancer to move in front of. In our example, the two test servers we have are members of the <code>network-fra1</code> internal network, and <code>subnet-fra1</code> is the name of the corresponding subnet. Click the green Create button below to instantiate the new load balancer.</p> <p></p> <p>The creation of the new load balancer starts and, unless something goes wrong, finishes successfully in a minute or so. For a view of the load balancer, make sure the left-hand side vertical pane of the Cleura\u00a0Cloud Management\u00a0Panel is fully visible, click on the Networking category to expand it, and then click once more on the Load Balancers option. In the main area of the Cleura\u00a0Cloud Management\u00a0Panel, select the new load balancer, click the three-dot icon on the right, and select View details from the pop-up menu that appears.</p> <p></p> <p>To create your load balancer, type something like this:</p> <pre><code>openstack loadbalancer create \\\n    --name mylb --vip-subnet-id subnet-fra1\n</code></pre> <pre><code>+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| admin_state_up      | True                                 |\n| availability_zone   | None                                 |\n| created_at          | 2023-01-16T19:20:01                  |\n| description         |                                      |\n| flavor_id           | None                                 |\n| id                  | 6f04228a-19ca-4e94-8965-10f146a9ad81 |\n| listeners           |                                      |\n| name                | mylb                                 |\n| operating_status    | OFFLINE                              |\n| pools               |                                      |\n| project_id          | dfc700467396428bacba4376e72cc3e9     |\n| provider            | amphora                              |\n| provisioning_status | PENDING_CREATE                       |\n| updated_at          | None                                 |\n| vip_address         | 10.15.25.169                         |\n| vip_network_id      | 6c340d2b-e2cf-478a-9074-37e6770decf8 |\n| vip_port_id         | c5b49ecb-3631-4e1c-99f0-04b3fba314e2 |\n| vip_qos_policy_id   | None                                 |\n| vip_subnet_id       | df6fb6ca-4751-4b74-8b3e-5fbda0117cea |\n| tags                |                                      |\n| additional_vips     |                                      |\n+---------------------+--------------------------------------+\n</code></pre> <p>In the example above, the name of the load balancer is <code>mylb</code>, and the subnet it will be in front of is named <code>subnet-fra1</code> (this is the subnet our two test servers are members of). You will notice in the command output that, at first, the <code>provisioning_status</code> is <code>PENDING_CREATE</code>. To make sure the load balancer has been successfully created, try this command a couple of times:</p> <pre><code>openstack loadbalancer show mylb -c provisioning_status\n</code></pre> <pre><code>+---------------------+--------+\n| Field               | Value  |\n+---------------------+--------+\n| provisioning_status | ACTIVE |\n+---------------------+--------+\n</code></pre>"},{"location":"howto/openstack/octavia/lbaas-tcp/#creating-a-listener","title":"Creating a listener","text":"<p>A load balancer needs a way to listen for incoming client connection requests. The listener allows a load balancer to do just that. The load balancer you instantiated has no listener, so let us see how you can equip it with one.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>Looking at the detailed view of the new load balancer, you see three tabs: Details, Listeners, and Pools. Click the Listeners tab and notice the message: \u201cNo Listeners for this LoadBalancer\u201d. Time to create one, so click the green button labeled Create a Listener.</p> <p></p> <p>From the right-hand side of the Cleura\u00a0Cloud Management\u00a0Panel, a pane named Create a Listener slides over. Type in a name for the new listener \u2014 and optionally a description.</p> <p></p> <p>Further down, from the Protocol dropdown menu, select the protocol the listener will be paying attention to. Since you are setting up a TCP load balancer, the protocol for the listener will also have to be TCP. Additionally, type in the listening port. The load balancer will be redirecting connections to a couple of servers, which listen for connections on port 61234/TCP, so it makes sense \u2014though it is not necessary\u2014 for the load balancer to use that same listening port. Since you have not yet defined a pool, ignore the Default pool dropdown menu for now. Instead, go ahead and click the green Create button.</p> <p></p> <p>The listener will be created in no time, and you will be able to see its characteristics in the Listeners tab of your load balancer detailed view.</p> <p></p> <p>Create a listener for your load balancer like this:</p> <pre><code>openstack loadbalancer listener create \\\n    --name mylb-listener --protocol TCP --protocol-port 61234 mylb\n</code></pre> <pre><code>+-----------------------------+--------------------------------------+\n| Field                       | Value                                |\n+-----------------------------+--------------------------------------+\n| admin_state_up              | True                                 |\n| connection_limit            | -1                                   |\n| created_at                  | 2023-01-16T19:32:59                  |\n| default_pool_id             | None                                 |\n| default_tls_container_ref   | None                                 |\n| description                 |                                      |\n| id                          | 8b4e3309-c460-44a8-a0af-8dd2868138f9 |\n| insert_headers              | None                                 |\n| l7policies                  |                                      |\n| loadbalancers               | 6f04228a-19ca-4e94-8965-10f146a9ad81 |\n| name                        | mylb-listener                        |\n| operating_status            | OFFLINE                              |\n| project_id                  | dfc700467396428bacba4376e72cc3e9     |\n| protocol                    | TCP                                  |\n| protocol_port               | 61234                                |\n| provisioning_status         | PENDING_CREATE                       |\n| sni_container_refs          | []                                   |\n| timeout_client_data         | 50000                                |\n| timeout_member_connect      | 5000                                 |\n| timeout_member_data         | 50000                                |\n| timeout_tcp_inspect         | 0                                    |\n| updated_at                  | None                                 |\n| client_ca_tls_container_ref | None                                 |\n| client_authentication       | NONE                                 |\n| client_crl_container_ref    | None                                 |\n| allowed_cidrs               | None                                 |\n| tls_ciphers                 | None                                 |\n| tls_versions                | None                                 |\n| alpn_protocols              | None                                 |\n| tags                        |                                      |\n+-----------------------------+--------------------------------------+\n</code></pre> <p>To check the <code>provisioning_status</code>, type the following:</p> <pre><code>openstack loadbalancer listener show mylb-listener -c provisioning_status\n</code></pre> <pre><code>+---------------------+--------+\n| Field               | Value  |\n+---------------------+--------+\n| provisioning_status | ACTIVE |\n+---------------------+--------+\n</code></pre> <p>Since you wanted to create a TCP load balancer, in the command above, you specified the connection protocol via the <code>--protocol</code> parameter. Also, the new load balancer will be redirecting connections to a couple of servers listening on port 61234, so it makes sense to set the listening port to 61234 (see the <code>--protocol-port</code> parameter).</p>"},{"location":"howto/openstack/octavia/lbaas-tcp/#creating-a-pool","title":"Creating a pool","text":"<p>Any server accepting connections from the listener is said to be a member of a pool. For our load balancer to work, we must create a pool and explicitly list its members.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>With the detailed view of the load balancer expanded, click the Pools tab. You will notice the message \u201cNo pools for this LoadBalancer\u201d, so click the green Create a Pool button.</p> <p></p> <p>A new pane named Create a Pool slides over. First, type in a name for the new pool. For the Algorithm, make sure to select ROUND_ROBIN. Since you are configuring a TCP load balancer, set the Protocol to TCP.</p> <p></p> <p>A little bit further down the pane, there is the Listener dropdown menu; select the one you created in the previous step. Leave the Session persistence parameter as it is, and create your pool with a click on the green Create button.</p> <p></p> <p>The pool is instantaneously available; as you can see, it has zero members and one listener (the one you created in the previous step).</p> <p></p> <p>Notice that the listener already knows about the new pool, even though you did not explicitly mention it.</p> <p></p> <p>To create a pool named <code>mylb-pool</code> that distributes incoming TCP connections to its members in a round-robin fashion from the listener named <code>mylb-listener</code>, type this command:</p> <pre><code>openstack loadbalancer pool create \\\n    --name mylb-pool --lb-algorithm ROUND_ROBIN \\\n    --listener mylb-listener --protocol TCP\n</code></pre> <pre><code>+----------------------+--------------------------------------+\n| Field                | Value                                |\n+----------------------+--------------------------------------+\n| admin_state_up       | True                                 |\n| created_at           | 2023-01-16T19:52:02                  |\n| description          |                                      |\n| healthmonitor_id     |                                      |\n| id                   | e5b9c18d-7c0b-4b95-9dc8-7a778aeb039c |\n| lb_algorithm         | ROUND_ROBIN                          |\n| listeners            | 8b4e3309-c460-44a8-a0af-8dd2868138f9 |\n| loadbalancers        | 6f04228a-19ca-4e94-8965-10f146a9ad81 |\n| members              |                                      |\n| name                 | mylb-pool                            |\n| operating_status     | OFFLINE                              |\n| project_id           | dfc700467396428bacba4376e72cc3e9     |\n| protocol             | TCP                                  |\n| provisioning_status  | PENDING_CREATE                       |\n| session_persistence  | None                                 |\n| updated_at           | None                                 |\n| tls_container_ref    | None                                 |\n| ca_tls_container_ref | None                                 |\n| crl_container_ref    | None                                 |\n| tls_enabled          | False                                |\n| tls_ciphers          | None                                 |\n| tls_versions         | None                                 |\n| tags                 |                                      |\n| alpn_protocols       | None                                 |\n+----------------------+--------------------------------------+\n</code></pre> <p>Once more, you may check the <code>provisioning_status</code> by typing the following:</p> <pre><code>openstack loadbalancer pool show mylb-pool -c provisioning_status\n</code></pre> <pre><code>+---------------------+--------+\n| Field               | Value  |\n+---------------------+--------+\n| provisioning_status | ACTIVE |\n+---------------------+--------+\n</code></pre>"},{"location":"howto/openstack/octavia/lbaas-tcp/#adding-members-to-the-pool","title":"Adding members to the pool","text":"<p>The pool you created has no members, so it is time to populate it.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>In the detailed view of your load balancer, go to the Pools tab and click the bulleted-list icon.</p> <p></p> <p>A new pane titled Modify Pool Members appears, listing all the servers that share the same region with the load balancer. In our example, there are three servers. We created <code>srv-lbaas-1</code> and <code>srv-lbaas-2</code> to test the load balancer, so now we click on the corresponding plus-sign icons to add those to the pool.</p> <p></p> <p>You may have noticed that the listening port of each server we added is by default 80, but we want port 61234. To change the listening port of a server, just click the corresponding notepad-and-pen icon.</p> <p></p> <p>To confirm the changes, click the green Update button.</p> <p></p> <p>At this point, the load balancer should have its listener plus a pool with two members.</p> <p></p> <p>Both our test servers are in the <code>subnet-fra1</code> subnet, one of them has IP <code>10.15.25.105</code>, and the other one has IP <code>10.15.25.236</code>. To add those two servers in pool <code>mylb-pool</code>, type:</p> <pre><code>openstack loadbalancer member create \\\n    --subnet-id subnet-fra1 --address 10.15.25.105 \\\n    --protocol-port 61234 mylb-pool\n</code></pre> <pre><code>+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| address             | 10.15.25.105                         |\n| admin_state_up      | True                                 |\n| created_at          | 2023-01-16T20:11:08                  |\n| id                  | 93aed0c8-b425-40b5-b166-f3e44ad9a986 |\n| name                |                                      |\n| operating_status    | NO_MONITOR                           |\n| project_id          | dfc700467396428bacba4376e72cc3e9     |\n| protocol_port       | 61234                                |\n| provisioning_status | PENDING_CREATE                       |\n| subnet_id           | df6fb6ca-4751-4b74-8b3e-5fbda0117cea |\n| updated_at          | None                                 |\n| weight              | 1                                    |\n| monitor_port        | None                                 |\n| monitor_address     | None                                 |\n| backup              | False                                |\n| tags                |                                      |\n+---------------------+--------------------------------------+\n</code></pre> <pre><code>openstack loadbalancer member create \\\n    --subnet-id subnet-fra1 --address 10.15.25.236 \\\n    --protocol-port 61234 mylb-pool\n</code></pre> <pre><code>+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| address             | 10.15.25.236                         |\n| admin_state_up      | True                                 |\n| created_at          | 2023-01-16T20:11:46                  |\n| id                  | d15527ce-a293-4184-9da5-dcc5f7352a0c |\n| name                |                                      |\n| operating_status    | NO_MONITOR                           |\n| project_id          | dfc700467396428bacba4376e72cc3e9     |\n| protocol_port       | 61234                                |\n| provisioning_status | PENDING_CREATE                       |\n| subnet_id           | df6fb6ca-4751-4b74-8b3e-5fbda0117cea |\n| updated_at          | None                                 |\n| weight              | 1                                    |\n| monitor_port        | None                                 |\n| monitor_address     | None                                 |\n| backup              | False                                |\n| tags                |                                      |\n+---------------------+--------------------------------------+\n</code></pre> <p>You may at any time list all members of a particular pool:</p> <pre><code>openstack loadbalancer member list mylb-pool\n</code></pre> <pre><code>+------------+------+------------+---------------------+------------+---------------+------------------+--------+\n| id         | name | project_id | provisioning_status | address    | protocol_port | operating_status | weight |\n+------------+------+------------+---------------------+------------+---------------+------------------+--------+\n| 93aed0c8-  |      | dfc7004673 | ACTIVE              | 10.15.25.1 |         61234 | NO_MONITOR       |      1 |\n| b425-40b5- |      | 96428bacba |                     | 05         |               |                  |        |\n| b166-      |      | 4376e72cc3 |                     |            |               |                  |        |\n| f3e44ad9a9 |      | e9         |                     |            |               |                  |        |\n| 86         |      |            |                     |            |               |                  |        |\n| d15527ce-  |      | dfc7004673 | ACTIVE              | 10.15.25.2 |         61234 | NO_MONITOR       |      1 |\n| a293-4184- |      | 96428bacba |                     | 36         |               |                  |        |\n| 9da5-      |      | 4376e72cc3 |                     |            |               |                  |        |\n| dcc5f7352a |      | e9         |                     |            |               |                  |        |\n| 0c         |      |            |                     |            |               |                  |        |\n+------------+------+------------+---------------------+------------+---------------+------------------+--------+\n</code></pre>"},{"location":"howto/openstack/octavia/lbaas-tcp/#assigning-a-floating-ip","title":"Assigning a floating IP","text":"<p>Your load balancer has an internal IP chosen from the subnet you indicated earlier. This makes it reachable from other servers in the same subnet but not from the Internet. You probably want the load balancer to be reachable from anywhere, meaning you have to equip it with a floating IP.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>While viewing your load balancer, click the three-dot icon on the right. From the pop-up menu that appears, select Modify Load Balancer.</p> <p></p> <p>The Modify Load Balancer pane appears. For the Floating IP option, select Create and attach IP.</p> <p></p> <p>A new pane slides over, named Create a Floating IP. For the Region parameter, select the one the load balancer resides in. For the Assign to parameter, make sure you choose Load Balancer. In our example, there\u2019s only one load balancer in the region we are working in, so the parameter Assign To is already set for us. To finalize the assignment, click the green Create and Assign button.</p> <p></p> <p>As you may see, your load balancer now has a floating IP. You can now use that to test the balancer and make sure it works as expected.</p> <p></p> <p>First, create a new floating IP:</p> <pre><code>openstack floating ip create ext-net\n</code></pre> <pre><code>+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| created_at          | 2023-01-16T20:25:25Z                 |\n| description         |                                      |\n| dns_domain          | None                                 |\n| dns_name            | None                                 |\n| fixed_ip_address    | None                                 |\n| floating_ip_address | 198.51.100.129                       |\n| floating_network_id | 2aec7a99-3783-4e2a-bd2b-bbe4fef97d1c |\n| id                  | 69457661-197e-444a-b9ce-54394263dafa |\n| name                | 198.51.100.129                       |\n| port_details        | None                                 |\n| port_id             | None                                 |\n| project_id          | dfc700467396428bacba4376e72cc3e9     |\n| qos_policy_id       | None                                 |\n| revision_number     | 0                                    |\n| router_id           | None                                 |\n| status              | DOWN                                 |\n| subnet_id           | None                                 |\n| tags                | []                                   |\n| updated_at          | 2023-01-16T20:25:25Z                 |\n+---------------------+--------------------------------------+\n</code></pre> <p>Then, load the ID of the new floating IP to a variable:</p> <pre><code>FLOAT_IP_ID=$(openstack floating ip list \\\n    --floating-ip-address 198.51.100.129 -c ID -f value)\n</code></pre> <p>Do the same for the VIP port ID of your load balancer:</p> <pre><code>VIP_PORT_ID=$(openstack loadbalancer show mylb \\\n    -c vip_port_id -f value)\n</code></pre> <p>Finally, assign the floating IP to your load balancer like so:</p> <pre><code>openstack floating ip set --port $VIP_PORT_ID $FLOAT_IP_ID\n</code></pre>"},{"location":"howto/openstack/octavia/lbaas-tcp/#testing-the-load-balancer","title":"Testing the load balancer","text":"<p>Each of the two test servers runs <code>ncat</code> to continuously listen on port 61234/TCP and respond to clients with a simple message, revealing its hostname:</p> <pre><code>ncat -kv -l 61234 -c 'echo Yello from $(hostname)!'\n</code></pre> <p>Since the load balancer has a floating IP (<code>198.51.100.129</code>, in our example), and we know the port it listens to, we can try connecting to it via <code>wget</code> and see what happens:</p> <pre><code>wget -q 198.51.100.129:61234 -O -\n</code></pre> <pre><code>Yello from srv-lbaas-1!\n</code></pre> <p>It looks like we talked to the first test server. If we run the exact same <code>wget</code> command for a second time, since the load balancer distributes client connection requests in a round-robin fashion, we expect to talk to the second server:</p> <pre><code>wget -q 198.51.100.129:61234 -O -\n</code></pre> <pre><code>Yello from srv-lbaas-2!\n</code></pre> <p>During our testing, executing the <code>wget</code> command repeatedly, we were getting those two hostnames one after the other \u2014 and that was proof our TCP load balancer was working as expected. But there is one more expectation of any load balancer: the ability to skip backend hosts when they are inaccessible.</p> <p>To test our load balancer in this new scenario, let us first use <code>wget</code> to connect to port 61234 and jot down the backend server that will respond:</p> <pre><code>wget -q 198.51.100.129:61234 -O -\n</code></pre> <pre><code>Yello from srv-lbaas-2!\n</code></pre> <p>We see that <code>srv-lbaas-2</code> responded. That means the next time we try to connect, <code>srv-lbaas-1</code> will respond. But if we SSH into <code>srv-lbaas-1</code> and terminate <code>ncat</code>, then after connecting with <code>wget</code> we will get a response from <code>srv-lbaas-2</code> \u2014 again. This, at least, is our expectation. So without further ado, we SSH into <code>srv-lbaas-1</code>, we terminate <code>ncat</code>, we log out, and from our local terminal, we type:</p> <pre><code>wget -q 198.51.100.129:61234 -O -\n</code></pre> <pre><code>Yello from srv-lbaas-2!\n</code></pre> <p>Exactly as we expected, <code>srv-lbaas-2</code> has responded again, proving our load balancer behaves correctly. As an exercise, you might want to shutdown your <code>srv-lbaas-1</code> and/or re-enable <code>ncat</code>, and check the behavior of the load balancer. There should be no surprises.</p>"},{"location":"howto/openstack/octavia/lbaas-tcp/#adding-a-health-monitor","title":"Adding a health monitor","text":"<p>You can add a health monitor to the pool of your load balancer, so whenever \u2014and for whatever reason\u2014 one or more of the pool member services are inaccessible, the load balancer will know and won\u2019t even bother trying to redirect client requests to them. Of course, whenever an inaccessible service gets accessible again, the load balancer will take notice and start treating the corresponding pool member as a fully functional server.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>In the detailed view of your load balancer, pull up the Pools tab. In the Health Monitor column there is a \u201c0\u201d, for the pool has no health monitor yet.</p> <p></p> <p>To add a health monitor, click the notepad-and-pen icon. A pane titled Modify Pool slides over. Scroll down if you have to, and stop when the Health Monitor section is fully visible. You will see a message saying, \u201cNo health monitor created\u201d, so click the green Create a Healthmonitor button to create one.</p> <p></p> <p>Pick a name for the new health monitor, and set its type to TCP. That is the connection protocol that will be used to determine whether pool member services are accessible. Finalize your choices with a click on the green Create button.</p> <p></p> <p>You will then see that in the Health Monitor column there is a \u201c1\u201d \u2014 and that means the monitor is active. To see real-time information regarding pool members operating status, click over \u201c1\u201d.</p> <p></p> <p>To create a health monitor for pool <code>mylb-pool</code> of load balancer <code>mylb</code>, type something like the following:</p> <pre><code>openstack loadbalancer healthmonitor create \\\n    --name=mylb-pool-healthmon --type=TCP \\\n    --delay=10 --timeout=5 --max-retries=1 \\\n    mylb-pool\n</code></pre> <pre><code>+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| project_id          | dfc700467396428bacba4376e72cc3e9     |\n| name                | mylb-pool-healthmon                  |\n| admin_state_up      | True                                 |\n| pools               | e5b9c18d-7c0b-4b95-9dc8-7a778aeb039c |\n| created_at          | 2023-01-22T20:22:05                  |\n| provisioning_status | PENDING_CREATE                       |\n| updated_at          | None                                 |\n| delay               | 10                                   |\n| expected_codes      | None                                 |\n| max_retries         | 1                                    |\n| http_method         | None                                 |\n| timeout             | 5                                    |\n| max_retries_down    | 3                                    |\n| url_path            | None                                 |\n| type                | TCP                                  |\n| id                  | 8e720877-969d-4653-9e7e-1ae2df66a0b6 |\n| operating_status    | OFFLINE                              |\n| http_version        | None                                 |\n| domain_name         | None                                 |\n| tags                |                                      |\n+---------------------+--------------------------------------+\n</code></pre> <p>The name of the new health monitor is <code>mylb-pool-healthmon</code>, and the TCP protocol will be used to check whether members of pool <code>mylb-pool</code> are online or offline. To check the provisioning status of the health monitor, type:</p> <pre><code>openstack loadbalancer healthmonitor show \\\n    mylb-pool-healthmon -c provisioning_status\n</code></pre> <pre><code>+---------------------+--------+\n| Field               | Value  |\n+---------------------+--------+\n| provisioning_status | ACTIVE |\n+---------------------+--------+\n</code></pre> <p>Once the health monitor is provisioned, you may at any time check the pool members status like so:</p> <pre><code>openstack loadbalancer member list mylb-pool\n</code></pre> <pre><code>+-----------+-----------+------------+---------------------+-----------+---------------+------------------+--------+\n| id        | name      | project_id | provisioning_status | address   | protocol_port | operating_status | weight |\n+-----------+-----------+------------+---------------------+-----------+---------------+------------------+--------+\n| c4351167- | srv-      | dfc7004673 | ACTIVE              | 10.15.25. |         61234 | ONLINE           |      1 |\n| fcfa-     | lbaas-1   | 96428bacba |                     | 105       |               |                  |        |\n| 48fb-     |           | 4376e72cc3 |                     |           |               |                  |        |\n| 82ac-     |           | e9         |                     |           |               |                  |        |\n| 76ddfdc73 |           |            |                     |           |               |                  |        |\n| ca2       |           |            |                     |           |               |                  |        |\n| 04a32731- | srv-      | dfc7004673 | ACTIVE              | 10.15.25. |         61234 | ONLINE           |      1 |\n| f3a6-     | lbaas-2   | 96428bacba |                     | 236       |               |                  |        |\n| 41ca-     |           | 4376e72cc3 |                     |           |               |                  |        |\n| a638-     |           | e9         |                     |           |               |                  |        |\n| 6f8767437 |           |            |                     |           |               |                  |        |\n| 50c       |           |            |                     |           |               |                  |        |\n+-----------+-----------+------------+---------------------+-----------+---------------+------------------+--------+\n</code></pre>"},{"location":"howto/openstack/octavia/lbaas-tcp/#observing-and-testing-the-monitor","title":"Observing and testing the monitor","text":"<p>Having a health monitor for your load balancer\u2019s pool up and running, whenever the service of interest in any of the pool members gets inaccessible, the monitor notices and immediately considers the corresponding member as offline. From then on, and until the same service gets accessible again, the load balancer does not even try to redirect client connections to the affected member. During our testing, we killed <code>ncat</code> running on <code>srv-lbaas-1</code>, and then took a look at the health monitor from the Cleura\u00a0Cloud Management\u00a0Panel \u2014 and also from a local terminal.</p> Cleura\u00a0Cloud Management\u00a0PanelOpenStack CLI <p>In the detailed view of your load balancer, pull up the Pools tab and click anywhere on its line. After a second or two, you will see information regarding all pool members. Pay attention to the Operating Status column, where you can see the status of any of the pool members.</p> <p></p> <p>To check the operating status of any of the pool members of your load balancer, type something like this:</p> <pre><code>openstack loadbalancer member list mylb-pool\n</code></pre> <pre><code>+-----------+-----------+------------+---------------------+-----------+---------------+------------------+--------+\n| id        | name      | project_id | provisioning_status | address   | protocol_port | operating_status | weight |\n+-----------+-----------+------------+---------------------+-----------+---------------+------------------+--------+\n| c4351167- | srv-      | dfc7004673 | ACTIVE              | 10.15.25. |         61234 | ERROR            |      1 |\n| fcfa-     | lbaas-1   | 96428bacba |                     | 105       |               |                  |        |\n| 48fb-     |           | 4376e72cc3 |                     |           |               |                  |        |\n| 82ac-     |           | e9         |                     |           |               |                  |        |\n| 76ddfdc73 |           |            |                     |           |               |                  |        |\n| ca2       |           |            |                     |           |               |                  |        |\n| 04a32731- | srv-      | dfc7004673 | ACTIVE              | 10.15.25. |         61234 | ONLINE           |      1 |\n| f3a6-     | lbaas-2   | 96428bacba |                     | 236       |               |                  |        |\n| 41ca-     |           | 4376e72cc3 |                     |           |               |                  |        |\n| a638-     |           | e9         |                     |           |               |                  |        |\n| 6f8767437 |           |            |                     |           |               |                  |        |\n| 50c       |           |            |                     |           |               |                  |        |\n+-----------+-----------+------------+---------------------+-----------+---------------+------------------+--------+\n</code></pre> <p>In the example above, see the <code>operating_status</code> column, specifically the status of server <code>srv-lbaas-1</code>. Regarding the operating status of all pool members, you can always limit the scope of your query like so:</p> <pre><code>openstack loadbalancer member list mylb-pool \\\n    -c name -c operating_status\n</code></pre> <pre><code>+-------------+------------------+\n| name        | operating_status |\n+-------------+------------------+\n| srv-lbaas-1 | ERROR            |\n| srv-lbaas-2 | ONLINE           |\n+-------------+------------------+\n</code></pre> <p>Finally, let us see what happens when we repeatedly try to access the remote service via the load balancer:</p> <pre><code>$ wget -q 198.51.100.129:61234 -O -\nYello from srv-lbaas-2!\n\n$ wget -q 198.51.100.129:61234 -O -\nYello from srv-lbaas-2!\n\n$ wget -q 198.51.100.129:61234 -O -\nYello from srv-lbaas-2!\n</code></pre> <p>Since pool member <code>srv-lbaas-1</code> is offline to the health monitor, we keep getting an answer from <code>srv-lbaas-2</code>. Actually, while the service of interest in <code>srv-lbaas-1</code> is inaccessible, then no matter how many times we run the command above we will keep getting a response from <code>srv-lbaas-2</code> only.</p>"},{"location":"howto/openstack/octavia/metrics/","title":"Enabling load balancer metrics","text":"<p>This feature is available in select Cleura\u00a0Cloud regions. Please check the feature support matrix for details.</p> <p>Once you have created load balancers in Cleura\u00a0Cloud, you can add a special listener that exposes load balancer metrics. This listener is meant to be used as a data source for an existing Prometheus monitoring system.</p>"},{"location":"howto/openstack/octavia/metrics/#prerequisites","title":"Prerequisites","text":"<p>The Cleura\u00a0Cloud Management\u00a0Panel does not support defining metrics endpoints, so you will have to work with the OpenStack CLI. Enable it for the region you will be working in, and make sure you have the Python <code>octaviaclient</code> package installed. For that, use either the package manager of your operating system, or <code>pip</code>:</p> Debian/UbuntuMac OS X with HomebrewPython Package <pre><code>apt install python3-octaviaclient\n</code></pre> <p>This particular Python module is unavailable via <code>brew</code>, but you can install it via <code>pip</code>.</p> <pre><code>pip install python-octaviaclient\n</code></pre>"},{"location":"howto/openstack/octavia/metrics/#assumptions-and-scenario","title":"Assumptions and scenario","text":"<p>We assume you already have an HTTPS-terminated load balancer that forwards client requests to a back-end server pool. In our test scenario, the load balancer was accepting HTTPS requests for <code>whoogle.example.com</code> and forwarding them to a two-member pool, with servers each running a Docker container for Whoogle Search.</p>"},{"location":"howto/openstack/octavia/metrics/#creating-a-metrics-listener","title":"Creating a metrics listener","text":"<p>Here is <code>mylb</code>, the load balancer we used during testing:</p> <pre><code>$ openstack loadbalancer list\n\n+---------------+------+---------------+--------------+---------------------+------------------+----------+\n| id            | name | project_id    | vip_address  | provisioning_status | operating_status | provider |\n+---------------+------+---------------+--------------+---------------------+------------------+----------+\n| eaf6d4f3-     | mylb | dfc7004673964 | 10.15.25.155 | ACTIVE              | ONLINE           | amphora  |\n| 8d73-4b77-    |      | 28bacba4376e7 |              |                     |                  |          |\n| 8bc4-         |      | 2cc3e9        |              |                     |                  |          |\n| 788a3b4e3916  |      |               |              |                     |                  |          |\n+---------------+------+---------------+--------------+---------------------+------------------+----------+\n</code></pre> <p>You may now add <code>mylb-listener-metrics</code>, a new Prometheus-based listener for <code>mylb</code>.</p> <p>Most likely, you will want to restrict access to this endpoint to just the IP address of your Prometheus server, which presumably lives on a different network and accesses the load balancer via its public (floating) IP address. You can use the <code>--allowed-cidr</code> command-line option for that purpose. The example below<sup>1</sup> assumes that your Prometheus server\u2019s outgoing IP address is <code>203.0.113.132</code>:</p> <pre><code>$ openstack loadbalancer listener create \\\n    --name mylb-listener-metrics \\\n    --protocol PROMETHEUS \\\n    --protocol-port 8088 \\\n    --allowed-cidr \"203.0.113.132/32\" \\\n    mylb\n+-----------------------------+--------------------------------------+\n| Field                       | Value                                |\n+-----------------------------+--------------------------------------+\n| admin_state_up              | True                                 |\n| connection_limit            | -1                                   |\n| created_at                  | 2023-01-29T16:12:24                  |\n| default_pool_id             | None                                 |\n| default_tls_container_ref   | None                                 |\n| description                 |                                      |\n| id                          | c1bba40d-3b45-4b17-86fd-81d98909822e |\n| insert_headers              | None                                 |\n| l7policies                  |                                      |\n| loadbalancers               | eaf6d4f3-8d73-4b77-8bc4-788a3b4e3916 |\n| name                        | mylb-listener-metrics                |\n| operating_status            | OFFLINE                              |\n| project_id                  | dfc700467396428bacba4376e72cc3e9     |\n| protocol                    | PROMETHEUS                           |\n| protocol_port               | 8088                                 |\n| provisioning_status         | PENDING_CREATE                       |\n| sni_container_refs          | []                                   |\n| timeout_client_data         | 50000                                |\n| timeout_member_connect      | 5000                                 |\n| timeout_member_data         | 50000                                |\n| timeout_tcp_inspect         | 0                                    |\n| updated_at                  | None                                 |\n| client_ca_tls_container_ref | None                                 |\n| client_authentication       | NONE                                 |\n| client_crl_container_ref    | None                                 |\n| allowed_cidrs               | ['203.0.113.132/32']                 |\n| tls_ciphers                 | None                                 |\n| tls_versions                | None                                 |\n| alpn_protocols              | None                                 |\n| tags                        |                                      |\n+-----------------------------+--------------------------------------+\n</code></pre> <p>To check the provisioning status of <code>mylb-listener-metrics</code>, type:</p> <pre><code>$ openstack loadbalancer listener show mylb-listener-metrics \\\n  -c provisioning_status\n+---------------------+--------+\n| Field               | Value  |\n+---------------------+--------+\n| provisioning_status | ACTIVE |\n+---------------------+--------+\n</code></pre>"},{"location":"howto/openstack/octavia/metrics/#testing-the-listener","title":"Testing the listener","text":"<p>Once the listener reports as being <code>ACTIVE</code>, you should be able to check its endpoint.</p> <p>From a client that matches your <code>allowed_cidrs</code> filter, you can do so using <code>curl</code>. The example below assumes that your load balancer uses a public (floating) IP of <code>198.51.100.234</code>:</p> <pre><code>$ curl -s http://198.51.100.234:8088/metrics | head\n# HELP octavia_loadbalancer_cpu Load balancer CPU utilization (percentage).\n# TYPE octavia_loadbalancer_cpu gauge\noctavia_loadbalancer_cpu 0.0\n# HELP octavia_loadbalancer_memory Load balancer memory utilization (percentage).\n# TYPE octavia_loadbalancer_memory gauge\noctavia_loadbalancer_memory 37.2\n# HELP octavia_memory_pool_failures_total Total number of failed memory pool allocations.\n# TYPE octavia_memory_pool_failures_total counter\noctavia_memory_pool_failures_total 0\n# HELP octavia_loadbalancer_max_connections Hard limit on the number of per-process connections (configured or imposed by Ulimit-n)\n</code></pre> <p>Overall, the metrics endpoint exports more than 140 different metrics of the <code>gauge</code> and <code>counter</code> types. All exported metrics use the prefix <code>octavia_</code>.</p>"},{"location":"howto/openstack/octavia/metrics/#adding-the-listener-to-your-prometheus-configuration","title":"Adding the listener to your Prometheus configuration","text":"<p>Once you have verified that your listener is functional, you can add it to your Prometheus configuration.</p> <p>Again, this example assumes that your load balancer uses a public (floating) IP of <code>198.51.100.234</code>:</p> <pre><code>[scrape_configs]\n- job_name: 'mylb'\n  static_configs:\n  - targets:\n    - '198.51.100.234:8088'\n</code></pre> <ol> <li> <p>If your <code>openstack</code> CLI does not support the <code>--protocol\u00a0PROMETHEUS</code> option, you may have to upgrade your installed <code>python-octaviaclient</code> package.\u00a0\u21a9</p> </li> </ol>"},{"location":"howto/openstack/octavia/tls-lb/","title":"HTTPS-terminating load balancers","text":"<p>In Cleura\u00a0Cloud\u2019s load balancing service, OpenStack Octavia, you can configure load balancers so that they manage HTTPS termination. That is to say that the load balancer encrypts and decrypts HTTPS traffic, and forwards HTTP to and from a backend web server.</p> <p>To do so, the load balancer must have access to encryption credentials (such as certificates and private keys), which it stores in Barbican.</p>"},{"location":"howto/openstack/octavia/tls-lb/#pkcs-12-certificate-bundles","title":"PKCS #12 Certificate Bundles","text":"<p>The PKCS #12 archive format includes SSL certificates, certificate chains, and private keys all in one bundle. Most certificate providers give you the option of downloading certificate credentials using the PKCS #12 format.</p> <p>In case your certificate provider has made your certificate chain and key available separately, using the PEM format, you can easily convert it to PKCS #12 using the following <code>openssl</code> command:</p> <pre><code>openssl pkcs12 -export -inkey key.pem -in fullchain.pem -out bundle.p12\n</code></pre> <p>When prompted for an export password, use a blank one.</p>"},{"location":"howto/openstack/octavia/tls-lb/#creating-barbican-secrets-from-pkcs-12-bundles","title":"Creating Barbican secrets from PKCS #12 bundles","text":"<p>To create a secret from a stored PKCS #12 bundle, you need pass in the contents of the bundle, pre-encoded with Base64, as the secret\u2019s payload.</p> <pre><code>$ openstack secret store \\\n  --name='tls_secret1' \\\n  -t 'application/octet-stream' \\\n  -e 'base64' \\\n  --payload=\"$(base64 &lt; server.p12)\"\n+---------------+---------------------------------------------------------------------------------+\n| Field         | Value                                                                           |\n+---------------+---------------------------------------------------------------------------------+\n| Secret href   | https://kna1.citycloud.com:9311/v1/secrets/69bd82f5-60c9-4764-99ec-7a3dff05d2aa |\n| Name          | tls_secret1                                                                     |\n| Created       | None                                                                            |\n| Status        | None                                                                            |\n| Content types | {'default': 'application/octet-stream'}                                         |\n| Algorithm     | aes                                                                             |\n| Bit length    | 256                                                                             |\n| Secret type   | opaque                                                                          |\n| Mode          | cbc                                                                             |\n| Expiration    | None                                                                            |\n+---------------+---------------------------------------------------------------------------------+\n</code></pre>"},{"location":"howto/openstack/octavia/tls-lb/#creating-https-enabled-load-balancer-listeners","title":"Creating HTTPS-enabled load balancer listeners","text":"<p>Once you have created your secret containing your certificate data, you can create a load balancer listener with the following properties:</p> <ul> <li>It uses the <code>TERMINATED_HTTPS</code> protocol,</li> <li>It sets its \u201cdefault TLS container\u201d to the Barbican secret   containing the PKCS #12 bundle,</li> <li>It listens on the standard HTTPS port, 443.</li> </ul> <p>You create such a listener with the following command:</p> <pre><code>$ openstack loadbalancer listener create \\\n  --protocol-port 443 \\\n  --protocol TERMINATED_HTTPS \\\n  --name listener1 \\\n  --default-tls-container-ref=https://kna1.citycloud.com:9311/v1/secrets/dacfbec1-fbed-403f-a4dc-303e28942dae  \\\n  &lt;loadbalancer-name-or-id&gt;\n+-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Field                       | Value                                                                                                                                                                                                                                                                              |\n+-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| admin_state_up              | True                                                                                                                                                                                                                                                                               |\n| connection_limit            | -1                                                                                                                                                                                                                                                                                 |\n| created_at                  | 2021-01-20T11:51:46                                                                                                                                                                                                                                                                |\n| default_pool_id             | None                                                                                                                                                                                                                                                                               |\n| default_tls_container_ref   | https://kna1.citycloud.com:9311/v1/secrets/dacfbec1-fbed-403f-a4dc-303e28942dae                                                                                                                                                                                                    |\n| description                 |                                                                                                                                                                                                                                                                                    |\n| id                          | 4ec6b23d-d08a-4de0-9e12-54ac690ee1ec                                                                                                                                                                                                                                               |\n| insert_headers              | None                                                                                                                                                                                                                                                                               |\n| l7policies                  |                                                                                                                                                                                                                                                                                    |\n| loadbalancers               | 2c2a0760-c3a8-48d2-bdd0-288c3d33a43f                                                                                                                                                                                                                                               |\n| name                        | listener1                                                                                                                                                                                                                                                                          |\n| operating_status            | OFFLINE                                                                                                                                                                                                                                                                            |\n| project_id                  | 4a9484063d4c40d29301ad745c0e2c69                                                                                                                                                                                                                                                   |\n| protocol                    | TERMINATED_HTTPS                                                                                                                                                                                                                                                                   |\n| protocol_port               | 443                                                                                                                                                                                                                                                                                |\n| provisioning_status         | PENDING_CREATE                                                                                                                                                                                                                                                                     |\n| sni_container_refs          | []                                                                                                                                                                                                                                                                                 |\n| timeout_client_data         | 50000                                                                                                                                                                                                                                                                              |\n| timeout_member_connect      | 5000                                                                                                                                                                                                                                                                               |\n| timeout_member_data         | 50000                                                                                                                                                                                                                                                                              |\n| timeout_tcp_inspect         | 0                                                                                                                                                                                                                                                                                  |\n| updated_at                  | None                                                                                                                                                                                                                                                                               |\n| client_ca_tls_container_ref | None                                                                                                                                                                                                                                                                               |\n| client_authentication       | NONE                                                                                                                                                                                                                                                                               |\n| client_crl_container_ref    | None                                                                                                                                                                                                                                                                               |\n| allowed_cidrs               | None                                                                                                                                                                                                                                                                               |\n| tls_ciphers                 | TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:TLS_AES_128_GCM_SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256 |\n| tls_versions                |                                                                                                                                                                                                                                                                                    |\n+-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"howto/openstack/octavia/tls-lb/#updating-the-tls-certificate-for-a-https-listener","title":"Updating the TLS certificate for a HTTPS listener","text":"<p>When the certificate associated with a <code>TERMINATED_HTTPS</code> listener is about to expire, you will need to replace it. You can do this online, with no user-noticeable interruption to your service.</p> <ol> <li>Create a new PKCS#12 bundle from    the updated key, certificate, and CA certificate.</li> <li>Create a new Barbican    secret from the    bundle.</li> <li>List the listener(s) associated with your load balancer:    <pre><code>openstack loadbalancer listener list \\\n  --loadbalancer &lt;loadbalancer-name-or-id&gt;\n</code></pre></li> <li>For all listeners using the <code>TERMINATED_HTTPS</code> protocol, run the    following command:    <pre><code>openstack loadbalancer listener set \\\n  --default-tls-container-ref=https://kna1.citycloud.com:9311/v1/secrets/e2d8acc1-c6b9-4c01-9373-cc167b075c25  \\\n  &lt;listener-name-or-id&gt;\n</code></pre></li> </ol> <p>Once all your load balancer listeners have completed the update, you may proceed to delete the old, now-unused secret:</p> <pre><code>openstack secret delete \\\n  https://kna1.citycloud.com:9311/v1/secrets/dacfbec1-fbed-403f-a4dc-303e28942dae\n</code></pre>"},{"location":"howto/support/raise-issues/","title":"Raising support issues","text":"<p>The Service\u00a0Center portal is designed for customers with a paid support package tier, typically organizations. Using this portal, you can create, manage, and track issue tickets.</p> <p></p> <p>If you are not subscribed to a paid support package plan, you can still reach out to us via email to raise tickets.</p>"},{"location":"howto/support/raise-issues/#issue-properties","title":"Issue properties","text":""},{"location":"howto/support/raise-issues/#issue-type","title":"Issue type","text":"<p>Depending on your selections while filling out a request type, additional fields may be dynamically displayed to gather relevant information.</p>"},{"location":"howto/support/raise-issues/#requests","title":"Requests","text":"<p>Requests are used for general inquiries, access management, or feedback. Available options include:</p>"},{"location":"howto/support/raise-issues/#general-requests","title":"General Requests","text":"<ul> <li>Request for information</li> <li>General</li> <li>Invoice</li> <li>Cleura\u00a0Cloud Management\u00a0Panel (CCMP)</li> <li>Quota</li> <li>Prior Incident</li> <li>Data Center (DC) Access</li> <li>Status Page</li> <li>Sales</li> <li>Problem</li> </ul>"},{"location":"howto/support/raise-issues/#staff-requests","title":"Staff Requests","text":"<ul> <li> <p>Onboarding:</p> <ul> <li>Status Page (for Cleura\u00a0Compliant\u00a0Cloud customers)</li> <li>Service\u00a0Center portal</li> </ul> </li> <li> <p>Offboarding:</p> <ul> <li>Status Page (for Cleura\u00a0Compliant\u00a0Cloud customers)</li> <li>Service\u00a0Center portal</li> </ul> </li> </ul>"},{"location":"howto/support/raise-issues/#feedback-proposals","title":"Feedback &amp; Proposals","text":"<ul> <li>Feedback</li> <li>Improvement Proposals</li> </ul>"},{"location":"howto/support/raise-issues/#issues","title":"Issues","text":"<p>Issues are used to report ongoing problems. Available option:</p> <ul> <li>Incident:<ul> <li>Report ongoing incidents that disrupt normal operations.</li> </ul> </li> </ul>"},{"location":"howto/support/raise-issues/#managed-services","title":"Managed Services","text":"<p>Managed Services tickets relate to hosted services or backup operations. Available options:</p> <ul> <li> <p>Hosting:</p> <ul> <li>Canceling</li> </ul> </li> <li> <p>File Backup:</p> <ul> <li>Restoring</li> <li>Canceling</li> </ul> </li> </ul>"},{"location":"howto/support/raise-issues/#priority-matrix","title":"Priority matrix","text":"<p>We use a Priority matrix to prioritize tickets effectively. This allows us to evaluate Urgency and Impact, ensuring issues are resolved efficiently and fairly.</p> <p>When creating a ticket, you will be asked to assign a Priority based on the following matrix:</p> High Impact Medium Impact Low Impact High Urgency P1 P2 P3 Medium Urgency P2 P2 P3 Low Urgency P3 P3 P4"},{"location":"howto/support/raise-issues/#raising-tickets-via-the-service-center-portal","title":"Raising tickets via the Service\u00a0Center portal","text":"<p>The Service\u00a0Center portal organizes requests into a single menu where all request types are available. The interface dynamically displays fields depending on the selections you make.</p>"},{"location":"howto/support/raise-issues/#steps-to-create-a-ticket","title":"Steps to create a ticket","text":"<ol> <li>Visit the Service\u00a0Center portal.</li> <li>Log in if prompted.  </li> <li>Navigate to the relevant service project.<ul> <li>A user can have access to several service projects. You should select the project that relates to your request.</li> </ul> </li> <li>Select the Issue type that best matches your request.</li> <li>Fill in the required fields and any additional fields that appear dynamically based on your selections.</li> <li>If you\u2019re part of an organization and want the ticket visible to others, select Share with [organization name] at the bottom of the form. Otherwise, choose Private request.  </li> <li>Click Create to submit your ticket.</li> </ol> <p>Tip: Use the search bar at the top of the portal page to quickly find your tickets or search our knowledge base for helpful articles.</p>"},{"location":"howto/support/raise-issues/#raising-tickets-via-email","title":"Raising tickets via email","text":"<p>If you choose to raise a ticket by email, please follow these guidelines:</p> <ol> <li>Email address: Send your email to support@cleura.com from an account matching your profile information.</li> <li>Language: Use English for quicker responses. While some staff speak other languages, using languages other than English may delay responses.</li> <li>Subject: Provide a clear and descriptive subject line, e.g., \u201cRequest to increase project quota\u201d or \u201cUnable to log in to Cleura\u00a0Cloud Management\u00a0Panel.\u201d</li> <li>Details: Include as much information as possible, especially:<ul> <li>Steps you\u2019ve taken to resolve the issue.</li> <li>The UUID (Universally Unique Identifier) of affected resources, if applicable.</li> </ul> </li> </ol> <p>Note: Customers with a paid support package must use the Service\u00a0Center portal for raising tickets.</p>"},{"location":"howto/support/raise-issues/#ticket-communication","title":"Ticket communication","text":"<p>Once your ticket is created, here\u2019s what to expect:</p> <ol> <li>Confirmation email: You will receive an email confirming your ticket submission. To add more details, reply directly to this email.</li> <li>Support engineer assignment: A Technical Support Engineer will review your case, confirm the details, and take the necessary steps to resolve or fulfill your request.  </li> <li>Additional information requests: Be prepared to provide additional details if requested by the Support Engineer.</li> </ol> <p>Regardless of your support plan, you will receive email notifications for updates to your ticket. You can always reply to these notifications to provide additional information or updates.</p> <p>For customers with a paid support package, you can only use the Service\u00a0Center portal for all communication.</p>"},{"location":"reference/","title":"Reference","text":"<p>This is our reference section. It serves to provide general reference information about Cleura\u00a0Cloud services.</p>"},{"location":"reference/#feature-and-service-availability","title":"Feature and service availability","text":"<ul> <li> <p>Our Feature Support Matrix lists cloud features and their availability across Cleura\u00a0Cloud regions.</p> </li> <li> <p>The Limitations section lists support limitations for specific Cleura\u00a0Cloud services.</p> </li> <li> <p>The Flavors reference explains our naming convention for pre-defined CPU/RAM/disk configurations (\u201cflavors\u201d) for server instances, and their availability across Cleura\u00a0Cloud regions.</p> </li> <li> <p>The Volume Types reference lists the available persistent block storage (\u201cvolume\u201d) types.</p> </li> <li> <p>Our Service Version Matrix lists the open source software versions running in our Cleura\u00a0Cloud regions.</p> </li> </ul>"},{"location":"reference/#api-reference","title":"API reference","text":"<ul> <li>The API reference documentation provides useful information for accessing Application Programming Interfaces (APIs), getting information, and manipulating all sorts of Cleura\u00a0Cloud objects.</li> </ul>"},{"location":"reference/#legal-reference","title":"Legal reference","text":"<ul> <li>The Legal reference section contains information about the terms and agreements governing Cleura\u00a0Cloud services.</li> </ul>"},{"location":"reference/api/","title":"API reference","text":"<p>Services in Cleura\u00a0Cloud are accessible via Application Programming Interfaces (APIs). This section contains pointers to the reference documentation for the supported APIs:</p> <ul> <li>Cleura\u00a0Cloud REST\u00a0API</li> <li>OpenStack API</li> </ul>"},{"location":"reference/api/cc/","title":"Cleura Cloud REST API reference documentation","text":"<p>You may at any time make good use of the Cleura\u00a0Cloud REST\u00a0API with a tool like <code>curl</code>. Regarding documentation, there is a detailed, automatically generated page, which you turn to for advice on endpoints and how to use them.</p> <p>To access the API, you need to have an account in Cleura\u00a0Cloud, and also create a valid access token.</p> <p>All actions exposed via the Cleura\u00a0Cloud Management\u00a0Panel are also available through the Cleura\u00a0Cloud REST\u00a0API. One of the most common use cases is taking advantage of it for programmatically pulling all sorts of information for further processing. For instance, see how you can retrieve invoice data via the API. For any other use of the Cleura\u00a0Cloud REST\u00a0API, please see the sections in the documentation page.</p>"},{"location":"reference/api/openstack/","title":"OpenStack API reference documentation","text":"<p>For a starting point on reference information about the OpenStack APIs in Cleura\u00a0Cloud, and how to use them with a tool like <code>curl</code>, refer to the OpenStack API documentation landing page. Individual service APIs have their own detailed API reference documentation pages, such as those for the Compute (Nova) API or the Networking (Neutron) API.</p> <p>You may also be interested in the API Quick Start Guide for information about how to authenticate against the OpenStack API, and send API requests.</p> <p>To access the OpenStack API in Cleura\u00a0Cloud, you need to have an account, and also download a valid credentials file, as you would for enabling the OpenStack CLI. All actions exposed via the OpenStack CLI are also available by calling the API directly.</p>"},{"location":"reference/api/openstack/#openstack-sdks","title":"OpenStack SDKs","text":"<p>Although the OpenStack API is perfectly usable via direct HTTP/HTTPS requests to the API endpoints, most developers prefer to use one of the Software Development Kits (SDKs) that wrap the OpenStack API. These SDKs are available for many languages:</p> <ul> <li>Python: openstacksdk</li> <li>Go: Gophercloud (see also its reference documentation)</li> <li>.NET: OpenStack.NET</li> <li>PHP: php-opencloud</li> <li>Ruby: fog (see also its OpenStack provider documentation)</li> <li>Java: OpenStack4j</li> <li>JavaScript/Node.js: pkgcloud</li> </ul> <p>If your target language has a supported SDK, it may be advisable to use one of them rather than work with hand-crafted HTTP/HTTPS requests.</p>"},{"location":"reference/features/","title":"Feature support matrix","text":"<p>Our services constantly evolve, and we gradually add features to regions as they become available and mature.</p> <p>This page lists the cloud features available in each Cleura\u00a0Cloud region.</p> <ul> <li> <p>Public Cloud: features supported in our   Cleura\u00a0Public\u00a0Cloud regions.</p> </li> <li> <p>Compliant Cloud: features supported in our   Cleura\u00a0Compliant\u00a0Cloud regions.</p> </li> </ul> <p>Please note that certain support limitations apply to some features.</p>"},{"location":"reference/features/compliant/","title":"Compliant Cloud","text":"<p> Feature is in production and fully supported</p> <p> Feature is in deployment, not yet supported</p> <p> Feature is deprecated, being phased out</p> <p> Feature is not available</p>"},{"location":"reference/features/compliant/#block-storage","title":"Block storage","text":"Sto1HS Sto2HS Highly available storage High-performance local storage Volume encryption"},{"location":"reference/features/compliant/#object-storage","title":"Object storage","text":"Sto1HS Sto2HS S3 API S3 SSE-C S3 object lock Swift API"},{"location":"reference/features/compliant/#networking-layer-23","title":"Networking (Layer 2/3)","text":"Sto1HS Sto2HS IPv4 (with NAT) IPv6 VPN (IPsec with PSK)"},{"location":"reference/features/compliant/#load-balancers","title":"Load Balancers","text":"Sto1HS Sto2HS Transport layer (TCP/UDP) Application layer (HTTP) Application layer (HTTPS, with secrets management for TLS certificates) Metrics endpoint"},{"location":"reference/features/public/","title":"Public Cloud","text":"<p> Feature is in production and fully supported</p> <p> Feature is in deployment, not yet supported</p> <p> Feature is deprecated, being phased out</p> <p> Feature is not available</p>"},{"location":"reference/features/public/#virtualization","title":"Virtualization","text":"Kna1 Sto2 Fra1 Physical CPUs"},{"location":"reference/features/public/#block-storage","title":"Block storage","text":"Kna1 Sto2 Fra1 Highly available storage High-performance local storage Volume encryption"},{"location":"reference/features/public/#object-storage","title":"Object storage","text":"Kna1 Sto2 Fra1 S3 API S3 SSE-C S3 object lock Swift API"},{"location":"reference/features/public/#networking-layer-23","title":"Networking (Layer 2/3)","text":"Kna1 Sto2 Fra1 IPv4 (with NAT) IPv6 VPN (IPsec with PSK)"},{"location":"reference/features/public/#load-balancers","title":"Load Balancers","text":"Kna1 Sto2 Fra1 Transport layer (TCP/UDP) Application layer (HTTP) Application layer (HTTPS, with secrets management for TLS certificates) Metrics endpoint"},{"location":"reference/features/public/#kubernetes-management","title":"Kubernetes management","text":"Kna1 Sto2 Fra1 OpenStack Magnum Gardener"},{"location":"reference/flavors/","title":"Flavors","text":"<p>Any server instance running in Cleura\u00a0Cloud has a flavor, which defines the number of virtual CPU cores, the amount of virtual RAM, and other performance-related factors.</p> <p>Using flavors, you define a server\u2019s performance characteristics and supported features.</p>"},{"location":"reference/flavors/#naming-convention","title":"Naming convention","text":"<p>Flavor names in Cleura\u00a0Cloud follow a convention, which can be summarized as <code>X.YcZgb</code>:</p> <ul> <li><code>X</code> stands for a lowercase letter identifying the compute   tier, with <code>b</code> representing the general-purpose   tier. It is always followed by a full-stop\u00a0(<code>.</code>).</li> <li><code>Y</code> stands for the number of virtual CPU cores. This number is   always followed by the letter <code>c</code>.</li> <li><code>Z</code> stands for the allocated amount of virtual RAM, in   gibibytes. This   number is always followed by the string <code>gb</code>.</li> </ul> <p>For example, the flavor named <code>b.4c32gb</code> would be used for a general-purpose compute instance with 4 cores and 32 GiB RAM.</p>"},{"location":"reference/flavors/#compute-tiers","title":"Compute tiers","text":"<p>Cleura\u00a0Cloud defines the following compute tiers:</p> <ul> <li><code>b</code>: General purpose. This is the default compute tier. Instances   launched with matching flavors use highly available network-attached   storage. This makes them flexible to migrate within the   Cleura\u00a0Cloud infrastructure, without interruption.   Some   limitations   apply to instances with attached encrypted   volumes.</li> <li><code>s</code>: High-performance local storage. Instances   launched with matching flavors use local, directly-attached   storage. This generally provides higher throughput and lower   latency for I/O intensive applications, but instances launched with   these flavors must configure their own high availability and data   replication.</li> <li><code>c</code>: Physical CPUs. Instances launched with matching flavors will be   assigned physical CPU cores, rather then virtual ones. These flavors   are recommended for CPU-intensive workloads, or when there is a   requirement for guaranteed CPU resources.</li> </ul> <p>Some tiers are only available in select Cleura\u00a0Cloud regions. For details on tier availability, see the Feature support matrix.</p> <p>The general-purpose tier is always available to all Cleura\u00a0Cloud customers. For access to other tiers, contact our Service\u00a0Center.</p>"},{"location":"reference/images/","title":"Images","text":"<p>Public Glance images in Cleura\u00a0Cloud contain regularly updated minimal versions of server operating systems.</p> <p>These images also contain the cloud-init package applicable to the operating system, to support the injection of SSH public keys and other user data.</p>"},{"location":"reference/images/#naming-conventions","title":"Naming conventions","text":""},{"location":"reference/images/#image-names","title":"Image names","text":"<p>Image names in Cleura\u00a0Cloud follow a convention, which can be summarized as <code>${NAME}\u00a0${VERSION_ID}\u00a0${CODENAME}\u00a0${ARCH}</code>:</p> <ul> <li><code>NAME</code>: Operating system name, such as <code>Ubuntu</code>, <code>Debian</code>, <code>Rocky</code>, etc.</li> <li><code>VERSION_ID</code>: Operating system version, as in <code>22.04</code>, <code>11</code>, <code>9</code>, etc.</li> <li><code>CODENAME</code>: Operating system codename, if present, like <code>Jammy Jellyfish</code>, <code>Bullseye</code>, etc.</li> <li><code>ARCH</code>: Platform architecture for which the operating system was built, for example: <code>x86_64</code>, <code>aarch64</code>, etc.</li> </ul>"},{"location":"reference/images/#tags-and-properties","title":"Tags and properties","text":"<p>Each public image is assigned a specific set of tags and properties. You can use these tags to filter and list images based on certain conditions. You may also use them to simply examine how an image is configured.</p>"},{"location":"reference/images/#tags","title":"Tags","text":"<p>All public images available in Cleura\u00a0Cloud support the following image tags:</p> <ul> <li><code>os:${NAME}</code>: a short identifier for the operating system, such as <code>os:ubuntu</code>, <code>os:debian</code>, <code>os:rocky</code>, etc.</li> <li><code>os_version:${VERSION_ID}</code>: the operating system version, such as <code>os_version:22.04</code>, <code>os_version:11</code>, <code>os_version:9</code>, etc.</li> </ul>"},{"location":"reference/images/#properties","title":"Properties","text":"<p>All public images available in Cleura\u00a0Cloud support the following image properties:</p> <ul> <li><code>architecture=${ARCH}</code>: Platform architecture, such as <code>architecture=x86_64</code></li> <li><code>os_distro=${NAME}</code>: distribution name, such as <code>os_distro=ubuntu</code></li> <li><code>os_version=${VERSION_ID}</code>: operating system version, such as <code>os_version=22.04</code></li> </ul> <p>Other properties may also be set on individual images. In particular, Cleura\u00a0Cloud aims to set image properties according to the metadata standard defined by the Sovereign Cloud Stack (SCS) initiative.</p>"},{"location":"reference/images/#community-images","title":"Community images","text":"<p>At Cleura\u00a0Cloud, we regularly update and rotate our images to always provide secure public images.</p> <p>During rotation, we change an image\u2019s visibility from <code>public</code> to <code>community</code>, while keeping its image name. This enables tools like Heat or Terraform to pass validation checks without attempting to alter environments.</p> <p>You can retrieve an image\u2019s original build date (for images with both <code>public</code> and <code>community</code> visibility) by checking its <code>build_date</code> tag or <code>image_build_date</code> property.</p> <p>Usage of community images is not recommended and is always upon full responsibility of the user.</p>"},{"location":"reference/legal/","title":"Legal Reference","text":"<p>For legal documentation, including our Terms of Service and Data Processing Agreement, please refer to https://cleura.com/resources/legal/terms/.</p>"},{"location":"reference/limitations/","title":"Limitations","text":"<p>This section lists technical and support limitations in Cleura\u00a0Cloud.</p> <p>You may refer to this section in order to determine whether a specific OpenStack, object storage, or Kubernetes feature is supported in Cleura\u00a0Cloud.</p>"},{"location":"reference/limitations/kubernetes/","title":"Kubernetes service limitations","text":""},{"location":"reference/limitations/kubernetes/#openstack-magnum","title":"OpenStack Magnum","text":""},{"location":"reference/limitations/kubernetes/#container-orchestration-engines","title":"Container Orchestration Engines","text":"<p>In Cleura\u00a0Cloud, Magnum only supports the <code>kubernetes</code> Container Orchestration Engine (COE). The legacy <code>swarm</code> and <code>mesos</code> COEs are not supported.</p>"},{"location":"reference/limitations/kubernetes/#kubernetes-version","title":"Kubernetes version","text":"<p>The latest Kubernetes version you can install in Cleura\u00a0Cloud with OpenStack Magnum is 1.27.</p>"},{"location":"reference/limitations/kubernetes/#ip-version","title":"IP version","text":"<p>In Cleura\u00a0Cloud, you can use OpenStack Magnum to deploy Kubernetes clusters that use either IPv4 or IPv6. Dual-stack clusters or services are not supported.</p>"},{"location":"reference/limitations/kubernetes/#cluster-networking","title":"Cluster networking","text":"<p>The only supported Magnum network driver in Cleura\u00a0Cloud is <code>calico</code>. We do not support the <code>flannel</code> network driver.</p>"},{"location":"reference/limitations/kubernetes/#persistent-volumes-pvs","title":"Persistent volumes (PVs)","text":"<p>In Magnum-managed Kubernetes clusters in Cleura\u00a0Cloud, the only supported PV access mode is <code>ReadWriteOnce</code> (<code>RWO</code>). Note that this still enables multiple Pods to access the same volume, as long as they are configured to run on the same node.</p> <p>You cannot use <code>ReadWriteOncePod</code> (<code>RWOP</code>), <code>ReadWriteMany</code> (<code>RWX</code>), or <code>ReadOnlyMany</code> (<code>ROX</code>) PVs.</p>"},{"location":"reference/limitations/kubernetes/#gardener","title":"Gardener","text":""},{"location":"reference/limitations/kubernetes/#kubernetes-version_1","title":"Kubernetes version","text":"<p>The latest Kubernetes version you can install in Cleura\u00a0Cloud with Gardener is 1.31.</p>"},{"location":"reference/limitations/kubernetes/#ip-version_1","title":"IP version","text":"<p>In Cleura\u00a0Cloud, you can use Gardener to deploy Kubernetes clusters that use IPv4. IPv6 clusters or services (whether single-stack or dual-stack) are not supported.</p>"},{"location":"reference/limitations/kubernetes/#service-annotations","title":"Service annotations","text":"<p>In Gardener-managed clusters, we do not support <code>loadbalancer.openstack.org</code> annotations for Kubernetes services of the <code>LoadBalancer</code> type.</p>"},{"location":"reference/limitations/kubernetes/#persistent-volumes-pvs_1","title":"Persistent volumes (PVs)","text":"<p>In Gardener-managed Kubernetes clusters, the supported PV access modes are <code>ReadWriteOnce</code> (<code>RWO</code>), and <code>ReadWriteOncePod</code> (<code>RWOP</code>). Note that <code>RWO</code> enables multiple Pods to access the same volume, as long as they are configured to run on the same node. Use <code>RWOP</code> to restrict the volume to one pod only.</p> <p>You cannot use <code>ReadWriteMany</code> (<code>RWX</code>) or <code>ReadOnlyMany</code> (<code>ROX</code>) PVs.</p>"},{"location":"reference/limitations/kubernetes/#dynamic-volume-provisioning-storage-classes","title":"Dynamic volume provisioning storage classes","text":"<p>In Cleura\u00a0Cloud, Gardener-managed clusters contain a single storage class named <code>default</code>, which is the supported storage class you should use for all dynamically-provisioned persistent volumes. We do not provide support for any user-defined storage classes.</p>"},{"location":"reference/limitations/object-storage/","title":"Object storage limitations","text":""},{"location":"reference/limitations/object-storage/#swift-api","title":"Swift API","text":""},{"location":"reference/limitations/object-storage/#versioning","title":"Versioning","text":"<p>When using object versioning with the Swift API, only the <code>X-Versions-Location</code> header is supported. In Cleura\u00a0Cloud, there is no support for <code>X-History-Location</code>.</p>"},{"location":"reference/limitations/object-storage/#rlistings-acl-element","title":"<code>.rlistings</code> ACL element","text":"<p>In access control lists (ACLs) set on containers via the Swift API, the <code>.rlistings</code> ACL element has no effect. In a public container (one with one or more <code>.r:</code> elements in its ACL), any client that is able to retrieve objects is also able to list them.</p>"},{"location":"reference/limitations/object-storage/#container-to-container-sync","title":"Container-to-container sync","text":"<p>In Cleura\u00a0Cloud, there is no support for container-to-container synchronization.</p>"},{"location":"reference/limitations/object-storage/#s3-api","title":"S3 API","text":""},{"location":"reference/limitations/object-storage/#bucket-replication","title":"Bucket replication","text":"<p>There is currently no support for S3 bucket replication in Cleura\u00a0Cloud.</p>"},{"location":"reference/limitations/object-storage/#bucket-notifications","title":"Bucket notifications","text":"<p>There is currently no support for S3 bucket notifications in Cleura\u00a0Cloud.</p>"},{"location":"reference/limitations/object-storage/#object-lock-retention-modes","title":"Object lock retention modes","text":"<p>For buckets configured with object lock, the only supported retention mode is <code>COMPLIANCE</code>. We do not support <code>GOVERNANCE</code> mode.</p>"},{"location":"reference/limitations/object-storage/#sse-kms","title":"SSE-KMS","text":"<p>There is currently no support for SSE-KMS in Cleura\u00a0Cloud. SSE-C, in contrast, is fully supported.</p>"},{"location":"reference/limitations/object-storage/#request-checksum-calculation","title":"Request checksum calculation","text":"<p>Due to changes in AWS SDKs in early 2025, uploading objects to Cleura\u00a0Cloud object storage services may fail when using tools that depend on those SDKs. Calls using the previously optional checksum requirement will fail because our backend storage doesn\u2019t recognize these requests.</p> <p>This can be resolved by setting the environment variable <code>AWS_REQUEST_CHECKSUM_CALCULATION</code> to <code>WHEN_REQUIRED</code> or by changing the AWS CLI defaults. Other tools can have similar options that can be changed to retain compatibility.</p> awsTerraform/OpenTofu <pre><code>export AWS_REQUEST_CHECKSUM_CALCULATION=WHEN_REQUIRED\n</code></pre> <p>or</p> <p><pre><code>[default]\nrequest_checksum_calculation = WHEN_REQUIRED\n</code></pre> For better compatibility, ensure you\u2019re using a current version (&gt;=2.23.5) of the AWS CLI that should accept <code>AWS_REQUEST_CHECKSUM_CALCULATION</code> environment variable and config options.</p> <p>Please disable checksums. <pre><code>terraform {\n  required_version = \"&gt;= 1.6.3\"\n\n  backend \"s3\" {\n    endpoints = {\n      s3 = \"https://s3-&lt;region&gt;.citycloud.com\"\n    }\n\n  bucket = \"&lt;your_bucket_name&gt;\"\n  key    = \"&lt;state_file_name&gt;\"\n\n  # Deactivate a few AWS-specific checks\n  skip_credentials_validation = true\n  skip_requesting_account_id  = true\n  skip_metadata_api_check     = true\n  skip_region_validation      = true\n  skip_s3_checksum            = true\n  region                      = \"us-east-1\"\n  }\n}\n</code></pre></p>"},{"location":"reference/limitations/openstack/","title":"OpenStack service limitations","text":""},{"location":"reference/limitations/openstack/#openstack-users-groups-and-roles","title":"OpenStack users, groups, and roles","text":""},{"location":"reference/limitations/openstack/#administrative-privileges","title":"Administrative privileges","text":"<p>No OpenStack user that you create while enabling the OpenStack CLI ever gets privileges exceeding administrative rights bound to a project.</p> <p>This means that you cannot use an OpenStack user to create a new project. You must do so via the Cleura\u00a0Cloud Management\u00a0Panel, or the Cleura\u00a0Cloud REST\u00a0API.</p> <p>It also means that you cannot use the following <code>openstack</code> CLI commands; they all return <code>Unauthorized</code> (HTTP 403):</p> <ul> <li><code>openstack user [create|delete|list|set]</code>. <p>You can use <code>openstack user show</code> and <code>openstack user password set</code>, but only for your own user account.</p> </li> <li><code>openstack group &lt;subcommand&gt;</code></li> <li><code>openstack role &lt;subcommand&gt;</code></li> <li><code>openstack project &lt;subcommand&gt;</code> (except <code>list</code>). <p>You can use <code>openstack project list</code>, but this will only list the project(s) that your user account has access to.</p> </li> <li><code>openstack domain &lt;subcommand&gt;</code></li> </ul>"},{"location":"reference/limitations/openstack/#cinder","title":"Cinder","text":""},{"location":"reference/limitations/openstack/#volume-backup-service","title":"Volume backup service","text":"<p>Cleura\u00a0Cloud does not support the OpenStack volume backup service (<code>cinder-backup</code>).</p> <p>For automated, scheduled volume snapshots, consider configuring your servers for Disaster Recovery (DR) via the Cleura\u00a0Cloud Management\u00a0Panel.</p>"},{"location":"reference/limitations/openstack/#glance","title":"Glance","text":""},{"location":"reference/limitations/openstack/#image-imports","title":"Image imports","text":"<p>The web-download import method, which allows importing images from remote URLs, is not supported. Although we recommend you use one of our supported images, it is possible to create or upload a custom image using the <code>openstack</code> CLI.</p>"},{"location":"reference/limitations/openstack/#nova","title":"Nova","text":""},{"location":"reference/limitations/openstack/#nested-virtualization","title":"Nested virtualization","text":"<p>Running nested virtualization is only supported for Nova instances (servers) running Linux. The server must run a Linux kernel of version 5.0 or later, and QEMU/KVM 4.1 or later.</p> <p>This means that you can run nested virtualization on servers booted from a CentOS 9 (or later), Ubuntu 20.04 (or later), or Debian 11 (or later) base image.</p> <p>Furthermore, you must ensure that the Nova server passes the <code>pcid</code> CPU feature flag to nested guests.</p>"},{"location":"reference/limitations/openstack/#maximum-attached-volumes-per-server","title":"Maximum attached volumes per server","text":"<p>A Nova server in Cleura\u00a0Cloud can concurrently attach a maximum of 25 persistent volumes. This is a limitation of the <code>virtio-blk</code> storage driver that ships as part of the guest operating system\u2019s kernel.</p>"},{"location":"reference/limitations/openstack/#neutron","title":"Neutron","text":""},{"location":"reference/limitations/openstack/#dynamic-routing","title":"Dynamic routing","text":"<p>Neutron in Cleura\u00a0Cloud does not support dynamic routing protocols in a customer-accessible manner. We currently do not expose the ability to configure BGP speakers or peers.</p> <p>While Cleura\u00a0Cloud does use BGP dynamic routing internally, our Neutron configuration restricts the ability to use these features to administrative accounts only.</p>"},{"location":"reference/limitations/openstack/#octavia","title":"Octavia","text":""},{"location":"reference/limitations/openstack/#dual-stack-support","title":"Dual-stack support","text":"<p>A single load balancer managed by OpenStack Octavia can support IPv4 or IPv6, but not both. To expose a service via IPv4 and IPv6, you must set up two separate load balancers pointing to the same backend.</p>"},{"location":"reference/limitations/openstack/#designate","title":"Designate","text":"<p>The OpenStack Designate DNS-as-a-service (DNSaaS) facility is currently not available in Cleura\u00a0Cloud. You must manage your own DNS records for public IP addresses.</p>"},{"location":"reference/limitations/openstack/#manila","title":"Manila","text":"<p>The OpenStack Manila filesystem-as-a-service (FSaaS) facility is currently not available in Cleura\u00a0Cloud. If you require multiple servers to be able to access the same files, create a server that exposes an internal NFS or CIFS service, backed by a Cinder volume.</p>"},{"location":"reference/quotas/openstack/","title":"OpenStack quotas","text":"<p>Most of the OpenStack resources you create in Cleura\u00a0Cloud are subject to quotas. Once you hit a quota limit, the creation of new resources of that type will fail, until you either reduce your resource utilization or your quota is raised.</p> <p>Cleura\u00a0Cloud applies most quotas on a per-project basis. Your Cleura\u00a0Cloud account can manage up to 3 projects.</p> <p>The following quotas apply in any of your Cleura\u00a0Cloud projects by default:</p> Quota name Value Notes <code>cores</code> 20 Maximum number of virtual CPU cores allocatable across all servers (\u201cinstances\u201d). <code>floating_ips</code> 50 Maximum number of public (\u201cfloating\u201d) IPv4 addresses allocated in your project. The limit on floating IP addresses applies regardless of how many are \u201cin use\u201d (that is, associated with a port) at any given time. <code>gigabytes</code> 1000 Maximum total amount of persistent storage used by all volumes (in GiB). <code>instances</code> 10 Maximum number of servers in your project. This limit applies irrespective of whether the server is running, suspended, or shut down. <code>networks</code> 100 Maximum number of networks. <code>ports</code> 500 Maximum total number of ports (virtual network interfaces) across all servers, routers, and load balancers. <code>ram</code> 51200 Maximum total amount of RAM used by all servers (in MiB). <code>rbac_policies</code> 10 Maximum number of Role-Based Access Control (RBAC) rules defined for your networks. (Rarely used.) <code>routers</code> 10 Maximum number of virtual routers. <code>secgroup_rules</code> 100 Maximum number of security group rules (across all security\u00a0groups). <code>secgroups</code> 10 Maximum number of security\u00a0groups. <code>server_group_members</code> 10 Maximum number of servers allocated to one server\u00a0group. <code>server_groups</code> 10 Maximum number of server\u00a0groups. <code>snapshots</code> 10 Maximum number of volume snapshots. <code>subnets</code> 100 Maximum number of subnets. Note that subnets are either for IPv4 or IPv6, so in a dual-stack environment every network corresponds to two subnets. <code>volumes</code> 50 Maximum total number of persistent volumes. <p>There is one other quota that applies per OpenStack API user, not per project:</p> Quota name Value Notes <code>key_pairs</code> 100 Maximum number of secure shell key pairs."},{"location":"reference/quotas/openstack/#reviewing-your-quota","title":"Reviewing your quota","text":"<p>Using the <code>openstack</code> CLI, you can always review the applicable quota settings for your project with the following command:</p> <pre><code>openstack quota show\n</code></pre>"},{"location":"reference/quotas/openstack/#requesting-a-quota-increase","title":"Requesting a quota increase","text":"<p>If you find that you need to deploy more resources in one project than the default quota allows, or if you need to manage more than 3 projects in your account, please file a support request with our Service\u00a0Center.</p>"},{"location":"reference/versions/","title":"Service version matrix","text":"<p>Services in Cleura\u00a0Cloud are updated on a regular basis and on a rolling schedule.</p> <p>This section lists the cloud API service versions available in each Cleura\u00a0Cloud region.</p> <ul> <li> <p>Public Cloud: versions running in our Cleura\u00a0Public\u00a0Cloud regions.</p> </li> <li> <p>Compliant Cloud: versions running in our Cleura\u00a0Compliant\u00a0Cloud regions.</p> </li> </ul>"},{"location":"reference/versions/#openstack-services","title":"OpenStack Services","text":"<p>OpenStack releases are named in alphabetical order, and occur on a six-month release schedule. In Cleura\u00a0Public\u00a0Cloud we upgrade OpenStack releases annually; this means that we normally deploy every other OpenStack release and skip the intervening one.</p> <p>Cleura\u00a0Cloud currently runs OpenStack Caracal in all regions.</p>"},{"location":"reference/versions/#ceph-services","title":"Ceph Services","text":"<p>Ceph major releases are also named in alphabetical order, and occur on a roughly annual schedule.</p> <p>Cleura\u00a0Cloud currently runs Ceph Reef.</p>"},{"location":"reference/versions/compliant/","title":"Compliant Cloud","text":""},{"location":"reference/versions/compliant/#openstack-services","title":"OpenStack Services","text":"Sto1HS Sto2HS Barbican (secret storage) Caracal Caracal Cinder (block storage) Caracal Caracal Glance (image management) Caracal Caracal Heat (orchestration) Caracal Caracal Keystone (identity management) Caracal Caracal Magnum (container management) Caracal Caracal Neutron (networking) Caracal Caracal Nova (server virtualization) Caracal Caracal Octavia (load balancing) Caracal Caracal"},{"location":"reference/versions/compliant/#ceph-services","title":"Ceph Services","text":"Sto1HS Sto2HS Block storage (for OpenStack) Reef Reef Object storage (Swift API) Reef Reef Object storage (S3 API) Reef Reef"},{"location":"reference/versions/public/","title":"Public Cloud","text":""},{"location":"reference/versions/public/#openstack-services","title":"OpenStack Services","text":"Kna1 Sto2 Fra1 Barbican (secret storage) Caracal Caracal Caracal Cinder (block storage) Caracal Caracal Caracal Glance (image management) Caracal Caracal Caracal Heat (orchestration) Caracal Caracal Caracal Keystone (identity management) Caracal Caracal Caracal Magnum (container management) Caracal Caracal Caracal Neutron (networking) Caracal Caracal Caracal Nova (server virtualization) Caracal Caracal Caracal Octavia (load balancing) Caracal Caracal Caracal"},{"location":"reference/versions/public/#ceph-services","title":"Ceph Services","text":"Kna1 Sto2 Fra1 Block storage (for OpenStack) Reef Reef Reef Object storage (Swift API) Reef Reef Object storage (S3 API) Reef Reef"},{"location":"reference/volumes/","title":"Volume service reference","text":""},{"location":"reference/volumes/#volume-types","title":"Volume types","text":"<p>The following volume types are available in Cleura\u00a0Cloud for persistent block storage devices (\u201cvolumes\u201d) managed by OpenStack Cinder.</p> <p>If you create a volume without specifying a volume type, then the default volume type applies.</p> Volume type name Default Encryption max IOPS<sup>1</sup> <code>cbs</code> 10000 <code>cbs-encrypted</code> 10000 <p>It is possible \u2014 though somewhat involved \u2014 to change the type of an existing volume (also known as retyping).</p>"},{"location":"reference/volumes/#administrative-access-to-volume-encryption-secrets","title":"Administrative access to volume encryption secrets","text":"<p>If you want to enable Cleura\u00a0Cloud support to administratively migrate virtual machines with attached encrypted\u00a0volumes, please share your encryption secrets with the following user IDs:</p> Cleura\u00a0Cloud region Administrative user UUID Fra1 <code>a3bee416cf67420995855d602d2bccd3</code> Kna1 <code>a3bee416cf67420995855d602d2bccd3</code> Sto2 <code>a3bee416cf67420995855d602d2bccd3</code> Sto1HS <code>ac35ff3bedf440fe8062a5ba7e17d590</code> Sto2HS <code>17c689c90f9549a3b30ac8ecbef9abb1</code> <ol> <li> <p>The maximum IOPS specification is essentially a cap, which creates an upper bound for individual device performance under ideal conditions. Actual IOPS may vary based on system load and utilization. IOPS limits are only enforced in Cleura\u00a0Public\u00a0Cloud regions.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/","title":"About our tutorials","text":"<p>Our tutorials cover specific use cases in Cleura\u00a0Cloud. We host them on our learning platform, Cleura\u00a0Cloud\u00a0Academy.</p> <p>Our tutorials generally require that you have a Cleura\u00a0Cloud account, and that you are registered on shop.cleura.com. Once you are registered, you may go to our Cloud Course Catalog, pick any of the tutorials, and put them in your cart for free.</p> <p>Although access to our tutorials is free of charge, please keep in mind that any resources you create in a tutorial will be charged at our normal rates. You may, of course, run a tutorial using your free trial credit.</p> <p>If you find our tutorials helpful, you might also be interested in our self-paced online training courses, available for a fee from our course booking site.</p> <p>Our tutorials are grouped by general topics:</p> <ul> <li>Ansible tutorials</li> <li>Container tutorials</li> <li>OpenStack Heat tutorials</li> <li>OpenTofu tutorials</li> </ul>"},{"location":"tutorials/ansible/","title":"Ansible tutorials","text":"<p>These tutorials cover how you can best automate virtual resources in Cleura\u00a0Cloud using Ansible.</p> <ul> <li>Managing Cleura\u00a0Cloud resources with Ansible: Use the <code>openstack.cloud</code> Ansible collection to create networks, routers, and servers in Cleura\u00a0Cloud.</li> <li>Using an Ansible dynamic inventory to discover and manage Cleura\u00a0Cloud servers: Use an Ansible dynamic inventory plugin to automate the discovery of your cloud resources, enabling the management of massively scalable virtual data centers in Cleura\u00a0Cloud.</li> </ul>"},{"location":"tutorials/containers/","title":"Container tutorials","text":"<p>These tutorials cover the deployment of containerized applications in Cleura\u00a0Cloud, using Kubernetes.</p> <ul> <li>Managing containerized workloads in Cleura\u00a0Cloud: Deploy and manage Kubernetes clusters in Cleura\u00a0Cloud with Gardener.</li> </ul>"},{"location":"tutorials/heat/","title":"OpenStack Heat tutorials","text":"<p>These tutorials cover the management of complex virtual resource assemblies (\u201cstacks\u201d) in Cleura\u00a0Cloud using the OpenStack Heat orchestration framework.</p> <ul> <li>Using OpenStack\u00a0Heat to manage Cleura\u00a0Cloud resources: Use OpenStack Heat to automate the creation, update, and configuration of Cleura\u00a0Cloud resources.</li> </ul>"},{"location":"tutorials/tf/","title":"OpenTofu tutorials","text":"<p>These tutorials cover the deployment of virtual resources in Cleura\u00a0Cloud using OpenTofu configurations.</p> <ul> <li>Using OpenTofu to manage Cleura\u00a0Cloud resources: Use OpenTofu to create and manage Cleura\u00a0Cloud server and network resources.</li> </ul>"}]}