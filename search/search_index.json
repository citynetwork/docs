{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Start Here This is the Cleura Beta documentation web site. What does \u201cBeta\u201d mean? The fact that this site is in a Beta stage means that any information you find here should be reliable and accurate for Cleura products and services. If you find any published documentation that is inaccurate or not in line with functionality as you observe it on Cleura, we would very much appreciate if you filed a documentation bug . However, the documentation on this site may be incomplete, meaning that it does not yet cover all functionality available in Cleura. In addition, page paths (URLs) are not yet stable. In other words, content may move from one path to another between visits. If you are looking for content that may have moved, please use the Search function. We are currently in the process of migrating our Knowledge Base and other documentation to this site. If you want to help, here\u2019s how! Site analytics and cookies This site does not set cookies, which is why we also don\u2019t need to annoy you with a cookie consent banner. We use cookie-free site analytics from Plausible .","title":"Start Here"},{"location":"#start-here","text":"This is the Cleura Beta documentation web site.","title":"Start Here"},{"location":"#what-does-beta-mean","text":"The fact that this site is in a Beta stage means that any information you find here should be reliable and accurate for Cleura products and services. If you find any published documentation that is inaccurate or not in line with functionality as you observe it on Cleura, we would very much appreciate if you filed a documentation bug . However, the documentation on this site may be incomplete, meaning that it does not yet cover all functionality available in Cleura. In addition, page paths (URLs) are not yet stable. In other words, content may move from one path to another between visits. If you are looking for content that may have moved, please use the Search function. We are currently in the process of migrating our Knowledge Base and other documentation to this site. If you want to help, here\u2019s how!","title":"What does \"Beta\" mean?"},{"location":"#site-analytics-and-cookies","text":"This site does not set cookies, which is why we also don\u2019t need to annoy you with a cookie consent banner. We use cookie-free site analytics from Plausible .","title":"Site analytics and cookies"},{"location":"concepts/","text":"Concepts and Background This section contains conceptual and background information about Cleura and the services we support. This is not so much about how you do something (for that, please see our How-To Guides ), more about why things work in a certain way, or why we have implemented them in a particular way in Cleura.","title":"Concepts and Background"},{"location":"concepts/#concepts-and-background","text":"This section contains conceptual and background information about Cleura and the services we support. This is not so much about how you do something (for that, please see our How-To Guides ), more about why things work in a certain way, or why we have implemented them in a particular way in Cleura.","title":"Concepts and Background"},{"location":"contrib/","text":"Contribution Guide See something on this site that is inaccurate, missing, or that could simply be improved? There are multiple ways for you to help make this site better, and we welcome all of them. You can make modifications and contributions using Git , and we apply certain checks to ensure consistent documentation quality. Technical Writing Resources Whether you are an experienced technical writer, a regular (or not-so-regular) open source contributor, or you\u2019re making your very first writing contribution, there are a ton of helpful resources on technical writing. Here are a few: Digital Ocean \u2019s Technical Writing Guidelines Red Hat \u2019s Writing Style Guide Gareth Dwyer \u2019s Technical Writing repository Bolaji Ayodeji \u2019s Awesome Technical Writing collection Markdown The documentation on this site uses Markdown . Markdown is a documentation format that is rich enough to be useful for good technical documentation, and yet simpler and easier to learn than other formats like reStructuredText or DocBook XML. If you\u2019re unfamiliar with Markdown, you can read up on its basics in this classic article by John Gruber if you\u2019re interested, but chances are that you\u2019ll also find all the information you\u2019ll need in this cheat sheet by Adam Pritchard , or the Start Writing guide from GitHub . Or you simply look at the source of one of the pages on this site (try the Edit button on this one!) and figure it out as you go along \u2014 it\u2019s really pretty straightforward. License With the sole exception of trademarks like \u201cCleura\u201d and the Cleura logo, the content on this site is available under the Creative Commons Attribution-ShareAlike 4.0 International license , as you can see from the icon at the bottom of each page. Please keep in mind that you are making your contribution under those terms.","title":"Contribution Guide"},{"location":"contrib/#contribution-guide","text":"See something on this site that is inaccurate, missing, or that could simply be improved? There are multiple ways for you to help make this site better, and we welcome all of them. You can make modifications and contributions using Git , and we apply certain checks to ensure consistent documentation quality.","title":"Contribution Guide"},{"location":"contrib/#technical-writing-resources","text":"Whether you are an experienced technical writer, a regular (or not-so-regular) open source contributor, or you\u2019re making your very first writing contribution, there are a ton of helpful resources on technical writing. Here are a few: Digital Ocean \u2019s Technical Writing Guidelines Red Hat \u2019s Writing Style Guide Gareth Dwyer \u2019s Technical Writing repository Bolaji Ayodeji \u2019s Awesome Technical Writing collection","title":"Technical Writing Resources"},{"location":"contrib/#markdown","text":"The documentation on this site uses Markdown . Markdown is a documentation format that is rich enough to be useful for good technical documentation, and yet simpler and easier to learn than other formats like reStructuredText or DocBook XML. If you\u2019re unfamiliar with Markdown, you can read up on its basics in this classic article by John Gruber if you\u2019re interested, but chances are that you\u2019ll also find all the information you\u2019ll need in this cheat sheet by Adam Pritchard , or the Start Writing guide from GitHub . Or you simply look at the source of one of the pages on this site (try the Edit button on this one!) and figure it out as you go along \u2014 it\u2019s really pretty straightforward.","title":"Markdown"},{"location":"contrib/#license","text":"With the sole exception of trademarks like \u201cCleura\u201d and the Cleura logo, the content on this site is available under the Creative Commons Attribution-ShareAlike 4.0 International license , as you can see from the icon at the bottom of each page. Please keep in mind that you are making your contribution under those terms.","title":"License"},{"location":"contrib/modifications/","text":"Modifying content on this site You have two options for editing content: directly in your browser using GitHub, or using a Git-based workflow from your local work environment. Notes on content additions Sections: Generally, content additions should fit somewhere within the existing top-level and second-level sections. Try not to introduce a new top-level or second-level section. Screenshots: If you are contributing a change that contains screenshots from Cleura Cloud Management Panel, they should use a resolution of 1920\u00d71080 pixels (1080p). If your screen uses a larger resolution, use Firefox Responsive Design Mode or Chrome/Chromium Device Mode to configure your browser with 1080p. Screenshots should be added to a directory named assets , located in the same directory as the Markdown file you are adding or editing. CLI screen dumps: If you are contributing a change that contains a screen dump from the openstack command-line client, please limit its width to 100 characters. You can do this by setting the following environment variable in your terminal, before you start work on your change. export CLIFF_MAX_TERM_WIDTH = 100 Modifying content from your browser Every page on this site has an Edit button \ud83d\udd8d\ufe0f. If you click it, it\u2019ll take you straight to the corresponding source page in GitHub. Then, you can follow GitHub\u2019s documentation on how to propose changes to another user\u2019s repository. Modifying content using Git The Git repository for this site lives at https://github.com/citynetwork/docs . You can fork that repository , make the proposed changes in your fork, and then send us a standard GitHub pull request . For this purpose, use git in combination with either GitHub\u2019s web interface, or the gh command-line interface (CLI). First, create a fork of the documentation repository: git client and web browser gh client Open https://github.com/citynetwork/docs and click the Fork button. When you create your new fork, it\u2019s fine to leave the Copy the main branch only option enabled. Then, proceed to create a new local checkout of your fork: git clone git@github.com:<yourusername>/<your-repo-fork> cleura-docs cd cleura-docs gh repo fork --clone https://github.com/citynetwork/docs -- cleura-docs cd cleura-docs Next, create a local topic branch and make your modifications: git checkout -b <your-topic-branch-name> # edit your files git add <files-to-add> git commit Please see our notes on commit messages . Finally, create a pull request (PR) from your changes: git client and web browser gh client Run the following git command (assuming origin is the remote that points to your fork): git push origin <your-topic-branch-name> Then, open your browser to the URL suggested by the git push command, and proceed to create a pull request. gh pr create --fill Monitoring changes as you edit To see your changes as you work on them, you can use tox . Having created a topic branch with your modifications, run: cd cleura-docs git checkout <your-topic-branch-name> tox -e serve A local copy of the documentation will then run on your local machine and be accessible from http://localhost:8000 in your browser. When you are planning to make several changes in rapid succession, you may want to speed up rendering the site after each change. You may do so by disabling a plugin that checks all links (including external links) for accessibility: cd cleura-docs export DOCS_ENABLE_HTMLPROOFER = false tox -e serve Commit messages When you submit a change, you will need to provide a commit message, which is very nearly as important as the change itself. Excellent guides on what constitutes a good commit message are available from Tim Pope and Colleen Murphy . In addition, we have adopted the Conventional Commits style for commit message subjects. Please make sure that your commit message starts with one of the following prefixes: feat: denotes a content addition, such as adding documentation for some Cleura Cloud functionality that was not included in the documentation before. fix: denotes a content correction, such as fixing a documentation bug. build: denotes a change to the build process, such as an improvement to a CI check. chore: denotes a minor change that is neither a feature, nor a fix, nor a build improvement, such as when you edit the .mailmap file. docs: denotes a change to the documention for this site, such as an update to the README.md file.","title":"Modifying content on this site"},{"location":"contrib/modifications/#modifying-content-on-this-site","text":"You have two options for editing content: directly in your browser using GitHub, or using a Git-based workflow from your local work environment.","title":"Modifying content on this site"},{"location":"contrib/modifications/#notes-on-content-additions","text":"Sections: Generally, content additions should fit somewhere within the existing top-level and second-level sections. Try not to introduce a new top-level or second-level section. Screenshots: If you are contributing a change that contains screenshots from Cleura Cloud Management Panel, they should use a resolution of 1920\u00d71080 pixels (1080p). If your screen uses a larger resolution, use Firefox Responsive Design Mode or Chrome/Chromium Device Mode to configure your browser with 1080p. Screenshots should be added to a directory named assets , located in the same directory as the Markdown file you are adding or editing. CLI screen dumps: If you are contributing a change that contains a screen dump from the openstack command-line client, please limit its width to 100 characters. You can do this by setting the following environment variable in your terminal, before you start work on your change. export CLIFF_MAX_TERM_WIDTH = 100","title":"Notes on content additions"},{"location":"contrib/modifications/#modifying-content-from-your-browser","text":"Every page on this site has an Edit button \ud83d\udd8d\ufe0f. If you click it, it\u2019ll take you straight to the corresponding source page in GitHub. Then, you can follow GitHub\u2019s documentation on how to propose changes to another user\u2019s repository.","title":"Modifying content from your browser"},{"location":"contrib/modifications/#modifying-content-using-git","text":"The Git repository for this site lives at https://github.com/citynetwork/docs . You can fork that repository , make the proposed changes in your fork, and then send us a standard GitHub pull request . For this purpose, use git in combination with either GitHub\u2019s web interface, or the gh command-line interface (CLI). First, create a fork of the documentation repository: git client and web browser gh client Open https://github.com/citynetwork/docs and click the Fork button. When you create your new fork, it\u2019s fine to leave the Copy the main branch only option enabled. Then, proceed to create a new local checkout of your fork: git clone git@github.com:<yourusername>/<your-repo-fork> cleura-docs cd cleura-docs gh repo fork --clone https://github.com/citynetwork/docs -- cleura-docs cd cleura-docs Next, create a local topic branch and make your modifications: git checkout -b <your-topic-branch-name> # edit your files git add <files-to-add> git commit Please see our notes on commit messages . Finally, create a pull request (PR) from your changes: git client and web browser gh client Run the following git command (assuming origin is the remote that points to your fork): git push origin <your-topic-branch-name> Then, open your browser to the URL suggested by the git push command, and proceed to create a pull request. gh pr create --fill","title":"Modifying content using Git"},{"location":"contrib/modifications/#monitoring-changes-as-you-edit","text":"To see your changes as you work on them, you can use tox . Having created a topic branch with your modifications, run: cd cleura-docs git checkout <your-topic-branch-name> tox -e serve A local copy of the documentation will then run on your local machine and be accessible from http://localhost:8000 in your browser. When you are planning to make several changes in rapid succession, you may want to speed up rendering the site after each change. You may do so by disabling a plugin that checks all links (including external links) for accessibility: cd cleura-docs export DOCS_ENABLE_HTMLPROOFER = false tox -e serve","title":"Monitoring changes as you edit"},{"location":"contrib/modifications/#commit-messages","text":"When you submit a change, you will need to provide a commit message, which is very nearly as important as the change itself. Excellent guides on what constitutes a good commit message are available from Tim Pope and Colleen Murphy . In addition, we have adopted the Conventional Commits style for commit message subjects. Please make sure that your commit message starts with one of the following prefixes: feat: denotes a content addition, such as adding documentation for some Cleura Cloud functionality that was not included in the documentation before. fix: denotes a content correction, such as fixing a documentation bug. build: denotes a change to the build process, such as an improvement to a CI check. chore: denotes a minor change that is neither a feature, nor a fix, nor a build improvement, such as when you edit the .mailmap file. docs: denotes a change to the documention for this site, such as an update to the README.md file.","title":"Commit messages"},{"location":"contrib/quality/","text":"Quality checks There are a few checks that we apply to the configuration of this site. These checks run automatically via GitHub Actions workflows when you send your PR : We check the commit message with gitlint , and enforce the Conventional Commits commit message style. We check whether the documentation still builds correctly, with your change applied. We check to make sure that no internal or external links in the documentation are dead. This is one example where the checks might fail through no fault of yours \u2014 some external link may have disappeared between the most recent change and your contribution, by pure coincidence. When that happens, we\u2019ll fix it together. We check some YAML conventions with yamllint . However, most contributions would probably only touch Markdown files and not YAML, so you\u2019re unlikely to trip over this. If you\u2019re working in your local Git repository and your work environment has tox installed, you can also run the checks locally: tox You can also configure your local checkout to run quality checks on each commit. To do that, run: git config core.hooksPath .githooks","title":"Quality checks"},{"location":"contrib/quality/#quality-checks","text":"There are a few checks that we apply to the configuration of this site. These checks run automatically via GitHub Actions workflows when you send your PR : We check the commit message with gitlint , and enforce the Conventional Commits commit message style. We check whether the documentation still builds correctly, with your change applied. We check to make sure that no internal or external links in the documentation are dead. This is one example where the checks might fail through no fault of yours \u2014 some external link may have disappeared between the most recent change and your contribution, by pure coincidence. When that happens, we\u2019ll fix it together. We check some YAML conventions with yamllint . However, most contributions would probably only touch Markdown files and not YAML, so you\u2019re unlikely to trip over this. If you\u2019re working in your local Git repository and your work environment has tox installed, you can also run the checks locally: tox You can also configure your local checkout to run quality checks on each commit. To do that, run: git config core.hooksPath .githooks","title":"Quality checks"},{"location":"howto/","text":"About our How-To guides In this section you\u2019ll find details about how you can accomplish specific tasks in Cleura and the services we support. There are several categories of How-To guides, and they tend to be focused on a specific cloud technology. Getting Started How-Tos help you create an account in Cleura Cloud, and start using our services. Kubernetes How-Tos cover how you can create and manage your Kubernetes deployments using Cleura Cloud Management Panel. Object storage How-Tos deal with the S3 and Swift object storage APIs, and how you can use them for object storage in Cleura. OpenStack CLI/API How-Tos cover tasks that you can accomplish with the OpenStack command line interfaces and application programming interfaces. They generally do not depend on any adjacent services or tools, just your Cleura OpenStack credentials, the openstack client, and/or the native OpenStack APIs.","title":"About our How-To guides"},{"location":"howto/#about-our-how-to-guides","text":"In this section you\u2019ll find details about how you can accomplish specific tasks in Cleura and the services we support. There are several categories of How-To guides, and they tend to be focused on a specific cloud technology. Getting Started How-Tos help you create an account in Cleura Cloud, and start using our services. Kubernetes How-Tos cover how you can create and manage your Kubernetes deployments using Cleura Cloud Management Panel. Object storage How-Tos deal with the S3 and Swift object storage APIs, and how you can use them for object storage in Cleura. OpenStack CLI/API How-Tos cover tasks that you can accomplish with the OpenStack command line interfaces and application programming interfaces. They generally do not depend on any adjacent services or tools, just your Cleura OpenStack credentials, the openstack client, and/or the native OpenStack APIs.","title":"About our How-To guides"},{"location":"howto/getting-started/create-account/","text":"Creating a new account To gain access to the Cleura Cloud Management Panel, you first have to create a new account. For that, navigate to https://cleura.cloud . At the bottom right-hand side of the page, click on the Create account button. Select the new account type (that would be Company or Private ), carefully type in a valid email address, and choose your country. At your leisure, please read the City Network General Terms And Conditions and our Data Processing Agreement . Agree to these documents (select Yes ), check the I\u2019m not a robot box, and then click on the Create button. This will redirect you to the Cleura Cloud Management Panel, and since you are logging in from a new account for the first time, you now have to take three simple steps. Step 1 - Confirm your email . Check your inbox or your SPAM/junk folder for an email from no-reply@cleura.com with the subject Thank you for your registration - Cleura Cloud . Open that email and click on the link in the message body. Step 2 - Account information . After clicking on the confirmation link you move on to step two, where you enter all relevant information that uniquely identifies the brand-new account. Type in, for example, a username for the account user, and make sure you define a strong password for them. (A password manager may come in handy.) Please note that all fields are mandatory, so take a little time and fill them in accordingly. Should you have a rebate code, do not forget to click on I have a rebate code and type it in below. When you are done, click on the Save button. Step 3 - Account verification . While the new account is being created, and before it becomes fully operational, you have to take one last step toward verification. You do that either by entering valid credit card information or by placing a simple phone call. Should you choose to verify by credit card, rest assured that no charge will take place \u2014 no money will be drawn from the card, in other words. On the other hand, if you prefer to verify by phone, you may certainly do so during business hours (08:00 \u2013 17:00 CET/CEST UTC+1/UTC+2). If you choose to call, please remember that you will be asked for the username of the new account, so have that piece of info handy. After the account verification is complete, you are greeted by the Cleura Cloud Management Panel. Feel free to follow through the introductory guide to the environment \u2014 that will not take long \u2014 or skip it and start taking advantage of the Cleura Cloud without delay.","title":"Creating a new account"},{"location":"howto/getting-started/create-account/#creating-a-new-account","text":"To gain access to the Cleura Cloud Management Panel, you first have to create a new account. For that, navigate to https://cleura.cloud . At the bottom right-hand side of the page, click on the Create account button. Select the new account type (that would be Company or Private ), carefully type in a valid email address, and choose your country. At your leisure, please read the City Network General Terms And Conditions and our Data Processing Agreement . Agree to these documents (select Yes ), check the I\u2019m not a robot box, and then click on the Create button. This will redirect you to the Cleura Cloud Management Panel, and since you are logging in from a new account for the first time, you now have to take three simple steps. Step 1 - Confirm your email . Check your inbox or your SPAM/junk folder for an email from no-reply@cleura.com with the subject Thank you for your registration - Cleura Cloud . Open that email and click on the link in the message body. Step 2 - Account information . After clicking on the confirmation link you move on to step two, where you enter all relevant information that uniquely identifies the brand-new account. Type in, for example, a username for the account user, and make sure you define a strong password for them. (A password manager may come in handy.) Please note that all fields are mandatory, so take a little time and fill them in accordingly. Should you have a rebate code, do not forget to click on I have a rebate code and type it in below. When you are done, click on the Save button. Step 3 - Account verification . While the new account is being created, and before it becomes fully operational, you have to take one last step toward verification. You do that either by entering valid credit card information or by placing a simple phone call. Should you choose to verify by credit card, rest assured that no charge will take place \u2014 no money will be drawn from the card, in other words. On the other hand, if you prefer to verify by phone, you may certainly do so during business hours (08:00 \u2013 17:00 CET/CEST UTC+1/UTC+2). If you choose to call, please remember that you will be asked for the username of the new account, so have that piece of info handy. After the account verification is complete, you are greeted by the Cleura Cloud Management Panel. Feel free to follow through the introductory guide to the environment \u2014 that will not take long \u2014 or skip it and start taking advantage of the Cleura Cloud without delay.","title":"Creating a new account"},{"location":"howto/getting-started/enable-openstack-cli/","text":"Enabling the OpenStack CLI The OpenStack Command Line Interface (CLI) tool, also known as OpenStack Client (OSC) or simply openstack , conveniently provides access to various OpenStack APIs. Using the OpenStack CLI tool, you can remotely create and manage the lifecycle of objects related, for example, to Compute, Networking, or Storage. Before installing openstack to your local laptop or workstation, you first need to have an OpenStack user in your Cleura account. Next, you create and download a special RC file onto your computer, modify it to reflect your OpenStack user\u2019s credentials, and source it. Only then will you be able to use any installed openstack client. Creating an OpenStack user From your favorite web browser, navigate to the Cleura Cloud page, and login into your Cleura account. Please make sure the left-hand side pane on the Cleura Cloud Management Panel is fully visible, click the Users category to expand it, and click on Openstack Users . Then, at the top right-hand side of the Cleura Cloud Management Panel, click once more the Add new Openstack user option. A new pane will slide into view, titled Create Openstack User . Type in a username and a password for the new OpenStack user. To ensure you typed the password correctly, you must re-type it below. This password should be adequately strong, and thus a password manager may come in handy. Scroll down a bit, so the Regions section is in full view. Expand one or more of the available regions you want your new user to have access to. For each one of the expanded regions, select one or more Projects . For each project, activate one or more Roles . (Hint: For an overview of the rights that roles provide, hover the mouse pointer over the exclamation mark icon by the Roles .) Optionally, type in a description for the new OpenStack user. Then, create the user by clicking the green Create button below the Description box. The new OpenStack user will be ready in just a few seconds. At any time, you can view all available OpenStack users by going to the left-hand side pane on the Cleura Cloud Management Panel and selecting Users > Openstack Users . Creating and downloading an RC file On the Cleura Cloud Management Panel expand the left-hand side pane, click Users and then Openstack Users . You will then see, listed in the main pane titled Openstack Users , all available users. Click on the three-dotted round icon on the right of the user you want to create an RC file for. From the pop-up that appears, select Generate RC file . The RC file will be generated automatically. Before downloading it onto your local computer, you must select one of the available projects to relate the RC file to. Do so and then click the blue Download button. A \u201cSave as\u201d dialog window appears, specific to the operating system you are currently using. Select a convenient location and save your RC file. Modifying and sourcing the RC file The general naming for RC files goes like this: your_username--region_name--project_name--rc So, assuming your username is olafsdottir , and the RC file has been created for the fra1 region and the katla project, your RC file name should be this: olafsdottir--fra1--katla--rc Take a look at the contents of this file \u2014 they should be like this: export OS_USERNAME = olafsdottir export OS_PASSWORD = <your password goes here> export OS_AUTH_URL = https://fra1.citycloud.com:5000 export OS_USER_DOMAIN_NAME = ... export OS_PROJECT_DOMAIN_NAME = ... export OS_REGION_NAME = Fra1 export OS_PROJECT_NAME = \"katla\" export OS_TENANT_NAME = \"katla\" export OS_AUTH_VERSION = 3 export OS_IDENTITY_API_VERSION = 3 Before you source the RC file, and thus initialize all relevant environment variables, make sure to edit the file and put your OpenStack user password in place of <your password goes here> . Also, change the permissions of the file, so it is readable and writable by your local user only: chmod 600 olafsdottir--fra1--katla--rc Then, go ahead and source it: source olafsdottir--fra1--katla--rc Installing the OpenStack CLI If you do not have the OpenStack CLI tool readily available, use your operating system\u2019s package manager or pip to install it. Some examples follow. Debian/Ubuntu Mac OS X with Homebrew Python package apt update && apt install python3-openstackclient brew install openstackclient pip install python-openstackclient Testing access Provided you have already sourced your RC file, you can now use the openstack command line tool to access various OpenStack APIs on the Cleura Cloud. To make sure your local installation of openstack works as expected, type: openstack token issue If openstack can indeed connect to the Cleura Cloud OpenStack APIs, then you will get information, in tabular format, regarding the issuance of a new token. To get general help regarding openstack , type: openstack --help When you need help on a specific command, type something like openstack help command . Auto-adjusting the CLI output to your terminal size Many of the subcommands available in the openstack CLI produce tabular about by default. To ensure that this output always fits neatly into your terminal window, you may add the following line either to OpenStack RC file(s), or to your shell initialization file (like .profile or .bashrc ): export CLIFF_FIT_WIDTH = 1 Then, be sure to either re-source the file you modified, and/or restart your shell.","title":"Enabling the OpenStack CLI"},{"location":"howto/getting-started/enable-openstack-cli/#enabling-the-openstack-cli","text":"The OpenStack Command Line Interface (CLI) tool, also known as OpenStack Client (OSC) or simply openstack , conveniently provides access to various OpenStack APIs. Using the OpenStack CLI tool, you can remotely create and manage the lifecycle of objects related, for example, to Compute, Networking, or Storage. Before installing openstack to your local laptop or workstation, you first need to have an OpenStack user in your Cleura account. Next, you create and download a special RC file onto your computer, modify it to reflect your OpenStack user\u2019s credentials, and source it. Only then will you be able to use any installed openstack client.","title":"Enabling the OpenStack CLI"},{"location":"howto/getting-started/enable-openstack-cli/#creating-an-openstack-user","text":"From your favorite web browser, navigate to the Cleura Cloud page, and login into your Cleura account. Please make sure the left-hand side pane on the Cleura Cloud Management Panel is fully visible, click the Users category to expand it, and click on Openstack Users . Then, at the top right-hand side of the Cleura Cloud Management Panel, click once more the Add new Openstack user option. A new pane will slide into view, titled Create Openstack User . Type in a username and a password for the new OpenStack user. To ensure you typed the password correctly, you must re-type it below. This password should be adequately strong, and thus a password manager may come in handy. Scroll down a bit, so the Regions section is in full view. Expand one or more of the available regions you want your new user to have access to. For each one of the expanded regions, select one or more Projects . For each project, activate one or more Roles . (Hint: For an overview of the rights that roles provide, hover the mouse pointer over the exclamation mark icon by the Roles .) Optionally, type in a description for the new OpenStack user. Then, create the user by clicking the green Create button below the Description box. The new OpenStack user will be ready in just a few seconds. At any time, you can view all available OpenStack users by going to the left-hand side pane on the Cleura Cloud Management Panel and selecting Users > Openstack Users .","title":"Creating an OpenStack user"},{"location":"howto/getting-started/enable-openstack-cli/#creating-and-downloading-an-rc-file","text":"On the Cleura Cloud Management Panel expand the left-hand side pane, click Users and then Openstack Users . You will then see, listed in the main pane titled Openstack Users , all available users. Click on the three-dotted round icon on the right of the user you want to create an RC file for. From the pop-up that appears, select Generate RC file . The RC file will be generated automatically. Before downloading it onto your local computer, you must select one of the available projects to relate the RC file to. Do so and then click the blue Download button. A \u201cSave as\u201d dialog window appears, specific to the operating system you are currently using. Select a convenient location and save your RC file.","title":"Creating and downloading an RC file"},{"location":"howto/getting-started/enable-openstack-cli/#modifying-and-sourcing-the-rc-file","text":"The general naming for RC files goes like this: your_username--region_name--project_name--rc So, assuming your username is olafsdottir , and the RC file has been created for the fra1 region and the katla project, your RC file name should be this: olafsdottir--fra1--katla--rc Take a look at the contents of this file \u2014 they should be like this: export OS_USERNAME = olafsdottir export OS_PASSWORD = <your password goes here> export OS_AUTH_URL = https://fra1.citycloud.com:5000 export OS_USER_DOMAIN_NAME = ... export OS_PROJECT_DOMAIN_NAME = ... export OS_REGION_NAME = Fra1 export OS_PROJECT_NAME = \"katla\" export OS_TENANT_NAME = \"katla\" export OS_AUTH_VERSION = 3 export OS_IDENTITY_API_VERSION = 3 Before you source the RC file, and thus initialize all relevant environment variables, make sure to edit the file and put your OpenStack user password in place of <your password goes here> . Also, change the permissions of the file, so it is readable and writable by your local user only: chmod 600 olafsdottir--fra1--katla--rc Then, go ahead and source it: source olafsdottir--fra1--katla--rc","title":"Modifying and sourcing the RC file"},{"location":"howto/getting-started/enable-openstack-cli/#installing-the-openstack-cli","text":"If you do not have the OpenStack CLI tool readily available, use your operating system\u2019s package manager or pip to install it. Some examples follow. Debian/Ubuntu Mac OS X with Homebrew Python package apt update && apt install python3-openstackclient brew install openstackclient pip install python-openstackclient","title":"Installing the OpenStack CLI"},{"location":"howto/getting-started/enable-openstack-cli/#testing-access","text":"Provided you have already sourced your RC file, you can now use the openstack command line tool to access various OpenStack APIs on the Cleura Cloud. To make sure your local installation of openstack works as expected, type: openstack token issue If openstack can indeed connect to the Cleura Cloud OpenStack APIs, then you will get information, in tabular format, regarding the issuance of a new token. To get general help regarding openstack , type: openstack --help When you need help on a specific command, type something like openstack help command .","title":"Testing access"},{"location":"howto/getting-started/enable-openstack-cli/#auto-adjusting-the-cli-output-to-your-terminal-size","text":"Many of the subcommands available in the openstack CLI produce tabular about by default. To ensure that this output always fits neatly into your terminal window, you may add the following line either to OpenStack RC file(s), or to your shell initialization file (like .profile or .bashrc ): export CLIFF_FIT_WIDTH = 1 Then, be sure to either re-source the file you modified, and/or restart your shell.","title":"Auto-adjusting the CLI output to your terminal size"},{"location":"howto/kubernetes/","text":"Kubernetes in Cleura In Cleura you can run Kubernetes in various ways. Cleura Cloud Management Panel includes management interfaces for Gardener and OpenStack Magnum . Gardener Gardener is a Kubernetes-native system that provides automated management and operation of Kubernetes clusters as a service. It allows you to create clusters and automatically handle their lifecycle operations, including configurable maintenance windows, hibernation schedules, and automatic updates to Kubernetes control plane and worker nodes. You can read more about Gardener and its capabilities on its documentation website . To learn how to use Gardener in the Cleura Cloud Management Panel, refer to Creating Kubernetes clusters using Cleura Cloud Management Panel . Magnum Magnum lets you create clusters via the OpenStack APIs. To do that, you base your configuration on a Cluster Template. The template defines parameters describing how the cluster will be constructed, such as worker flavors. In each region, we offer predefined public Cluster Templates with ready-to-use configurations. To learn more about Cluster Templates, check out the Magnum documentation . Once you have chosen your Cluster Template, you move on to create a cluster based on that template . When you create the cluster, you can define the number of nodes in your cluster, ask for multiple master nodes with a load balancer in front, etc.","title":"Kubernetes in Cleura"},{"location":"howto/kubernetes/#kubernetes-in-cleura","text":"In Cleura you can run Kubernetes in various ways. Cleura Cloud Management Panel includes management interfaces for Gardener and OpenStack Magnum .","title":"Kubernetes in Cleura"},{"location":"howto/kubernetes/#gardener","text":"Gardener is a Kubernetes-native system that provides automated management and operation of Kubernetes clusters as a service. It allows you to create clusters and automatically handle their lifecycle operations, including configurable maintenance windows, hibernation schedules, and automatic updates to Kubernetes control plane and worker nodes. You can read more about Gardener and its capabilities on its documentation website . To learn how to use Gardener in the Cleura Cloud Management Panel, refer to Creating Kubernetes clusters using Cleura Cloud Management Panel .","title":"Gardener"},{"location":"howto/kubernetes/#magnum","text":"Magnum lets you create clusters via the OpenStack APIs. To do that, you base your configuration on a Cluster Template. The template defines parameters describing how the cluster will be constructed, such as worker flavors. In each region, we offer predefined public Cluster Templates with ready-to-use configurations. To learn more about Cluster Templates, check out the Magnum documentation . Once you have chosen your Cluster Template, you move on to create a cluster based on that template . When you create the cluster, you can define the number of nodes in your cluster, ask for multiple master nodes with a load balancer in front, etc.","title":"Magnum"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/","text":"Creating Kubernetes clusters with Gardener If you want to create a Kubernetes cluster, you can do it via Cleura Cloud Management Panel using Gardener. This how-to shows you how to do that, and how to deploy a sample application on such a cluster. Prerequisites To access the cluster from your computer, you will need kubectl installed on your machine. Creating a Kubernetes cluster in Cleura Cloud Management Panel To get started, navigate to https://cleura.cloud , and in the side panel choose Kubernetes \u2192 Managed Kubernetes . You will see a Gardener page, in which you can create and manage your clusters. Click Create Kubernetes cluster . In Gardener\u2019s terminology, a Kubernetes cluster is referred as a Shoot cluster . You can see the name \u201cShoot\u201d in various places throughout the panel\u2019s UI, so it is good to know what it means. To learn more, check out Gardener architecture . In the opened form, fill in the name of the cluster and a region to see the rest of the options. Hover over question mark icons at each form field to learn more about it. For the purposes of this how-to, you can leave everything at default values and click Create at the bottom. In the form you can see a section about worker groups. This name refers to Kubernetes worker nodes. In the list of clusters, you will see your new Gardener shoot bootstrapping. The icon on the left marks the progress. Creating the cluster can take up to several minutes. Extract kubeconfig file from Shoot cluster When the Shoot cluster is up and running, you need to get a kubeconfig file to be able to access it. To do that, click on the cluster to expand its properties, and open Kubeconfig . You should now see the file\u2019s contents. You have the option to Copy Config or Rotate Kubeconfig if your credentials got compromised. Copy the content of the kubeconfig and insert it into ~/.kube/config . Create the directory and the file if needed. By default, Kubectl searches for its configuration in ~/.kube/config , but you can modify this behaviour if needed. More info here . Check if your kubectl uses the proper configuration by running: kubectl config view You should see something like this: apiVersion : v1 clusters : - cluster : certificate-authority-data : DATA+OMITTED server : https://api.test-cluster.p40698.staging-k8s.cleura.cloud name : shoot--p40698--test-cluster contexts : - context : cluster : shoot--p40698--test-cluster user : shoot--p40698--test-cluster-token name : shoot--p40698--test-cluster current-context : shoot--p40698--test-cluster kind : Config preferences : { } users : - name : shoot--p40698--test-cluster-token user : token : REDACTED Access your cluster with kubectl and deploy an application Check your available nodes by running: kubectl get nodes You should see Gardener\u2019s worker node that is available: NAME STATUS ROLES AGE VERSION shoot--p40698--test-cluster-czg4zf-z1-5d7b5-bfl7p Ready <none> 156m v1.24.3 Create a sample deployment with a Hello World application: kubectl create deployment hello-node --image = registry.k8s.io/echoserver:1.4 kubectl expose deployment hello-node --type = LoadBalancer --port = 8080 To access the created app, list the available services: kubectl get services You should get the load balancer service with its external IP and port number: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-node LoadBalancer 100.69.16.106 <External IP> 8080:32039/TCP 34m kubernetes ClusterIP 100.64.0.1 <none> 443/TCP 3h46m Open a browser and open <External IP>:8080 . You should see the page of the Hello World app.","title":"Creating Kubernetes clusters with Gardener"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/#creating-kubernetes-clusters-with-gardener","text":"If you want to create a Kubernetes cluster, you can do it via Cleura Cloud Management Panel using Gardener. This how-to shows you how to do that, and how to deploy a sample application on such a cluster.","title":"Creating Kubernetes clusters with Gardener"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/#prerequisites","text":"To access the cluster from your computer, you will need kubectl installed on your machine.","title":"Prerequisites"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/#creating-a-kubernetes-cluster-in-cleura-cloud-management-panel","text":"To get started, navigate to https://cleura.cloud , and in the side panel choose Kubernetes \u2192 Managed Kubernetes . You will see a Gardener page, in which you can create and manage your clusters. Click Create Kubernetes cluster . In Gardener\u2019s terminology, a Kubernetes cluster is referred as a Shoot cluster . You can see the name \u201cShoot\u201d in various places throughout the panel\u2019s UI, so it is good to know what it means. To learn more, check out Gardener architecture . In the opened form, fill in the name of the cluster and a region to see the rest of the options. Hover over question mark icons at each form field to learn more about it. For the purposes of this how-to, you can leave everything at default values and click Create at the bottom. In the form you can see a section about worker groups. This name refers to Kubernetes worker nodes. In the list of clusters, you will see your new Gardener shoot bootstrapping. The icon on the left marks the progress. Creating the cluster can take up to several minutes.","title":"Creating a Kubernetes cluster in Cleura Cloud Management Panel"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/#extract-kubeconfig-file-from-shoot-cluster","text":"When the Shoot cluster is up and running, you need to get a kubeconfig file to be able to access it. To do that, click on the cluster to expand its properties, and open Kubeconfig . You should now see the file\u2019s contents. You have the option to Copy Config or Rotate Kubeconfig if your credentials got compromised. Copy the content of the kubeconfig and insert it into ~/.kube/config . Create the directory and the file if needed. By default, Kubectl searches for its configuration in ~/.kube/config , but you can modify this behaviour if needed. More info here . Check if your kubectl uses the proper configuration by running: kubectl config view You should see something like this: apiVersion : v1 clusters : - cluster : certificate-authority-data : DATA+OMITTED server : https://api.test-cluster.p40698.staging-k8s.cleura.cloud name : shoot--p40698--test-cluster contexts : - context : cluster : shoot--p40698--test-cluster user : shoot--p40698--test-cluster-token name : shoot--p40698--test-cluster current-context : shoot--p40698--test-cluster kind : Config preferences : { } users : - name : shoot--p40698--test-cluster-token user : token : REDACTED","title":"Extract kubeconfig file from Shoot cluster"},{"location":"howto/kubernetes/gardener/create-shoot-cluster/#access-your-cluster-with-kubectl-and-deploy-an-application","text":"Check your available nodes by running: kubectl get nodes You should see Gardener\u2019s worker node that is available: NAME STATUS ROLES AGE VERSION shoot--p40698--test-cluster-czg4zf-z1-5d7b5-bfl7p Ready <none> 156m v1.24.3 Create a sample deployment with a Hello World application: kubectl create deployment hello-node --image = registry.k8s.io/echoserver:1.4 kubectl expose deployment hello-node --type = LoadBalancer --port = 8080 To access the created app, list the available services: kubectl get services You should get the load balancer service with its external IP and port number: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-node LoadBalancer 100.69.16.106 <External IP> 8080:32039/TCP 34m kubernetes ClusterIP 100.64.0.1 <none> 443/TCP 3h46m Open a browser and open <External IP>:8080 . You should see the page of the Hello World app.","title":"Access your cluster with kubectl and deploy an application"},{"location":"howto/object-storage/s3/credentials/","text":"Working with S3-compatible credentials When you want to interact with object storage in Cleura using tools that support an Amazon S3 compatible API (such as s3cmd , rclone , the aws CLI, or the Python boto3 library), you need an S3-compatible access key ID and secret key. Creating credentials You can create a set of S3-compatible credentials with the following command: openstack ec2 credentials create This will return an Access and Secret key that you can use to populate the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables (or whichever configuration options your application requires). Your S3-compatible credentials are always scoped to your Cleura region and project . You cannot reuse an access and secret key across multiple regions or projects. Also, your credentials are only \u201cS3-compatible\u201d in the sense that they use the same format as AWS S3 does. They are never valid against AWS S3 itself. Listing credentials You can list any previously-created credentials with: openstack ec2 credentials list Configuring your S3 API client Once you have obtained your S3-compatible access and secret key, you need to configure your S3 client with it. How exactly you do that depends on your preferred client: aws mc s3cmd Create a new profile, named after your Cleura region: aws configure set \\ --profile <region> \\ aws_access_key_id <access-key> aws configure set \\ --profile <region> \\ aws_secret_access_key <secret-key> For the aws CLI, you cannot define a region\u2019s endpoint in the profile. As such, you must add the --endpoint-url=https://s3-<region>.citycloud.com:8080 option to each aws s3api call. Create a new alias, named after your Cleura region: mc alias set <region> \\ https://s3-<region>.citycloud.com:8080 \\ <access-key> <secret-key> Once you have configured an alias like this, you are able to run bucket operations with mc using the alias/bucket syntax. s3cmd does not support configuration profiles, so you need to use a separate configuration file for each Cleura region you want to use: s3cmd -c ~/.s3cfg-<region> --configure Set your Access Key and Secret Key when prompted. Leave Default Region unchanged. Set S3 Endpoint to s3-<region>.citycloud.com:8080 . Set DNS-style bucket+hostname:port template for accessing a bucket to s3-<region>.citycloud.com:8080 as well. Set Use HTTPS protocol to Yes (the default). Configure GnuPG encryption and your HTTP proxy server, if needed. Test access with your supplied credentials. On subsequent invocations of the s3cmd CLI, always add the -c ~/.s3cfg-<region> option. Deleting credentials If at any time you need to delete a set of AWS-compatible credentials, you can do so with the following command: openstack ec2 credentials delete <access-key-id> Deleting a set of S3-compatible credentials will immediately revoke access for any applications that were using it.","title":"Working with S3-compatible credentials"},{"location":"howto/object-storage/s3/credentials/#working-with-s3-compatible-credentials","text":"When you want to interact with object storage in Cleura using tools that support an Amazon S3 compatible API (such as s3cmd , rclone , the aws CLI, or the Python boto3 library), you need an S3-compatible access key ID and secret key.","title":"Working with S3-compatible credentials"},{"location":"howto/object-storage/s3/credentials/#creating-credentials","text":"You can create a set of S3-compatible credentials with the following command: openstack ec2 credentials create This will return an Access and Secret key that you can use to populate the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables (or whichever configuration options your application requires). Your S3-compatible credentials are always scoped to your Cleura region and project . You cannot reuse an access and secret key across multiple regions or projects. Also, your credentials are only \u201cS3-compatible\u201d in the sense that they use the same format as AWS S3 does. They are never valid against AWS S3 itself.","title":"Creating credentials"},{"location":"howto/object-storage/s3/credentials/#listing-credentials","text":"You can list any previously-created credentials with: openstack ec2 credentials list","title":"Listing credentials"},{"location":"howto/object-storage/s3/credentials/#configuring-your-s3-api-client","text":"Once you have obtained your S3-compatible access and secret key, you need to configure your S3 client with it. How exactly you do that depends on your preferred client: aws mc s3cmd Create a new profile, named after your Cleura region: aws configure set \\ --profile <region> \\ aws_access_key_id <access-key> aws configure set \\ --profile <region> \\ aws_secret_access_key <secret-key> For the aws CLI, you cannot define a region\u2019s endpoint in the profile. As such, you must add the --endpoint-url=https://s3-<region>.citycloud.com:8080 option to each aws s3api call. Create a new alias, named after your Cleura region: mc alias set <region> \\ https://s3-<region>.citycloud.com:8080 \\ <access-key> <secret-key> Once you have configured an alias like this, you are able to run bucket operations with mc using the alias/bucket syntax. s3cmd does not support configuration profiles, so you need to use a separate configuration file for each Cleura region you want to use: s3cmd -c ~/.s3cfg-<region> --configure Set your Access Key and Secret Key when prompted. Leave Default Region unchanged. Set S3 Endpoint to s3-<region>.citycloud.com:8080 . Set DNS-style bucket+hostname:port template for accessing a bucket to s3-<region>.citycloud.com:8080 as well. Set Use HTTPS protocol to Yes (the default). Configure GnuPG encryption and your HTTP proxy server, if needed. Test access with your supplied credentials. On subsequent invocations of the s3cmd CLI, always add the -c ~/.s3cfg-<region> option.","title":"Configuring your S3 API client"},{"location":"howto/object-storage/s3/credentials/#deleting-credentials","text":"If at any time you need to delete a set of AWS-compatible credentials, you can do so with the following command: openstack ec2 credentials delete <access-key-id> Deleting a set of S3-compatible credentials will immediately revoke access for any applications that were using it.","title":"Deleting credentials"},{"location":"howto/object-storage/s3/expiry/","text":"Object expiry Object expiry requires that you configure your environment with working S3-compatible credentials . You can set a bucket\u2019s lifecycle configuration such that it automatically deletes objects after a certain number of days. First, you need to create a JSON file, lifecycle.json , that contains the lifecycle configuration rule. Be sure to set Days to your desired value: { \"Rules\" : [{ \"ID\" : \"cleanup\" , \"Status\" : \"Enabled\" , \"Prefix\" : \"\" , \"Expiration\" : { \"Days\" : 5 } }] } Then, apply this lifecycle configuration to your bucket using one of the following commands: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api put-bucket-lifecycle-configuration \\ --lifecycle-configuration file://lifecycle.json \\ --bucket <bucket-name> mc ilm import <region>/<bucket-name> < lifecycle.json s3cmd -c ~/.s3cfg-<region> setlifecycle lifecycle.json s3://<bucket-name>","title":"Object expiry"},{"location":"howto/object-storage/s3/expiry/#object-expiry","text":"Object expiry requires that you configure your environment with working S3-compatible credentials . You can set a bucket\u2019s lifecycle configuration such that it automatically deletes objects after a certain number of days. First, you need to create a JSON file, lifecycle.json , that contains the lifecycle configuration rule. Be sure to set Days to your desired value: { \"Rules\" : [{ \"ID\" : \"cleanup\" , \"Status\" : \"Enabled\" , \"Prefix\" : \"\" , \"Expiration\" : { \"Days\" : 5 } }] } Then, apply this lifecycle configuration to your bucket using one of the following commands: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api put-bucket-lifecycle-configuration \\ --lifecycle-configuration file://lifecycle.json \\ --bucket <bucket-name> mc ilm import <region>/<bucket-name> < lifecycle.json s3cmd -c ~/.s3cfg-<region> setlifecycle lifecycle.json s3://<bucket-name>","title":"Object expiry"},{"location":"howto/object-storage/s3/sse-c/","text":"Client-side encryption (SSE-C) You can use objects encryption via S3 API, according to the Amazon SSE-C specification. This means that you need to provide an encryption/decryption key with each request to the object. You can store the encryption key in Barbican, and provide it to the S3 client at runtime. Below we will provide more detailed explanation regarding how to use this in your workloads. Requirements This guide assumes familiarity with the following tools: python-openstackclient (with the python-barbicanclient plugin), pwgen , rclone version 1.54 or later. Create encryption details According to SSE-C specification, in order to use server-side encryption, any S3 client needs to provide three pieces of information, which it includes in the request headers for each S3 request being made: Encryption algorithm: the only valid option here is AES256. Encryption key: a generated random key that we will store in Barbican. It should be valid AES key, which means that key length must be 32 bytes. Encryption key checksum: MD5 checksum of the encryption key. It\u2019s used for integrity checks. In order to generate encryption key and store it in Barbican, proceed as follows. Generate secret: secret_raw=$(pwgen 32 1) Store secret in Barbican: barbican_secret_url=$(openstack secret store --name objectSecret --algorithm aes --bit-length 256 --payload ${secret_raw} -f value -c 'Secret href') Retrieve secret from Barbican: secret=$(openstack secret get ${barbican_secret_url} -p -c Payload -f value) Managing encrypted objects in S3 (with rclone ) SSE-C encryption has been implemented/fixed with version 1.54. Prior rclone versions won\u2019t work here. Download and install the latest rclone for your distribution: https://rclone.org/downloads/ Create or retrieve your access key ID and secret key . Create a configuration file named ~/.rclone.conf , with the following content: [cleura] type = s3 provider = Ceph env_auth = false access_key_id = <access key id> secret_access_key = <secret key> endpoint = <region>.citycloud.com:8080 acl = private sse_customer_algorithm = AES256 Create an S3 bucket: rclone mkdir cleura:encrypted Sync a directory to the S3 bucket, encrypting the files it contains on upload: rclone sync ~/media/ cleura:encrypted \\ --s3-sse-customer-key=${secret} Retrieve a file from S3 and decrypt it: rclone copy cleura:encrypted/file.png \\ --s3-sse-customer-key=${secret} For more examples on how to use rclone, please use its reference documentation: https://rclone.org/docs/#subcommands","title":"Client-side encryption (SSE-C)"},{"location":"howto/object-storage/s3/sse-c/#client-side-encryption-sse-c","text":"You can use objects encryption via S3 API, according to the Amazon SSE-C specification. This means that you need to provide an encryption/decryption key with each request to the object. You can store the encryption key in Barbican, and provide it to the S3 client at runtime. Below we will provide more detailed explanation regarding how to use this in your workloads.","title":"Client-side encryption (SSE-C)"},{"location":"howto/object-storage/s3/sse-c/#requirements","text":"This guide assumes familiarity with the following tools: python-openstackclient (with the python-barbicanclient plugin), pwgen , rclone version 1.54 or later.","title":"Requirements"},{"location":"howto/object-storage/s3/sse-c/#create-encryption-details","text":"According to SSE-C specification, in order to use server-side encryption, any S3 client needs to provide three pieces of information, which it includes in the request headers for each S3 request being made: Encryption algorithm: the only valid option here is AES256. Encryption key: a generated random key that we will store in Barbican. It should be valid AES key, which means that key length must be 32 bytes. Encryption key checksum: MD5 checksum of the encryption key. It\u2019s used for integrity checks. In order to generate encryption key and store it in Barbican, proceed as follows. Generate secret: secret_raw=$(pwgen 32 1) Store secret in Barbican: barbican_secret_url=$(openstack secret store --name objectSecret --algorithm aes --bit-length 256 --payload ${secret_raw} -f value -c 'Secret href') Retrieve secret from Barbican: secret=$(openstack secret get ${barbican_secret_url} -p -c Payload -f value)","title":"Create encryption details"},{"location":"howto/object-storage/s3/sse-c/#managing-encrypted-objects-in-s3-with-rclone","text":"SSE-C encryption has been implemented/fixed with version 1.54. Prior rclone versions won\u2019t work here. Download and install the latest rclone for your distribution: https://rclone.org/downloads/ Create or retrieve your access key ID and secret key . Create a configuration file named ~/.rclone.conf , with the following content: [cleura] type = s3 provider = Ceph env_auth = false access_key_id = <access key id> secret_access_key = <secret key> endpoint = <region>.citycloud.com:8080 acl = private sse_customer_algorithm = AES256 Create an S3 bucket: rclone mkdir cleura:encrypted Sync a directory to the S3 bucket, encrypting the files it contains on upload: rclone sync ~/media/ cleura:encrypted \\ --s3-sse-customer-key=${secret} Retrieve a file from S3 and decrypt it: rclone copy cleura:encrypted/file.png \\ --s3-sse-customer-key=${secret} For more examples on how to use rclone, please use its reference documentation: https://rclone.org/docs/#subcommands","title":"Managing encrypted objects in S3 (with rclone)"},{"location":"howto/object-storage/s3/versioning/","text":"Object versioning Object versioning requires that you configure your environment with working S3-compatible credentials . Enabling bucket versioning To enable versioning in a bucket, use one of the following commands: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api put-bucket-versioning \\ --versioning-configuration Status = Enabled \\ --bucket <bucket-name> mc version enable <region>/<bucket-name> This functionality is not available with the s3cmd command. Checking bucket versioning status To check whether object versioning is enabled on a bucket, use one of the following commands: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api get-bucket-versioning \\ --bucket <bucket-name> mc version info <region>/<bucket-name> This functionality is not available with the s3cmd command. Creating a versioned object Once object versioning is enabled on a bucket, the normal object creation and replacement commands behave in a manner different from that in unversioned buckets: If the object does not already exist, it is created (as in an unversioned bucket). If the object does already exist, it is not replaced. Instead, a new version appears in addition to the old one. aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api put-object \\ --bucket <bucket-name> \\ --key <object-name> \\ --body <local-filename> mc cp \\ <local-filename> \\ <region>/<bucket-name>/<object-name> s3cmd put <local-filename> s3://<bucket> Listing object versions In a bucket that has versioning enabled, you may list the versions available for an object: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api list-object-versions \\ --bucket <bucket-name> \\ --key <object-name> mc stat --versions <region>/<bucket-name> This functionality may be impacted by bugs in several versions of the mc client. This functionality is not available with the s3cmd command. Retrieving a versioned object To download a specific version of an object in a bucket, use one of the following commands: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api get-object \\ --bucket <bucket-name> \\ --key <object-name> \\ --version-id <versionid> \\ <local-filename> mc cp \\ --version-id <versionid> \\ <region>/<bucket-name>/<object-name> \\ <local-filename> This functionality is not available with the s3cmd command. When you download an object from a versioned bucket without specifying a version identifier, your S3 client will download the latest version of that object. Deleting a versioned object Like the commands to create objects, the commands to delete them behave differently once object versioning is enabled on a bucket. The command to delete an object will normally not delete it, but revert it to the prior version. The exception to this rule is when there is only a single version of the object left in the bucket, in which case object removal does occur. aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api delete-object \\ --bucket <bucket-name> \\ --key <object-name> mc rm \\ <region>/<bucket-name>/<object-name> s3cmd del s3://<bucket-name>/<object-name> You also have the option of deleting not the latest version, but a specific object version: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api delete-object \\ --version-id <versionid> \\ --bucket <bucket-name> \\ --key <object-name> mc rm \\ --version-id <versionid> \\ <region>/<bucket-name>/<object-name> This functionality is not available with the s3cmd command.","title":"Object versioning"},{"location":"howto/object-storage/s3/versioning/#object-versioning","text":"Object versioning requires that you configure your environment with working S3-compatible credentials .","title":"Object versioning"},{"location":"howto/object-storage/s3/versioning/#enabling-bucket-versioning","text":"To enable versioning in a bucket, use one of the following commands: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api put-bucket-versioning \\ --versioning-configuration Status = Enabled \\ --bucket <bucket-name> mc version enable <region>/<bucket-name> This functionality is not available with the s3cmd command.","title":"Enabling bucket versioning"},{"location":"howto/object-storage/s3/versioning/#checking-bucket-versioning-status","text":"To check whether object versioning is enabled on a bucket, use one of the following commands: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api get-bucket-versioning \\ --bucket <bucket-name> mc version info <region>/<bucket-name> This functionality is not available with the s3cmd command.","title":"Checking bucket versioning status"},{"location":"howto/object-storage/s3/versioning/#creating-a-versioned-object","text":"Once object versioning is enabled on a bucket, the normal object creation and replacement commands behave in a manner different from that in unversioned buckets: If the object does not already exist, it is created (as in an unversioned bucket). If the object does already exist, it is not replaced. Instead, a new version appears in addition to the old one. aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api put-object \\ --bucket <bucket-name> \\ --key <object-name> \\ --body <local-filename> mc cp \\ <local-filename> \\ <region>/<bucket-name>/<object-name> s3cmd put <local-filename> s3://<bucket>","title":"Creating a versioned object"},{"location":"howto/object-storage/s3/versioning/#listing-object-versions","text":"In a bucket that has versioning enabled, you may list the versions available for an object: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api list-object-versions \\ --bucket <bucket-name> \\ --key <object-name> mc stat --versions <region>/<bucket-name> This functionality may be impacted by bugs in several versions of the mc client. This functionality is not available with the s3cmd command.","title":"Listing object versions"},{"location":"howto/object-storage/s3/versioning/#retrieving-a-versioned-object","text":"To download a specific version of an object in a bucket, use one of the following commands: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api get-object \\ --bucket <bucket-name> \\ --key <object-name> \\ --version-id <versionid> \\ <local-filename> mc cp \\ --version-id <versionid> \\ <region>/<bucket-name>/<object-name> \\ <local-filename> This functionality is not available with the s3cmd command. When you download an object from a versioned bucket without specifying a version identifier, your S3 client will download the latest version of that object.","title":"Retrieving a versioned object"},{"location":"howto/object-storage/s3/versioning/#deleting-a-versioned-object","text":"Like the commands to create objects, the commands to delete them behave differently once object versioning is enabled on a bucket. The command to delete an object will normally not delete it, but revert it to the prior version. The exception to this rule is when there is only a single version of the object left in the bucket, in which case object removal does occur. aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api delete-object \\ --bucket <bucket-name> \\ --key <object-name> mc rm \\ <region>/<bucket-name>/<object-name> s3cmd del s3://<bucket-name>/<object-name> You also have the option of deleting not the latest version, but a specific object version: aws mc s3cmd aws --profile <region> \\ --endpoint-url = https://s3-<region>.citycloud.com:8080 \\ s3api delete-object \\ --version-id <versionid> \\ --bucket <bucket-name> \\ --key <object-name> mc rm \\ --version-id <versionid> \\ <region>/<bucket-name>/<object-name> This functionality is not available with the s3cmd command.","title":"Deleting a versioned object"},{"location":"howto/openstack/barbican/","text":"Using Barbican for secret storage Barbican is OpenStack\u2019s secret storage facility. In Cleura, Barbican is supported for the following purposes: Generic secret storage , encryption for persistent volumes , certificate storage for HTTPS load balancers . To manage secrets with Barbican, you will need the openstack command line interface (CLI), and its Barbican plugin. You can install them both with the following commands: pip install python-openstackclient python-barbicanclient On Debian/Ubuntu platforms, you may also install these utilities via their APT packages: apt install python3-openstackclient python3-barbicanclient","title":"Using Barbican for secret storage"},{"location":"howto/openstack/barbican/#using-barbican-for-secret-storage","text":"Barbican is OpenStack\u2019s secret storage facility. In Cleura, Barbican is supported for the following purposes: Generic secret storage , encryption for persistent volumes , certificate storage for HTTPS load balancers . To manage secrets with Barbican, you will need the openstack command line interface (CLI), and its Barbican plugin. You can install them both with the following commands: pip install python-openstackclient python-barbicanclient On Debian/Ubuntu platforms, you may also install these utilities via their APT packages: apt install python3-openstackclient python3-barbicanclient","title":"Using Barbican for secret storage"},{"location":"howto/openstack/barbican/generic-secret/","text":"Generic secret storage The simplest way to use Barbican is to create and retrieve a securely stored, generic secret. How to store a generic secret It is possible to store any secret data with Barbican. The command below will create a secret of the type passphrase , named mysecret , which contains the passphrase my very secret passphrase . openstack secret store \\ --secret-type passphrase \\ -p \"my very secret passphrase\" \\ -n mysecret The example output below uses Cleura\u2019s Fra1 region. In other regions, the secret URIs will differ. +---------------+--------------------------------------------------------------------------------+ | Field | Value | +---------------+--------------------------------------------------------------------------------+ | Secret href | https://fra1.citycloud.com:9311/v1/secrets/33ef0985-f89e-4bf0-b318-887ecac0cba | | Name | mysecret | | Created | None | | Status | None | | Content types | None | | Algorithm | aes | | Bit length | 256 | | Secret type | passphrase | | Mode | cbc | | Expiration | None | +---------------+--------------------------------------------------------------------------------+ Note that passphrase type secrets are symmetrically encrypted, using the AES encryption algorithm with a 256-bit key length. You can select other bit lengths and algorithms with the -b and -a command line options, if desired. How to retrieve secrets Secrets are stored in Barbican in an encrypted format. You can see a list of secrets created for your user with the following command: openstack secret list +--------------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------------------+-----------+------------+-------------+------+------------+ | Secret href | Name | Created | Status | Content types | Algorithm | Bit length | Secret type | Mode | Expiration | +--------------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------------------+-----------+------------+-------------+------+------------+ | https://fra1.citycloud.com:9311/v1/secrets/33ef0985-f89e-4bf0-b318-887ecac0cba | mysecret | 2021-04-29T10:33:18+00:00 | ACTIVE | {'default': 'application/octet-stream'} | aes | 256 | passphrase | cbc | None | | https://fra1.citycloud.com:9311/v1/secrets/ad628532-53b8-4d2f-91e5-0097b51da4e | None | 2021-04-27T13:52:10+00:00 | ACTIVE | {'default': 'application/octet-stream'} | aes | 256 | symmetric | None | None | +--------------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------------------+-----------+------------+-------------+------+------------+ You can retrieve the decrypted secret with the openstack secret get command, adding the -p (or --payload ) option: $ openstack secret get -p \\ https://fra1.citycloud.com:9311/v1/secrets/33ef0985-f89e-4bf0-b318-887ecac0cba +---------+---------------------------+ | Field | Value | +---------+---------------------------+ | Payload | my very secret passphrase | +---------+---------------------------+ Unlike many other OpenStack services, which allow you to retrieve object references by name or UUID, Barbican only lets you retrieve secrets by their full URI . That URI must include the https://<region>.citycloud.com:9311/v1/secrets/ prefix.","title":"Generic secret storage"},{"location":"howto/openstack/barbican/generic-secret/#generic-secret-storage","text":"The simplest way to use Barbican is to create and retrieve a securely stored, generic secret.","title":"Generic secret storage"},{"location":"howto/openstack/barbican/generic-secret/#how-to-store-a-generic-secret","text":"It is possible to store any secret data with Barbican. The command below will create a secret of the type passphrase , named mysecret , which contains the passphrase my very secret passphrase . openstack secret store \\ --secret-type passphrase \\ -p \"my very secret passphrase\" \\ -n mysecret The example output below uses Cleura\u2019s Fra1 region. In other regions, the secret URIs will differ. +---------------+--------------------------------------------------------------------------------+ | Field | Value | +---------------+--------------------------------------------------------------------------------+ | Secret href | https://fra1.citycloud.com:9311/v1/secrets/33ef0985-f89e-4bf0-b318-887ecac0cba | | Name | mysecret | | Created | None | | Status | None | | Content types | None | | Algorithm | aes | | Bit length | 256 | | Secret type | passphrase | | Mode | cbc | | Expiration | None | +---------------+--------------------------------------------------------------------------------+ Note that passphrase type secrets are symmetrically encrypted, using the AES encryption algorithm with a 256-bit key length. You can select other bit lengths and algorithms with the -b and -a command line options, if desired.","title":"How to store a generic secret"},{"location":"howto/openstack/barbican/generic-secret/#how-to-retrieve-secrets","text":"Secrets are stored in Barbican in an encrypted format. You can see a list of secrets created for your user with the following command: openstack secret list +--------------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------------------+-----------+------------+-------------+------+------------+ | Secret href | Name | Created | Status | Content types | Algorithm | Bit length | Secret type | Mode | Expiration | +--------------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------------------+-----------+------------+-------------+------+------------+ | https://fra1.citycloud.com:9311/v1/secrets/33ef0985-f89e-4bf0-b318-887ecac0cba | mysecret | 2021-04-29T10:33:18+00:00 | ACTIVE | {'default': 'application/octet-stream'} | aes | 256 | passphrase | cbc | None | | https://fra1.citycloud.com:9311/v1/secrets/ad628532-53b8-4d2f-91e5-0097b51da4e | None | 2021-04-27T13:52:10+00:00 | ACTIVE | {'default': 'application/octet-stream'} | aes | 256 | symmetric | None | None | +--------------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------------------+-----------+------------+-------------+------+------------+ You can retrieve the decrypted secret with the openstack secret get command, adding the -p (or --payload ) option: $ openstack secret get -p \\ https://fra1.citycloud.com:9311/v1/secrets/33ef0985-f89e-4bf0-b318-887ecac0cba +---------+---------------------------+ | Field | Value | +---------+---------------------------+ | Payload | my very secret passphrase | +---------+---------------------------+ Unlike many other OpenStack services, which allow you to retrieve object references by name or UUID, Barbican only lets you retrieve secrets by their full URI . That URI must include the https://<region>.citycloud.com:9311/v1/secrets/ prefix.","title":"How to retrieve secrets"},{"location":"howto/openstack/barbican/share-secret/","text":"Sharing secrets via ACLs Normally, a Barbican secret is only available to the OpenStack API user that created it. However, under some circumstances it may be desirable to make a secret available to another user. To do so, you will need the secret\u2019s URI , the other user\u2019s OpenStack API user ID. Any Cleura user can always retrieve their own user ID with the following command: openstack token issue -f value -c user_id Once you have assembled this information, you can proceed with the openstack acl user add command: openstack acl user add \\ --user <user_id> \\ --operation-type read \\ https://region.citycloud.com:9311/v1/secrets/<secret_id> If you want to unshare the secret again, you simply use the corresponding openstack acl user remove command: openstack acl user remove \\ --user <user_id> \\ --operation-type read \\ https://region.citycloud.com:9311/v1/secrets/<secret_id>","title":"Sharing secrets via ACLs"},{"location":"howto/openstack/barbican/share-secret/#sharing-secrets-via-acls","text":"Normally, a Barbican secret is only available to the OpenStack API user that created it. However, under some circumstances it may be desirable to make a secret available to another user. To do so, you will need the secret\u2019s URI , the other user\u2019s OpenStack API user ID. Any Cleura user can always retrieve their own user ID with the following command: openstack token issue -f value -c user_id Once you have assembled this information, you can proceed with the openstack acl user add command: openstack acl user add \\ --user <user_id> \\ --operation-type read \\ https://region.citycloud.com:9311/v1/secrets/<secret_id> If you want to unshare the secret again, you simply use the corresponding openstack acl user remove command: openstack acl user remove \\ --user <user_id> \\ --operation-type read \\ https://region.citycloud.com:9311/v1/secrets/<secret_id>","title":"Sharing secrets via ACLs"},{"location":"howto/openstack/cinder/encrypted-volumes/","text":"Encrypted volumes When using Barbican for block storage encryption, you ensure that data in persistent storage volumes is stored in an encrypted fashion. That encryption is transparent to virtual machines (instances) that you attach the volume to. Creating an encrypted volume For the creation of an encrypted volume, you need to provide a specific volume type. You can retrieve the list of available volume types with the following command: openstack volume type list +--------------------------------------+-----------------------+-----------+ | ID | Name | Is Public | +--------------------------------------+-----------------------+-----------+ | a479a6b0-b283-41a5-b38b-5b08e7f902ca | volumes_hdd_encrypted | True | | d9dfa98a-238d-4ca0-9abf-701fceb05623 | __DEFAULT__ | True | | 86796611-fb12-4628-b6b1-e09469e301d7 | volumes_hdd | True | +--------------------------------------+-----------------------+-----------+ In Cleura, all volume types that support encryption use the suffix _encrypted . To create a volume with encryption, you need to explicitly specify the --type option to the openstack volume create command. The following example creates a volume using the volumes_hdd_encrypted type, naming it enc_drive and setting its size to 10 GiB: openstack volume create \\ --type volumes_hdd_encrypted \\ --size 10 \\ enc_drive +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2021-04-27T13:52:10.000000 | | description | None | | encrypted | True | | id | 33211b21-8d4f-48e9-b76f-ec73ffd19def | | multiattach | False | | name | enc_drive | | properties | | | replication_status | None | | size | 10 | | snapshot_id | None | | source_volid | None | | status | creating | | type | volumes_hdd_encrypted | | updated_at | None | | user_id | 966ad341f4e14920b5f589f900246ccc | +---------------------+--------------------------------------+ Upon volume creation, this will create a one-off encryption key, which is stored in Barbican and applies to this one volume only. In other words, the key created for this volume will be unable to decrypt any other volumes except the one it was created for. Retrieving a volume\u2019s encryption key Once you have created an encrypted volume, you may retrieve a reference to the Barbican secret that represents its encryption key. You do this with the following command: openstack volume show \\ --os-volume-api-version 3 .66 \\ -f value \\ -c encryption_key_id \\ enc_drive Instead of the volume name, you can of course also specify its UUID: openstack volume show \\ --os-volume-api-version 3 .66 \\ -f value \\ -c encryption_key_id \\ 33211b21-8d4f-48e9-b76f-ec73ffd19def Deleting an encrypted volume When you decide you no longer need an encrypted volume and want to delete it, you can do so with the openstack volume delete command. As long as you do this with the same user account as the one that created the volume, this will succeed without further intervention. However, if you are trying to delete a volume that was created by a different user, you\u2019ll run into the limitation that the secret associated with the volume is owned by that user. As a result, the deletion of the encrypted volume using your own user credentials will fail. There are two options to work around this limitation: You can switch to the user credentials of the user that created the volume (if you have access to them), and proceed with the deletion. You can ask the user that created the volume to add you to the Access Control List (ACL) for the secret . This will enable you to read the secret, and to delete the volume using your own credentials. Block device encryption caveats Once a volume is configured to use encryption and is also attached to an instance in Cleura, some caveats apply that you might want to keep in mind. Sometimes, automatically or through administrator intervention, we move one of your instances to another physical machine. This process is known as live migration, and it normally does not interrupt the instance\u2019s functionality at all \u2014 typically, neither you nor the application users notice that live migration has even happened. This is a very common occurrence when we do routine upgrades of the Cleura platform, during our pre-announced maintenance windows. The same considerations apply to physical node failure. If the physical machine running your instance fails, we can automatically recover it onto another machine \u2014 an action known as evacuation. Live migration or evacuation including encrypted volumes does, however, require that whoever does the migration also has at least read access to the volume\u2019s encryption secret. This means that you have two options: If you do trust us to include your instances in live migrations and evacuations, even if they attach encrypted volumes, then you can add our administrative account to the Access Control List (ACL) for your secrets . If you don\u2019t want to share your secrets but you still want to use encrypted volumes, you should build your own mechanism or process (preferably automated) so that your instances recover in case they become non-functional.","title":"Encrypted volumes"},{"location":"howto/openstack/cinder/encrypted-volumes/#encrypted-volumes","text":"When using Barbican for block storage encryption, you ensure that data in persistent storage volumes is stored in an encrypted fashion. That encryption is transparent to virtual machines (instances) that you attach the volume to.","title":"Encrypted volumes"},{"location":"howto/openstack/cinder/encrypted-volumes/#creating-an-encrypted-volume","text":"For the creation of an encrypted volume, you need to provide a specific volume type. You can retrieve the list of available volume types with the following command: openstack volume type list +--------------------------------------+-----------------------+-----------+ | ID | Name | Is Public | +--------------------------------------+-----------------------+-----------+ | a479a6b0-b283-41a5-b38b-5b08e7f902ca | volumes_hdd_encrypted | True | | d9dfa98a-238d-4ca0-9abf-701fceb05623 | __DEFAULT__ | True | | 86796611-fb12-4628-b6b1-e09469e301d7 | volumes_hdd | True | +--------------------------------------+-----------------------+-----------+ In Cleura, all volume types that support encryption use the suffix _encrypted . To create a volume with encryption, you need to explicitly specify the --type option to the openstack volume create command. The following example creates a volume using the volumes_hdd_encrypted type, naming it enc_drive and setting its size to 10 GiB: openstack volume create \\ --type volumes_hdd_encrypted \\ --size 10 \\ enc_drive +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2021-04-27T13:52:10.000000 | | description | None | | encrypted | True | | id | 33211b21-8d4f-48e9-b76f-ec73ffd19def | | multiattach | False | | name | enc_drive | | properties | | | replication_status | None | | size | 10 | | snapshot_id | None | | source_volid | None | | status | creating | | type | volumes_hdd_encrypted | | updated_at | None | | user_id | 966ad341f4e14920b5f589f900246ccc | +---------------------+--------------------------------------+ Upon volume creation, this will create a one-off encryption key, which is stored in Barbican and applies to this one volume only. In other words, the key created for this volume will be unable to decrypt any other volumes except the one it was created for.","title":"Creating an encrypted volume"},{"location":"howto/openstack/cinder/encrypted-volumes/#retrieving-a-volumes-encryption-key","text":"Once you have created an encrypted volume, you may retrieve a reference to the Barbican secret that represents its encryption key. You do this with the following command: openstack volume show \\ --os-volume-api-version 3 .66 \\ -f value \\ -c encryption_key_id \\ enc_drive Instead of the volume name, you can of course also specify its UUID: openstack volume show \\ --os-volume-api-version 3 .66 \\ -f value \\ -c encryption_key_id \\ 33211b21-8d4f-48e9-b76f-ec73ffd19def","title":"Retrieving a volume\u2019s encryption key"},{"location":"howto/openstack/cinder/encrypted-volumes/#deleting-an-encrypted-volume","text":"When you decide you no longer need an encrypted volume and want to delete it, you can do so with the openstack volume delete command. As long as you do this with the same user account as the one that created the volume, this will succeed without further intervention. However, if you are trying to delete a volume that was created by a different user, you\u2019ll run into the limitation that the secret associated with the volume is owned by that user. As a result, the deletion of the encrypted volume using your own user credentials will fail. There are two options to work around this limitation: You can switch to the user credentials of the user that created the volume (if you have access to them), and proceed with the deletion. You can ask the user that created the volume to add you to the Access Control List (ACL) for the secret . This will enable you to read the secret, and to delete the volume using your own credentials.","title":"Deleting an encrypted volume"},{"location":"howto/openstack/cinder/encrypted-volumes/#block-device-encryption-caveats","text":"Once a volume is configured to use encryption and is also attached to an instance in Cleura, some caveats apply that you might want to keep in mind. Sometimes, automatically or through administrator intervention, we move one of your instances to another physical machine. This process is known as live migration, and it normally does not interrupt the instance\u2019s functionality at all \u2014 typically, neither you nor the application users notice that live migration has even happened. This is a very common occurrence when we do routine upgrades of the Cleura platform, during our pre-announced maintenance windows. The same considerations apply to physical node failure. If the physical machine running your instance fails, we can automatically recover it onto another machine \u2014 an action known as evacuation. Live migration or evacuation including encrypted volumes does, however, require that whoever does the migration also has at least read access to the volume\u2019s encryption secret. This means that you have two options: If you do trust us to include your instances in live migrations and evacuations, even if they attach encrypted volumes, then you can add our administrative account to the Access Control List (ACL) for your secrets . If you don\u2019t want to share your secrets but you still want to use encrypted volumes, you should build your own mechanism or process (preferably automated) so that your instances recover in case they become non-functional.","title":"Block device encryption caveats"},{"location":"howto/openstack/magnum/new-k8s-cluster/","text":"Creating new Kubernetes clusters By employing OpenStack Magnum you can easily create Kubernetes clusters over OpenStack, using either the Cleura Cloud Management Panel or the OpenStack CLI. Let us demonstrate the creation of a Kubernetes cluster following both approaches. Prerequisites First and foremost, you need an account in Cleura Cloud . If you prefer to work with the OpenStack CLI, go ahead and enable it first . Then, in addition to the Python openstackclient module, make sure you also install the corresponding plugin module for Magnum. Use either the package manager of your operating system or pip : Debian/Ubuntu Mac OS X with Homebrew Python Package apt install python3-magnumclient This Python module is unavailable via brew , but you can install it via pip . pip install python-magnumclient Creating a Kubernetes cluster To create a Kubernetes cluster from the Cleura Cloud Management Panel, fire up your favorite web browser, navigate to the Cleura Cloud page, and log into your Cleura account. On the other hand, if you prefer to work with the OpenStack CLI, please do not forget to source the RC file first . Cleura Cloud Management Panel OpenStack CLI On the top right-hand side of the Cleura Cloud Management Panel, click the Create button. A new pane titled Create will slide into view from the right-hand side of the browser window. You will notice several rounded boxes on that pane. Go ahead and click the one labeled Magnum Cluster . A new pane titled Create a Magnum Cluster will slide over. At the top, type in a name for the new Kubernetes cluster and select one of the available regions. Then, select one of the available templates to base the new cluster on. In the example below, we have selected the template named Kubernetes 1.18.6 on Fedora-coreos 33 2C-4GB-20GB No Master LB . The naming of any template indicates the version of Kubernetes that is about to be deployed, the characteristics of the cluster nodes, the operating system they run, and the presence or not of a load balancer for the control plane. Optionally, select one of the available keypairs for secure SSH access to the individual cluster nodes. For now, you may skip the Advanced Option section. Click the green Create button, and Magnum will start creating the new Kubernetes cluster. Please note that the whole process may take several minutes to complete. A simple, general command for creating a new Kubernetes cluster with Magnum looks like this: openstack coe cluster create \\ --cluster-template $CLUSTER_TMPL \\ --keypair $KEYPAIR \\ $CLUSTER_NAME Let us list all available templates in the region: openstack coe cluster template list +--------------------------------------+----------------------------------------------------------------+------+ | uuid | name | tags | +--------------------------------------+----------------------------------------------------------------+------+ | 3f476f01-b3de-4687-a188-6829ed947db0 | Kubernetes 1.15.5 on Fedora-atomic 29 4C-8GB-20GB No Master LB | None | | c458f02d-54b0-4ef8-abbc-e1c25b61165a | Kubernetes 1.15.5 on Fedora-atomic 29 2C-4GB-20GB No Master LB | None | | f9e1a2ea-b1ff-43e7-8d1e-6dd5861b82cf | Kubernetes 1.18.6 on Fedora-coreos 33 2C-4GB-20GB No Master LB | None | +--------------------------------------+----------------------------------------------------------------+------+ Select the template you want by setting the corresponding uuid value to the CLUSTER_TMPL variable: CLUSTER_TMPL = \"f9e1a2ea-b1ff-43e7-8d1e-6dd5861b82cf\" # just an example Then, list all available keypairs\u2026 openstack keypair list +---------+-------------------------------------------------+------+ | Name | Fingerprint | Type | +---------+-------------------------------------------------+------+ | husavik | 34:3b:58:ba:ec:95:f5:17:17:df:04:38:11:89:e6:3d | ssh | +---------+-------------------------------------------------+------+ \u2026and set the KEYPAIR variable to the name of the keypair you want: KEYPAIR = \"husavik\" # again, this is just an example Finally, decide on a name for your new Kubernetes cluster: CLUSTER_NAME = \"bangor\" With everything in place, go ahead and create your new Kubernetes cluster: openstack coe cluster create \\ --cluster-template $CLUSTER_TMPL \\ --keypair husavik bangor If everything went well with your request for a new cluster, on your terminal, you would see a message like the following: Request to create cluster e0df8c62-c6f6-4c7d-b67e-33e3606e9ab6 accepted The cluster creation process takes some time to complete, and while you are waiting, you can check if everything is progressing smoothly: openstack coe cluster list -c status If everything is going well, the message you will get will be CREATE_IN_PROGRESS . When Magnum has finished creating the cluster, the message will be CREATE_COMPLETE . Viewing the Kubernetes cluster After the Kubernetes cluster is ready, you may at any time view it and get detailed information about it. Cleura Cloud Management Panel OpenStack CLI In the left vertical pane of the Cleura Cloud Management Panel, click through Kubernetes , Magnum , and Clusters . In the central pane on the right, you will then see all your Kubernetes clusters in every region. Click on the three-dot icon on the right of the cluster you want to inspect, and select View details . To list all available Kubernetes clusters, just type: openstack coe cluster list +---------------+--------+---------+------------+--------------+---------------+---------------+ | uuid | name | keypair | node_count | master_count | status | health_status | +---------------+--------+---------+------------+--------------+---------------+---------------+ | e0df8c62-c6f6 | bangor | husavik | 1 | 1 | CREATE_COMPLE | HEALTHY | | -4c7d-b67e-33 | | | | | TE | | | e3606e9ab6 | | | | | | | +---------------+--------+---------+------------+--------------+---------------+---------------+ For many more details on a specific cluster, note its name and run a command like this: openstack coe cluster show bangor +----------------------+---------------------------------------------------------------------------+ | Field | Value | +----------------------+---------------------------------------------------------------------------+ | status | CREATE_COMPLETE | | health_status | HEALTHY | | cluster_template_id | f9e1a2ea-b1ff-43e7-8d1e-6dd5861b82cf | | node_addresses | ['185.52.156.105'] | | uuid | e0df8c62-c6f6-4c7d-b67e-33e3606e9ab6 | | stack_id | e3725aed-f665-4e8d-9409-85f5ee5e2f4a | | status_reason | None | | created_at | 2022-11-14T07:32:02+00:00 | | updated_at | 2022-11-14T07:37:26+00:00 | | coe_version | v1.18.6 | | labels | {'kube_tag': 'v1.18.6', 'heat_container_agent_tag': 'train-stable'} | | labels_overridden | {} | | labels_skipped | {} | | labels_added | {} | | fixed_network | None | | fixed_subnet | None | | floating_ip_enabled | True | | faults | | | keypair | husavik | | api_address | https://89.46.80.136:6443 | | master_addresses | ['89.46.80.136'] | | master_lb_enabled | False | | create_timeout | 60 | | node_count | 1 | | discovery_url | https://discovery.etcd.io/23af721dc3ee773d2674db4881ff70cb | | docker_volume_size | 50 | | master_count | 1 | | container_version | 1.12.6 | | name | bangor | | master_flavor_id | 2C-4GB-20GB | | flavor_id | 2C-4GB-20GB | | health_status_reason | {'bangor-id6nijycp2wy-master-0.Ready': 'True', 'bangor-id6nijycp2wy- | | | node-0.Ready': 'True', 'api': 'ok'} | | project_id | dfc700467396428bacba4376e72cc3e9 | +----------------------+---------------------------------------------------------------------------+ Accessing the Kubernetes cluster with kubectl You may install the Kubernetes command line tool, kubectl , on your local computer, and run commands against your cluster. To install kubectl , use the package manager of your operating system. Debian/Ubuntu Mac OS X with Homebrew apt install kubectl brew install kubectl Before running commands against a specific cluster, you must have the corresponding config file on your computer. Cleura Cloud Management Panel OpenStack CLI Downloading a config file from the Cleura Cloud Management Panel is currently not supported. You can still fetch the config file of your newly created Kubernetes cluster using the OpenStack CLI. To download the config file for your Kubernetes cluster, type the following: openstack coe cluster config --dir = ${ PWD } bangor After saving the config file locally, set the value of variable KUBECONFIG to the full path of the file. Type, for example: export KUBECONFIG = ${ PWD } /config Then, you can use kubectl to run commands against your cluster. See, for instance, all cluster nodes\u2026 kubectl get nodes NAME STATUS ROLES AGE VERSION bangor-id6nijycp2wy-master-0 Ready master 113m v1.18.6 bangor-id6nijycp2wy-node-0 Ready <none> 111m v1.18.6 \u2026or all running pods in every namespace: kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-786ffb7797-tw2hg 1/1 Running 0 167m kube-system coredns-786ffb7797-vbqwn 1/1 Running 0 167m kube-system csi-cinder-controllerplugin-0 5/5 Running 0 167m kube-system csi-cinder-nodeplugin-4nr69 2/2 Running 0 166m kube-system csi-cinder-nodeplugin-vtwqf 2/2 Running 0 167m kube-system dashboard-metrics-scraper-6b4884c9d5-4mlrg 1/1 Running 0 167m kube-system k8s-keystone-auth-wk5v2 1/1 Running 0 167m kube-system kube-dns-autoscaler-75859754fd-2wsd9 1/1 Running 0 167m kube-system kube-flannel-ds-7z9dp 1/1 Running 0 167m kube-system kube-flannel-ds-dmvk6 1/1 Running 0 166m kube-system kubernetes-dashboard-c98496485-stn42 1/1 Running 0 167m kube-system magnum-metrics-server-79556d6999-xdlpm 1/1 Running 0 167m kube-system npd-5p6gk 1/1 Running 0 165m kube-system openstack-cloud-controller-manager-44rz9 1/1 Running 0 167m","title":"Creating new Kubernetes clusters"},{"location":"howto/openstack/magnum/new-k8s-cluster/#creating-new-kubernetes-clusters","text":"By employing OpenStack Magnum you can easily create Kubernetes clusters over OpenStack, using either the Cleura Cloud Management Panel or the OpenStack CLI. Let us demonstrate the creation of a Kubernetes cluster following both approaches.","title":"Creating new Kubernetes clusters"},{"location":"howto/openstack/magnum/new-k8s-cluster/#prerequisites","text":"First and foremost, you need an account in Cleura Cloud . If you prefer to work with the OpenStack CLI, go ahead and enable it first . Then, in addition to the Python openstackclient module, make sure you also install the corresponding plugin module for Magnum. Use either the package manager of your operating system or pip : Debian/Ubuntu Mac OS X with Homebrew Python Package apt install python3-magnumclient This Python module is unavailable via brew , but you can install it via pip . pip install python-magnumclient","title":"Prerequisites"},{"location":"howto/openstack/magnum/new-k8s-cluster/#creating-a-kubernetes-cluster","text":"To create a Kubernetes cluster from the Cleura Cloud Management Panel, fire up your favorite web browser, navigate to the Cleura Cloud page, and log into your Cleura account. On the other hand, if you prefer to work with the OpenStack CLI, please do not forget to source the RC file first . Cleura Cloud Management Panel OpenStack CLI On the top right-hand side of the Cleura Cloud Management Panel, click the Create button. A new pane titled Create will slide into view from the right-hand side of the browser window. You will notice several rounded boxes on that pane. Go ahead and click the one labeled Magnum Cluster . A new pane titled Create a Magnum Cluster will slide over. At the top, type in a name for the new Kubernetes cluster and select one of the available regions. Then, select one of the available templates to base the new cluster on. In the example below, we have selected the template named Kubernetes 1.18.6 on Fedora-coreos 33 2C-4GB-20GB No Master LB . The naming of any template indicates the version of Kubernetes that is about to be deployed, the characteristics of the cluster nodes, the operating system they run, and the presence or not of a load balancer for the control plane. Optionally, select one of the available keypairs for secure SSH access to the individual cluster nodes. For now, you may skip the Advanced Option section. Click the green Create button, and Magnum will start creating the new Kubernetes cluster. Please note that the whole process may take several minutes to complete. A simple, general command for creating a new Kubernetes cluster with Magnum looks like this: openstack coe cluster create \\ --cluster-template $CLUSTER_TMPL \\ --keypair $KEYPAIR \\ $CLUSTER_NAME Let us list all available templates in the region: openstack coe cluster template list +--------------------------------------+----------------------------------------------------------------+------+ | uuid | name | tags | +--------------------------------------+----------------------------------------------------------------+------+ | 3f476f01-b3de-4687-a188-6829ed947db0 | Kubernetes 1.15.5 on Fedora-atomic 29 4C-8GB-20GB No Master LB | None | | c458f02d-54b0-4ef8-abbc-e1c25b61165a | Kubernetes 1.15.5 on Fedora-atomic 29 2C-4GB-20GB No Master LB | None | | f9e1a2ea-b1ff-43e7-8d1e-6dd5861b82cf | Kubernetes 1.18.6 on Fedora-coreos 33 2C-4GB-20GB No Master LB | None | +--------------------------------------+----------------------------------------------------------------+------+ Select the template you want by setting the corresponding uuid value to the CLUSTER_TMPL variable: CLUSTER_TMPL = \"f9e1a2ea-b1ff-43e7-8d1e-6dd5861b82cf\" # just an example Then, list all available keypairs\u2026 openstack keypair list +---------+-------------------------------------------------+------+ | Name | Fingerprint | Type | +---------+-------------------------------------------------+------+ | husavik | 34:3b:58:ba:ec:95:f5:17:17:df:04:38:11:89:e6:3d | ssh | +---------+-------------------------------------------------+------+ \u2026and set the KEYPAIR variable to the name of the keypair you want: KEYPAIR = \"husavik\" # again, this is just an example Finally, decide on a name for your new Kubernetes cluster: CLUSTER_NAME = \"bangor\" With everything in place, go ahead and create your new Kubernetes cluster: openstack coe cluster create \\ --cluster-template $CLUSTER_TMPL \\ --keypair husavik bangor If everything went well with your request for a new cluster, on your terminal, you would see a message like the following: Request to create cluster e0df8c62-c6f6-4c7d-b67e-33e3606e9ab6 accepted The cluster creation process takes some time to complete, and while you are waiting, you can check if everything is progressing smoothly: openstack coe cluster list -c status If everything is going well, the message you will get will be CREATE_IN_PROGRESS . When Magnum has finished creating the cluster, the message will be CREATE_COMPLETE .","title":"Creating a Kubernetes cluster"},{"location":"howto/openstack/magnum/new-k8s-cluster/#viewing-the-kubernetes-cluster","text":"After the Kubernetes cluster is ready, you may at any time view it and get detailed information about it. Cleura Cloud Management Panel OpenStack CLI In the left vertical pane of the Cleura Cloud Management Panel, click through Kubernetes , Magnum , and Clusters . In the central pane on the right, you will then see all your Kubernetes clusters in every region. Click on the three-dot icon on the right of the cluster you want to inspect, and select View details . To list all available Kubernetes clusters, just type: openstack coe cluster list +---------------+--------+---------+------------+--------------+---------------+---------------+ | uuid | name | keypair | node_count | master_count | status | health_status | +---------------+--------+---------+------------+--------------+---------------+---------------+ | e0df8c62-c6f6 | bangor | husavik | 1 | 1 | CREATE_COMPLE | HEALTHY | | -4c7d-b67e-33 | | | | | TE | | | e3606e9ab6 | | | | | | | +---------------+--------+---------+------------+--------------+---------------+---------------+ For many more details on a specific cluster, note its name and run a command like this: openstack coe cluster show bangor +----------------------+---------------------------------------------------------------------------+ | Field | Value | +----------------------+---------------------------------------------------------------------------+ | status | CREATE_COMPLETE | | health_status | HEALTHY | | cluster_template_id | f9e1a2ea-b1ff-43e7-8d1e-6dd5861b82cf | | node_addresses | ['185.52.156.105'] | | uuid | e0df8c62-c6f6-4c7d-b67e-33e3606e9ab6 | | stack_id | e3725aed-f665-4e8d-9409-85f5ee5e2f4a | | status_reason | None | | created_at | 2022-11-14T07:32:02+00:00 | | updated_at | 2022-11-14T07:37:26+00:00 | | coe_version | v1.18.6 | | labels | {'kube_tag': 'v1.18.6', 'heat_container_agent_tag': 'train-stable'} | | labels_overridden | {} | | labels_skipped | {} | | labels_added | {} | | fixed_network | None | | fixed_subnet | None | | floating_ip_enabled | True | | faults | | | keypair | husavik | | api_address | https://89.46.80.136:6443 | | master_addresses | ['89.46.80.136'] | | master_lb_enabled | False | | create_timeout | 60 | | node_count | 1 | | discovery_url | https://discovery.etcd.io/23af721dc3ee773d2674db4881ff70cb | | docker_volume_size | 50 | | master_count | 1 | | container_version | 1.12.6 | | name | bangor | | master_flavor_id | 2C-4GB-20GB | | flavor_id | 2C-4GB-20GB | | health_status_reason | {'bangor-id6nijycp2wy-master-0.Ready': 'True', 'bangor-id6nijycp2wy- | | | node-0.Ready': 'True', 'api': 'ok'} | | project_id | dfc700467396428bacba4376e72cc3e9 | +----------------------+---------------------------------------------------------------------------+","title":"Viewing the Kubernetes cluster"},{"location":"howto/openstack/magnum/new-k8s-cluster/#accessing-the-kubernetes-cluster-with-kubectl","text":"You may install the Kubernetes command line tool, kubectl , on your local computer, and run commands against your cluster. To install kubectl , use the package manager of your operating system. Debian/Ubuntu Mac OS X with Homebrew apt install kubectl brew install kubectl Before running commands against a specific cluster, you must have the corresponding config file on your computer. Cleura Cloud Management Panel OpenStack CLI Downloading a config file from the Cleura Cloud Management Panel is currently not supported. You can still fetch the config file of your newly created Kubernetes cluster using the OpenStack CLI. To download the config file for your Kubernetes cluster, type the following: openstack coe cluster config --dir = ${ PWD } bangor After saving the config file locally, set the value of variable KUBECONFIG to the full path of the file. Type, for example: export KUBECONFIG = ${ PWD } /config Then, you can use kubectl to run commands against your cluster. See, for instance, all cluster nodes\u2026 kubectl get nodes NAME STATUS ROLES AGE VERSION bangor-id6nijycp2wy-master-0 Ready master 113m v1.18.6 bangor-id6nijycp2wy-node-0 Ready <none> 111m v1.18.6 \u2026or all running pods in every namespace: kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-786ffb7797-tw2hg 1/1 Running 0 167m kube-system coredns-786ffb7797-vbqwn 1/1 Running 0 167m kube-system csi-cinder-controllerplugin-0 5/5 Running 0 167m kube-system csi-cinder-nodeplugin-4nr69 2/2 Running 0 166m kube-system csi-cinder-nodeplugin-vtwqf 2/2 Running 0 167m kube-system dashboard-metrics-scraper-6b4884c9d5-4mlrg 1/1 Running 0 167m kube-system k8s-keystone-auth-wk5v2 1/1 Running 0 167m kube-system kube-dns-autoscaler-75859754fd-2wsd9 1/1 Running 0 167m kube-system kube-flannel-ds-7z9dp 1/1 Running 0 167m kube-system kube-flannel-ds-dmvk6 1/1 Running 0 166m kube-system kubernetes-dashboard-c98496485-stn42 1/1 Running 0 167m kube-system magnum-metrics-server-79556d6999-xdlpm 1/1 Running 0 167m kube-system npd-5p6gk 1/1 Running 0 165m kube-system openstack-cloud-controller-manager-44rz9 1/1 Running 0 167m","title":"Accessing the Kubernetes cluster with kubectl"},{"location":"howto/openstack/neutron/create-security-groups/","text":"Creating security groups By definition , security groups are \u201d[\u2026] sets of IP filter rules that are applied to all project instances, which define networking access to the instance. Group rules are project specific; project members can edit the default rules for their group and add new rule sets.\u201d Creating a security group Navigate to the Cleura Cloud Management Panel page, and log into your Cleura account. On the other hand, if you prefer to work with the OpenStack CLI, please do not forget to source the RC file first . Cleura Cloud Management Panel OpenStack CLI To create a security group click on Security Groups in the left-hand navigation menu: and then click on Create new Security Group in the top-right corner: An alternative way to create a Security Group is by clicking on Create \u2026 button in the top bar. Now give the security group a name and description, and choose in which region to create it, then click create : To create a security group use the following command: openstack security group create <name> When the command is executed successfully, you will get information regarding your new security group: +-----------------+--------------------------------------------------------------------------------+ | Field | Value | +-----------------+--------------------------------------------------------------------------------+ | created_at | 2022-11-14T09:15:14Z | | description | <name> | | id | 736da1d1-aa98-4da4-9ba4-2ab9aeea6a57 | | name | <name> | | project_id | cb43f189f7904fb88f3bbcfa22653ab8 | | revision_number | 1 | | rules | created_at='2022-11-14T09:15:14Z', direction='egress', ethertype='IPv4', | | | id='1f4c57cb-8e34-420c-a7e3-3b5625c79481', standard_attr_id='10579829', | | | updated_at='2022-11-14T09:15:14Z' | | | created_at='2022-11-14T09:15:14Z', direction='egress', ethertype='IPv6', | | | id='7c2c287e-9596-42ef-a5a8-0b09e38b206a', standard_attr_id='10579832', | | | updated_at='2022-11-14T09:15:14Z' | | stateful | True | | tags | [] | | updated_at | 2022-11-14T09:15:14Z | +-----------------+--------------------------------------------------------------------------------+ Removing default ingress rules By default, a security group named default has been already created for you, allowing all traffic from any source (ingress), and to any destination (egress). Cleura Cloud Management Panel OpenStack CLI Click on it and select the Rules tab to view its rules: View the details of the default security group using the following command: openstack security group show default you will get a printout similar to this: +-----------------+--------------------------------------------------------------------------------+ | Field | Value | +-----------------+--------------------------------------------------------------------------------+ | created_at | 2022-09-12T15:00:57Z | | description | Default security group | | id | 935b1317-a0c0-42e9-b68d-7cf16637df14 | | name | default | | project_id | cb43f189f7904fb88f3bbcfa22653ab8 | | revision_number | 5 | | rules | created_at='2022-09-12T15:00:59Z', direction='ingress', ethertype='IPv4', | | | id='5e5e9f4d-1faa-492d-91f1-c105b464072b', normalized_cidr='0.0.0.0/0', | | | remote_ip_prefix='0.0.0.0/0', standard_attr_id='10422245', | | | updated_at='2022-09-12T15:00:59Z' | | | created_at='2022-09-12T15:00:59Z', direction='ingress', ethertype='IPv6', | | | id='86b9413a-ad23-46c4-a35e-9306945dc63c', normalized_cidr='::/0', | | | remote_ip_prefix='::/0', standard_attr_id='10422248', | | | updated_at='2022-09-12T15:00:59Z' | | | created_at='2022-09-12T15:00:57Z', direction='egress', ethertype='IPv6', | | | id='ad4a19ef-7fab-4eba-9982-e5b109be121c', standard_attr_id='10422242', | | | updated_at='2022-09-12T15:00:57Z' | | | created_at='2022-09-12T15:00:57Z', direction='egress', ethertype='IPv4', | | | id='f53b1a12-edbb-480b-910b-a71c4836346f', standard_attr_id='10422236', | | | updated_at='2022-09-12T15:00:57Z' | | stateful | True | | tags | [] | | updated_at | 2022-09-12T15:00:59Z | +-----------------+--------------------------------------------------------------------------------+ We recommend to either create and use a new security group other than the default one, or restrict ingress traffic to specific ports and sources. If you want use the default group, remove the two ingress rules that allow all incoming traffic. Cleura Cloud Management Panel OpenStack CLI Click on the trashcan action button on the right-hand side for both ingress rules. Your default or newly created security group rules will now looks like this: To view the rules use the following command: openstack security group rule list default The printout will be similar to this: +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | ID | IP Protocol | Ethertype | IP Range | Port Range | Direction | Remote Security Group | Remote Address Group | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | 5e5e9f4d- | None | IPv4 | 0.0.0.0/0 | | ingress | None | None | | 1faa- | | | | | | | | | 492d- | | | | | | | | | 91f1- | | | | | | | | | c105b4640 | | | | | | | | | 72b | | | | | | | | | 86b9413a- | None | IPv6 | ::/0 | | ingress | None | None | | ad23- | | | | | | | | | 46c4- | | | | | | | | | a35e- | | | | | | | | | 9306945dc | | | | | | | | | 63c | | | | | | | | | ad4a19ef- | None | IPv6 | ::/0 | | egress | None | None | | 7fab- | | | | | | | | | 4eba- | | | | | | | | | 9982- | | | | | | | | | e5b109be1 | | | | | | | | | 21c | | | | | | | | | f53b1a12- | None | IPv4 | 0.0.0.0/0 | | egress | None | None | | edbb- | | | | | | | | | 480b- | | | | | | | | | 910b- | | | | | | | | | a71c48363 | | | | | | | | | 46f | | | | | | | | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ The IDs of the two ingress rules, one for IPv4 traffic and one for IPv6, in this case are: 5e5e9f4d-1faa-492d-91f1-c105b464072b and 86b9413a-ad23-46c4-a35e-9306945dc63c Delete them by using the following command: openstack security group rule delete \\ 5e5e9f4d-1faa-492d-91f1-c105b464072b 86b9413a-ad23-46c4-a35e-9306945dc63c Print the rules again: openstack security group rule list default Now the remaining rules are only the egress ones. +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | ID | IP Protocol | Ethertype | IP Range | Port Range | Direction | Remote Security Group | Remote Address Group | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | ad4a19ef- | None | IPv6 | ::/0 | | egress | None | None | | 7fab- | | | | | | | | | 4eba- | | | | | | | | | 9982- | | | | | | | | | e5b109be1 | | | | | | | | | 21c | | | | | | | | | f53b1a12- | None | IPv4 | 0.0.0.0/0 | | egress | None | None | | edbb- | | | | | | | | | 480b- | | | | | | | | | 910b- | | | | | | | | | a71c48363 | | | | | | | | | 46f | | | | | | | | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ Allowing SSH access The next thing to do, is to allow SSH access on port 22 to the server, only from specific networks. Cleura Cloud Management Panel OpenStack CLI To do this, click on the Create new rule button. To create this rule use the following command: openstack security group rule create \\ --protocol tcp --dst-port 22 --remote-ip 203 .0.113.58/32 default If you don\u2019t know your IP, simply visit icanhazip.com . In this example your IP is 203.0.113.58 and you want to allow SSH access from this IP address only, enter 203.0.113.58/32 as CIDR. If you want to allow SSH access from any address in that Class C subnet , instead enter 203.0.113.0/24 as CIDR. Allowing Web Traffic Next create the rules that allow anyone access the server on port 80 and port 443 . Cleura Cloud Management Panel OpenStack CLI Using the same logic as before, click on Create new rule . Select TCP Protocol and port 80 as both min and max range value. This time, CIDR is left empty, allowing incoming traffic from any IP/source. The same applies to port 443. This time don\u2019t specify \u2013remote-ip to allow traffic from all sources, using the following command: openstack security group rule create --protocol tcp --dst-port 80 default One more time for port 443: openstack security group rule create --protocol tcp --dst-port 443 default To view the updated rules, print the them again: openstack security group rule list default +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | ID | IP Protocol | Ethertype | IP Range | Port Range | Direction | Remote Security Group | Remote Address Group | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | 742bcc46- | tcp | IPv4 | 0.0.0.0/0 | 80:80 | ingress | None | None | | beb5- | | | | | | | | | 47a5- | | | | | | | | | 8eb1- | | | | | | | | | eb35da800 | | | | | | | | | 6ed | | | | | | | | | ad4a19ef- | None | IPv6 | ::/0 | | egress | None | None | | 7fab- | | | | | | | | | 4eba- | | | | | | | | | 9982- | | | | | | | | | e5b109be1 | | | | | | | | | 21c | | | | | | | | | cef0cd36- | tcp | IPv4 | 203.0.113 | 22:22 | ingress | None | None | | ad78- | | | .58/32 | | | | | | 4dbd- | | | | | | | | | b806- | | | | | | | | | 597300fd9 | | | | | | | | | e6a | | | | | | | | | f53b1a12- | None | IPv4 | 0.0.0.0/0 | | egress | None | None | | edbb- | | | | | | | | | 480b- | | | | | | | | | 910b- | | | | | | | | | a71c48363 | | | | | | | | | 46f | | | | | | | | | f90c598c- | tcp | IPv4 | 0.0.0.0/0 | 443:443 | ingress | None | None | | 3a5e- | | | | | | | | | 459f- | | | | | | | | | 8ed3- | | | | | | | | | 3c2538e7a | | | | | | | | | 24f | | | | | | | | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ All the rules for a simple web server are now in place. For any additional protocol or ingress rule, simply follow the same procedure as above.","title":"Creating security groups"},{"location":"howto/openstack/neutron/create-security-groups/#creating-security-groups","text":"By definition , security groups are \u201d[\u2026] sets of IP filter rules that are applied to all project instances, which define networking access to the instance. Group rules are project specific; project members can edit the default rules for their group and add new rule sets.\u201d","title":"Creating security groups"},{"location":"howto/openstack/neutron/create-security-groups/#creating-a-security-group","text":"Navigate to the Cleura Cloud Management Panel page, and log into your Cleura account. On the other hand, if you prefer to work with the OpenStack CLI, please do not forget to source the RC file first . Cleura Cloud Management Panel OpenStack CLI To create a security group click on Security Groups in the left-hand navigation menu: and then click on Create new Security Group in the top-right corner: An alternative way to create a Security Group is by clicking on Create \u2026 button in the top bar. Now give the security group a name and description, and choose in which region to create it, then click create : To create a security group use the following command: openstack security group create <name> When the command is executed successfully, you will get information regarding your new security group: +-----------------+--------------------------------------------------------------------------------+ | Field | Value | +-----------------+--------------------------------------------------------------------------------+ | created_at | 2022-11-14T09:15:14Z | | description | <name> | | id | 736da1d1-aa98-4da4-9ba4-2ab9aeea6a57 | | name | <name> | | project_id | cb43f189f7904fb88f3bbcfa22653ab8 | | revision_number | 1 | | rules | created_at='2022-11-14T09:15:14Z', direction='egress', ethertype='IPv4', | | | id='1f4c57cb-8e34-420c-a7e3-3b5625c79481', standard_attr_id='10579829', | | | updated_at='2022-11-14T09:15:14Z' | | | created_at='2022-11-14T09:15:14Z', direction='egress', ethertype='IPv6', | | | id='7c2c287e-9596-42ef-a5a8-0b09e38b206a', standard_attr_id='10579832', | | | updated_at='2022-11-14T09:15:14Z' | | stateful | True | | tags | [] | | updated_at | 2022-11-14T09:15:14Z | +-----------------+--------------------------------------------------------------------------------+","title":"Creating a security group"},{"location":"howto/openstack/neutron/create-security-groups/#removing-default-ingress-rules","text":"By default, a security group named default has been already created for you, allowing all traffic from any source (ingress), and to any destination (egress). Cleura Cloud Management Panel OpenStack CLI Click on it and select the Rules tab to view its rules: View the details of the default security group using the following command: openstack security group show default you will get a printout similar to this: +-----------------+--------------------------------------------------------------------------------+ | Field | Value | +-----------------+--------------------------------------------------------------------------------+ | created_at | 2022-09-12T15:00:57Z | | description | Default security group | | id | 935b1317-a0c0-42e9-b68d-7cf16637df14 | | name | default | | project_id | cb43f189f7904fb88f3bbcfa22653ab8 | | revision_number | 5 | | rules | created_at='2022-09-12T15:00:59Z', direction='ingress', ethertype='IPv4', | | | id='5e5e9f4d-1faa-492d-91f1-c105b464072b', normalized_cidr='0.0.0.0/0', | | | remote_ip_prefix='0.0.0.0/0', standard_attr_id='10422245', | | | updated_at='2022-09-12T15:00:59Z' | | | created_at='2022-09-12T15:00:59Z', direction='ingress', ethertype='IPv6', | | | id='86b9413a-ad23-46c4-a35e-9306945dc63c', normalized_cidr='::/0', | | | remote_ip_prefix='::/0', standard_attr_id='10422248', | | | updated_at='2022-09-12T15:00:59Z' | | | created_at='2022-09-12T15:00:57Z', direction='egress', ethertype='IPv6', | | | id='ad4a19ef-7fab-4eba-9982-e5b109be121c', standard_attr_id='10422242', | | | updated_at='2022-09-12T15:00:57Z' | | | created_at='2022-09-12T15:00:57Z', direction='egress', ethertype='IPv4', | | | id='f53b1a12-edbb-480b-910b-a71c4836346f', standard_attr_id='10422236', | | | updated_at='2022-09-12T15:00:57Z' | | stateful | True | | tags | [] | | updated_at | 2022-09-12T15:00:59Z | +-----------------+--------------------------------------------------------------------------------+ We recommend to either create and use a new security group other than the default one, or restrict ingress traffic to specific ports and sources. If you want use the default group, remove the two ingress rules that allow all incoming traffic. Cleura Cloud Management Panel OpenStack CLI Click on the trashcan action button on the right-hand side for both ingress rules. Your default or newly created security group rules will now looks like this: To view the rules use the following command: openstack security group rule list default The printout will be similar to this: +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | ID | IP Protocol | Ethertype | IP Range | Port Range | Direction | Remote Security Group | Remote Address Group | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | 5e5e9f4d- | None | IPv4 | 0.0.0.0/0 | | ingress | None | None | | 1faa- | | | | | | | | | 492d- | | | | | | | | | 91f1- | | | | | | | | | c105b4640 | | | | | | | | | 72b | | | | | | | | | 86b9413a- | None | IPv6 | ::/0 | | ingress | None | None | | ad23- | | | | | | | | | 46c4- | | | | | | | | | a35e- | | | | | | | | | 9306945dc | | | | | | | | | 63c | | | | | | | | | ad4a19ef- | None | IPv6 | ::/0 | | egress | None | None | | 7fab- | | | | | | | | | 4eba- | | | | | | | | | 9982- | | | | | | | | | e5b109be1 | | | | | | | | | 21c | | | | | | | | | f53b1a12- | None | IPv4 | 0.0.0.0/0 | | egress | None | None | | edbb- | | | | | | | | | 480b- | | | | | | | | | 910b- | | | | | | | | | a71c48363 | | | | | | | | | 46f | | | | | | | | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ The IDs of the two ingress rules, one for IPv4 traffic and one for IPv6, in this case are: 5e5e9f4d-1faa-492d-91f1-c105b464072b and 86b9413a-ad23-46c4-a35e-9306945dc63c Delete them by using the following command: openstack security group rule delete \\ 5e5e9f4d-1faa-492d-91f1-c105b464072b 86b9413a-ad23-46c4-a35e-9306945dc63c Print the rules again: openstack security group rule list default Now the remaining rules are only the egress ones. +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | ID | IP Protocol | Ethertype | IP Range | Port Range | Direction | Remote Security Group | Remote Address Group | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | ad4a19ef- | None | IPv6 | ::/0 | | egress | None | None | | 7fab- | | | | | | | | | 4eba- | | | | | | | | | 9982- | | | | | | | | | e5b109be1 | | | | | | | | | 21c | | | | | | | | | f53b1a12- | None | IPv4 | 0.0.0.0/0 | | egress | None | None | | edbb- | | | | | | | | | 480b- | | | | | | | | | 910b- | | | | | | | | | a71c48363 | | | | | | | | | 46f | | | | | | | | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+","title":"Removing default ingress rules"},{"location":"howto/openstack/neutron/create-security-groups/#allowing-ssh-access","text":"The next thing to do, is to allow SSH access on port 22 to the server, only from specific networks. Cleura Cloud Management Panel OpenStack CLI To do this, click on the Create new rule button. To create this rule use the following command: openstack security group rule create \\ --protocol tcp --dst-port 22 --remote-ip 203 .0.113.58/32 default If you don\u2019t know your IP, simply visit icanhazip.com . In this example your IP is 203.0.113.58 and you want to allow SSH access from this IP address only, enter 203.0.113.58/32 as CIDR. If you want to allow SSH access from any address in that Class C subnet , instead enter 203.0.113.0/24 as CIDR.","title":"Allowing SSH access"},{"location":"howto/openstack/neutron/create-security-groups/#allowing-web-traffic","text":"Next create the rules that allow anyone access the server on port 80 and port 443 . Cleura Cloud Management Panel OpenStack CLI Using the same logic as before, click on Create new rule . Select TCP Protocol and port 80 as both min and max range value. This time, CIDR is left empty, allowing incoming traffic from any IP/source. The same applies to port 443. This time don\u2019t specify \u2013remote-ip to allow traffic from all sources, using the following command: openstack security group rule create --protocol tcp --dst-port 80 default One more time for port 443: openstack security group rule create --protocol tcp --dst-port 443 default To view the updated rules, print the them again: openstack security group rule list default +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | ID | IP Protocol | Ethertype | IP Range | Port Range | Direction | Remote Security Group | Remote Address Group | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ | 742bcc46- | tcp | IPv4 | 0.0.0.0/0 | 80:80 | ingress | None | None | | beb5- | | | | | | | | | 47a5- | | | | | | | | | 8eb1- | | | | | | | | | eb35da800 | | | | | | | | | 6ed | | | | | | | | | ad4a19ef- | None | IPv6 | ::/0 | | egress | None | None | | 7fab- | | | | | | | | | 4eba- | | | | | | | | | 9982- | | | | | | | | | e5b109be1 | | | | | | | | | 21c | | | | | | | | | cef0cd36- | tcp | IPv4 | 203.0.113 | 22:22 | ingress | None | None | | ad78- | | | .58/32 | | | | | | 4dbd- | | | | | | | | | b806- | | | | | | | | | 597300fd9 | | | | | | | | | e6a | | | | | | | | | f53b1a12- | None | IPv4 | 0.0.0.0/0 | | egress | None | None | | edbb- | | | | | | | | | 480b- | | | | | | | | | 910b- | | | | | | | | | a71c48363 | | | | | | | | | 46f | | | | | | | | | f90c598c- | tcp | IPv4 | 0.0.0.0/0 | 443:443 | ingress | None | None | | 3a5e- | | | | | | | | | 459f- | | | | | | | | | 8ed3- | | | | | | | | | 3c2538e7a | | | | | | | | | 24f | | | | | | | | +-----------+-------------+-----------+-----------+------------+-----------+-----------------------+----------------------+ All the rules for a simple web server are now in place. For any additional protocol or ingress rule, simply follow the same procedure as above.","title":"Allowing Web Traffic"},{"location":"howto/openstack/neutron/multiple-public-ips/","text":"Assigning multiple public (floating) IPs to a server In Cleura, we do not pass external networks to the compute nodes. This means that you, as a user, can not directly attach a server to the public network. In order to provide connectivity to the public network (for IPv4), you need to use floating IPs. A floating IP is created in the public subnet, and is mapped to the specific network port. All traffic comes through a virtual router. For some scenarios, you might need to have more than one public IP assigned to a server. But in case of 1-to-1 NAT (which is how the floating IP is implemented under the hood) you can not assign more than one external IP to the internal one. And adding a new port to the VM is also not an option, since this would result in asymmetric routing, as replies will go through the first interface for which a default route is set. Instead, you must first configure an additional private (\u201cfixed\u201d) IP address for your port, then associate a public (\u201cfloating\u201d) IP address to map to it. Add an extra IP to the port Assume you already have a network port inside your private network: $ openstack port show 51dae637-ad79-4ba9-9e41-78e5e0f3332c -c fixed_ips +-----------+--------------------------------------------------------------------------+ | Field | Value | +-----------+--------------------------------------------------------------------------+ | fixed_ips | ip_address='10.2.0.58', subnet_id='5efeae9f-06b8-41a5-987f-085e8c7113a6' | +-----------+--------------------------------------------------------------------------+ And you also have a floating IP associated with it: $ openstack floating ip list -c ID -c \"Floating IP Address\" -c \"Fixed IP Address\" -c Port +--------------------------------------+---------------------+------------------+--------------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | +--------------------------------------+---------------------+------------------+--------------------------------------+ | 989f8a96-4ab4-4190-83e4-25b71d309ea9 | 192.0.2.169 | 10.2.0.58 | 51dae637-ad79-4ba9-9e41-78e5e0f3332c | | c45a5eaf-2f3a-4679-89fe-266a5cbe840a | 198.51.100.12 | None | None | +--------------------------------------+---------------------+------------------+--------------------------------------+ Then what you need to do, is to add extra IP address to your existing port: $ openstack port set --fixed-ip subnet=5efeae9f-06b8-41a5-987f-085e8c7113a6 51dae637-ad79-4ba9-9e41-78e5e0f3332c You can then confirm that the port does have two entries in its fixed_ips list: $ openstack port show 51dae637-ad79-4ba9-9e41-78e5e0f3332c -c fixed_ips +-----------+---------------------------------------------------------------------------+ | Field | Value | +-----------+---------------------------------------------------------------------------+ | fixed_ips | ip_address='10.2.0.228', subnet_id='5efeae9f-06b8-41a5-987f-085e8c7113a6' | | | ip_address='10.2.0.58', subnet_id='5efeae9f-06b8-41a5-987f-085e8c7113a6' | +-----------+---------------------------------------------------------------------------+ Don\u2019t forget to configure new IP as an alias to the interface inside your VM! When you have an IP address on your port that is not yet assigned to any floating IP, you can assign it to the new floating IP. Proceed with: $ openstack floating ip set c45a5eaf-2f3a-4679-89fe-266a5cbe840a --port 51dae637-ad79-4ba9-9e41-78e5e0f3332c --fixed-ip-address 10.2.0.228 Then, list the floating (public) IP addresses, together with their fixed (private) counterparts: $ openstack floating ip list -c ID -c \"Floating IP Address\" -c \"Fixed IP Address\" -c Port +--------------------------------------+---------------------+------------------+--------------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | +--------------------------------------+---------------------+------------------+--------------------------------------+ | 989f8a96-4ab4-4190-83e4-25b71d309ea9 | 192.0.2.169 | 10.2.0.58 | 51dae637-ad79-4ba9-9e41-78e5e0f3332c | | c45a5eaf-2f3a-4679-89fe-266a5cbe840a | 198.51.100.12 | 10.2.0.228 | 51dae637-ad79-4ba9-9e41-78e5e0f3332c | +--------------------------------------+---------------------+------------------+--------------------------------------+ Now your server is accessible through two different public IP addresses.","title":"Assigning multiple public (floating) IPs to a server"},{"location":"howto/openstack/neutron/multiple-public-ips/#assigning-multiple-public-floating-ips-to-a-server","text":"In Cleura, we do not pass external networks to the compute nodes. This means that you, as a user, can not directly attach a server to the public network. In order to provide connectivity to the public network (for IPv4), you need to use floating IPs. A floating IP is created in the public subnet, and is mapped to the specific network port. All traffic comes through a virtual router. For some scenarios, you might need to have more than one public IP assigned to a server. But in case of 1-to-1 NAT (which is how the floating IP is implemented under the hood) you can not assign more than one external IP to the internal one. And adding a new port to the VM is also not an option, since this would result in asymmetric routing, as replies will go through the first interface for which a default route is set. Instead, you must first configure an additional private (\u201cfixed\u201d) IP address for your port, then associate a public (\u201cfloating\u201d) IP address to map to it.","title":"Assigning multiple public (floating) IPs to a server"},{"location":"howto/openstack/neutron/multiple-public-ips/#add-an-extra-ip-to-the-port","text":"Assume you already have a network port inside your private network: $ openstack port show 51dae637-ad79-4ba9-9e41-78e5e0f3332c -c fixed_ips +-----------+--------------------------------------------------------------------------+ | Field | Value | +-----------+--------------------------------------------------------------------------+ | fixed_ips | ip_address='10.2.0.58', subnet_id='5efeae9f-06b8-41a5-987f-085e8c7113a6' | +-----------+--------------------------------------------------------------------------+ And you also have a floating IP associated with it: $ openstack floating ip list -c ID -c \"Floating IP Address\" -c \"Fixed IP Address\" -c Port +--------------------------------------+---------------------+------------------+--------------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | +--------------------------------------+---------------------+------------------+--------------------------------------+ | 989f8a96-4ab4-4190-83e4-25b71d309ea9 | 192.0.2.169 | 10.2.0.58 | 51dae637-ad79-4ba9-9e41-78e5e0f3332c | | c45a5eaf-2f3a-4679-89fe-266a5cbe840a | 198.51.100.12 | None | None | +--------------------------------------+---------------------+------------------+--------------------------------------+ Then what you need to do, is to add extra IP address to your existing port: $ openstack port set --fixed-ip subnet=5efeae9f-06b8-41a5-987f-085e8c7113a6 51dae637-ad79-4ba9-9e41-78e5e0f3332c You can then confirm that the port does have two entries in its fixed_ips list: $ openstack port show 51dae637-ad79-4ba9-9e41-78e5e0f3332c -c fixed_ips +-----------+---------------------------------------------------------------------------+ | Field | Value | +-----------+---------------------------------------------------------------------------+ | fixed_ips | ip_address='10.2.0.228', subnet_id='5efeae9f-06b8-41a5-987f-085e8c7113a6' | | | ip_address='10.2.0.58', subnet_id='5efeae9f-06b8-41a5-987f-085e8c7113a6' | +-----------+---------------------------------------------------------------------------+ Don\u2019t forget to configure new IP as an alias to the interface inside your VM! When you have an IP address on your port that is not yet assigned to any floating IP, you can assign it to the new floating IP. Proceed with: $ openstack floating ip set c45a5eaf-2f3a-4679-89fe-266a5cbe840a --port 51dae637-ad79-4ba9-9e41-78e5e0f3332c --fixed-ip-address 10.2.0.228 Then, list the floating (public) IP addresses, together with their fixed (private) counterparts: $ openstack floating ip list -c ID -c \"Floating IP Address\" -c \"Fixed IP Address\" -c Port +--------------------------------------+---------------------+------------------+--------------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | +--------------------------------------+---------------------+------------------+--------------------------------------+ | 989f8a96-4ab4-4190-83e4-25b71d309ea9 | 192.0.2.169 | 10.2.0.58 | 51dae637-ad79-4ba9-9e41-78e5e0f3332c | | c45a5eaf-2f3a-4679-89fe-266a5cbe840a | 198.51.100.12 | 10.2.0.228 | 51dae637-ad79-4ba9-9e41-78e5e0f3332c | +--------------------------------------+---------------------+------------------+--------------------------------------+ Now your server is accessible through two different public IP addresses.","title":"Add an extra IP to the port"},{"location":"howto/openstack/neutron/new-network/","text":"Creating new networks Before creating a server in Cleura Cloud, you need at least one network to make the new server a member of. Since you may have more than one network per region, let us now walk through creating a new network using the Cleura Cloud Management Panel, or using the OpenStack CLI. Prerequisites Whether you choose to work from the Cleura Cloud Management Panel or with the OpenStack CLI, you need to have an account in Cleura Cloud. Additionally, to use the OpenStack CLI make sure to enable it first . Creating a network To create a network from the Cleura Cloud Management Panel, fire up your favorite web browser, navigate to the Cleura Cloud page, and login into your Cleura account. On the other hand, if you prefer to work with OpenStack CLI, please do not forget to source the RC file first. Cleura Cloud Management Panel OpenStack CLI On the top right-hand side of the Cleura Cloud Management Panel, click the Create button. A new pane will slide into view from the right-hand side of the browser window, titled Create . You will notice several rounded boxes prominently displayed on that pane, each for defining, configuring, and instantiating a different Cleura Cloud object. Go ahead and click the Network box. A new pane titled Create Network will slide over. At the top, type in a name and select one of the available regions for the new network. Start by creating a new network, named nordostbahnhof : openstack network create nordostbahnhof By issuing the command above, you immediately get information regarding the new network: +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2022-10-30T14:31:49Z | | description | | | dns_domain | None | | id | 201d458b-9b47-4408-9736-980bec77d405 | | ipv4_address_scope | None | | ipv6_address_scope | None | | is_default | False | | is_vlan_transparent | None | | mtu | 1500 | | name | nordostbahnhof | | port_security_enabled | True | | project_id | dfc700467396428bacba4376e72cc3e9 | | provider:network_type | None | | provider:physical_network | None | | provider:segmentation_id | None | | qos_policy_id | None | | revision_number | 1 | | router:external | Internal | | segments | None | | shared | False | | status | ACTIVE | | subnets | | | tags | | | updated_at | 2022-10-30T14:31:50Z | +---------------------------+--------------------------------------+ Adding a subnet and a router Creating a new network does not necessarily mean it has all the features you most likely would expect. Unless you work from the Cleura Cloud Management Panel, where almost every component is activated for you with a few clicks here and there, when you use the OpenStack CLI there is some extra work you need to do before you get a network you would characterize as useful. Cleura Cloud Management Panel OpenStack CLI Expand the Advanced Options section below, make sure Port Security is enabled, and leave the MTU parameter blank. You probably want a full-featured network for your cloud servers, so please activate the Create a complete network containing a subnet and a router option. You will notice that a network address in CIDR notation is pre-configured for your network. You also get a couple of DNS servers, a Gateway, and a DHCP server. Scroll down a little bit if you have to. Assuming you want your cloud servers to reach hosts on the Internet, for the External network parameter make sure you select ext-net . Then, click the green Create button to initialize the new network. In a few seconds, the new network will be readily available. You now have to create a subnet for the new network. Let us call this subnet nordostbahnhof-subnet : openstack subnet create nordostbahnhof-subnet \\ --network nordostbahnhof --subnet-range 10 .20.30.0/24 Again, you get detailed information regarding the new subnet: +----------------------+--------------------------------------+ | Field | Value | +----------------------+--------------------------------------+ | allocation_pools | 10.20.30.2-10.20.30.254 | | cidr | 10.20.30.0/24 | | created_at | 2022-10-30T14:47:40Z | | description | | | dns_nameservers | | | dns_publish_fixed_ip | None | | enable_dhcp | True | | gateway_ip | 10.20.30.1 | | host_routes | | | id | 1b0822b3-62e8-4b40-92e8-8544c72d4c15 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | name | nordostbahnhof-subnet | | network_id | 201d458b-9b47-4408-9736-980bec77d405 | | project_id | dfc700467396428bacba4376e72cc3e9 | | revision_number | 0 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2022-10-30T14:47:40Z | +----------------------+--------------------------------------+ If you want servers connected to the nordostbahnhof network to have Internet access, you need a router in front of the network. Following our unofficial naming convention, go ahead and create a new router called nordostbahnhof-router : openstack router create nordostbahnhof-router As expected, you will see lots of information regarding the new router: +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2022-10-30T15:36:26Z | | description | | | enable_ndp_proxy | None | | external_gateway_info | null | | flavor_id | None | | ha | True | | id | 566de991-fc0e-4f85-b6c4-5c87694781f7 | | name | nordostbahnhof-router | | project_id | dfc700467396428bacba4376e72cc3e9 | | revision_number | 1 | | routes | | | status | ACTIVE | | tags | | | tenant_id | dfc700467396428bacba4376e72cc3e9 | | updated_at | 2022-10-30T15:36:26Z | +-------------------------+--------------------------------------+ You want the nordostbahnhof-router connected to the external network. The name of this network is ext-net : openstack router set nordostbahnhof-router --external-gateway ext-net Please note that if the command above is successful, you will get no output on your terminal. There is one last step to take, and that is to connect router nordostbahnhof-router to the subnet nordostbahnhof-subnet of network nordostbahnhof : openstack router add subnet nordostbahnhof-router nordostbahnhof-subnet Again, if the command above is successful, you will get no output. Listing networks and getting information At any time, you may connect to the Cleura Cloud Management Panel, list all networks you have already created, and get detailed information for any of these networks. Alternatively, you may get all that information using the OpenStack CLI. Cleura Cloud Management Panel OpenStack CLI You may see all defined networks, in all supported regions, by selecting Networking > Networks (see the left-hand side pane on the Cleura Cloud Management Panel). For more information regarding a specific network, click the corresponding three-dot icon (right-hand side) and select View details . Then, you can glance over all the details regarding the selected network\u2019s ports, subnets, and routers. To list all available networks in a specific region, just type: openstack network list You can always ask for more specific results. For instance, to see all internal networks only, type the following: openstack network list --internal You can also get detailed information about a specific network: openstack network show nordostbahnhof At any time, type openstack network list --help or openstack network show --help to see how to get information regarding networks, and what specific pieces of information you can have.","title":"Creating new networks"},{"location":"howto/openstack/neutron/new-network/#creating-new-networks","text":"Before creating a server in Cleura Cloud, you need at least one network to make the new server a member of. Since you may have more than one network per region, let us now walk through creating a new network using the Cleura Cloud Management Panel, or using the OpenStack CLI.","title":"Creating new networks"},{"location":"howto/openstack/neutron/new-network/#prerequisites","text":"Whether you choose to work from the Cleura Cloud Management Panel or with the OpenStack CLI, you need to have an account in Cleura Cloud. Additionally, to use the OpenStack CLI make sure to enable it first .","title":"Prerequisites"},{"location":"howto/openstack/neutron/new-network/#creating-a-network","text":"To create a network from the Cleura Cloud Management Panel, fire up your favorite web browser, navigate to the Cleura Cloud page, and login into your Cleura account. On the other hand, if you prefer to work with OpenStack CLI, please do not forget to source the RC file first. Cleura Cloud Management Panel OpenStack CLI On the top right-hand side of the Cleura Cloud Management Panel, click the Create button. A new pane will slide into view from the right-hand side of the browser window, titled Create . You will notice several rounded boxes prominently displayed on that pane, each for defining, configuring, and instantiating a different Cleura Cloud object. Go ahead and click the Network box. A new pane titled Create Network will slide over. At the top, type in a name and select one of the available regions for the new network. Start by creating a new network, named nordostbahnhof : openstack network create nordostbahnhof By issuing the command above, you immediately get information regarding the new network: +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2022-10-30T14:31:49Z | | description | | | dns_domain | None | | id | 201d458b-9b47-4408-9736-980bec77d405 | | ipv4_address_scope | None | | ipv6_address_scope | None | | is_default | False | | is_vlan_transparent | None | | mtu | 1500 | | name | nordostbahnhof | | port_security_enabled | True | | project_id | dfc700467396428bacba4376e72cc3e9 | | provider:network_type | None | | provider:physical_network | None | | provider:segmentation_id | None | | qos_policy_id | None | | revision_number | 1 | | router:external | Internal | | segments | None | | shared | False | | status | ACTIVE | | subnets | | | tags | | | updated_at | 2022-10-30T14:31:50Z | +---------------------------+--------------------------------------+","title":"Creating a network"},{"location":"howto/openstack/neutron/new-network/#adding-a-subnet-and-a-router","text":"Creating a new network does not necessarily mean it has all the features you most likely would expect. Unless you work from the Cleura Cloud Management Panel, where almost every component is activated for you with a few clicks here and there, when you use the OpenStack CLI there is some extra work you need to do before you get a network you would characterize as useful. Cleura Cloud Management Panel OpenStack CLI Expand the Advanced Options section below, make sure Port Security is enabled, and leave the MTU parameter blank. You probably want a full-featured network for your cloud servers, so please activate the Create a complete network containing a subnet and a router option. You will notice that a network address in CIDR notation is pre-configured for your network. You also get a couple of DNS servers, a Gateway, and a DHCP server. Scroll down a little bit if you have to. Assuming you want your cloud servers to reach hosts on the Internet, for the External network parameter make sure you select ext-net . Then, click the green Create button to initialize the new network. In a few seconds, the new network will be readily available. You now have to create a subnet for the new network. Let us call this subnet nordostbahnhof-subnet : openstack subnet create nordostbahnhof-subnet \\ --network nordostbahnhof --subnet-range 10 .20.30.0/24 Again, you get detailed information regarding the new subnet: +----------------------+--------------------------------------+ | Field | Value | +----------------------+--------------------------------------+ | allocation_pools | 10.20.30.2-10.20.30.254 | | cidr | 10.20.30.0/24 | | created_at | 2022-10-30T14:47:40Z | | description | | | dns_nameservers | | | dns_publish_fixed_ip | None | | enable_dhcp | True | | gateway_ip | 10.20.30.1 | | host_routes | | | id | 1b0822b3-62e8-4b40-92e8-8544c72d4c15 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | name | nordostbahnhof-subnet | | network_id | 201d458b-9b47-4408-9736-980bec77d405 | | project_id | dfc700467396428bacba4376e72cc3e9 | | revision_number | 0 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2022-10-30T14:47:40Z | +----------------------+--------------------------------------+ If you want servers connected to the nordostbahnhof network to have Internet access, you need a router in front of the network. Following our unofficial naming convention, go ahead and create a new router called nordostbahnhof-router : openstack router create nordostbahnhof-router As expected, you will see lots of information regarding the new router: +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2022-10-30T15:36:26Z | | description | | | enable_ndp_proxy | None | | external_gateway_info | null | | flavor_id | None | | ha | True | | id | 566de991-fc0e-4f85-b6c4-5c87694781f7 | | name | nordostbahnhof-router | | project_id | dfc700467396428bacba4376e72cc3e9 | | revision_number | 1 | | routes | | | status | ACTIVE | | tags | | | tenant_id | dfc700467396428bacba4376e72cc3e9 | | updated_at | 2022-10-30T15:36:26Z | +-------------------------+--------------------------------------+ You want the nordostbahnhof-router connected to the external network. The name of this network is ext-net : openstack router set nordostbahnhof-router --external-gateway ext-net Please note that if the command above is successful, you will get no output on your terminal. There is one last step to take, and that is to connect router nordostbahnhof-router to the subnet nordostbahnhof-subnet of network nordostbahnhof : openstack router add subnet nordostbahnhof-router nordostbahnhof-subnet Again, if the command above is successful, you will get no output.","title":"Adding a subnet and a router"},{"location":"howto/openstack/neutron/new-network/#listing-networks-and-getting-information","text":"At any time, you may connect to the Cleura Cloud Management Panel, list all networks you have already created, and get detailed information for any of these networks. Alternatively, you may get all that information using the OpenStack CLI. Cleura Cloud Management Panel OpenStack CLI You may see all defined networks, in all supported regions, by selecting Networking > Networks (see the left-hand side pane on the Cleura Cloud Management Panel). For more information regarding a specific network, click the corresponding three-dot icon (right-hand side) and select View details . Then, you can glance over all the details regarding the selected network\u2019s ports, subnets, and routers. To list all available networks in a specific region, just type: openstack network list You can always ask for more specific results. For instance, to see all internal networks only, type the following: openstack network list --internal You can also get detailed information about a specific network: openstack network show nordostbahnhof At any time, type openstack network list --help or openstack network show --help to see how to get information regarding networks, and what specific pieces of information you can have.","title":"Listing networks and getting information"},{"location":"howto/openstack/nova/config-drive/","text":"Launching a server with a configuration drive Background: OpenStack metadata discovery OpenStack Compute uses metadata to inject custom configurations to servers on boot. You can add custom scripts, install packages, and add SSH keys to the servers using metadata. By default, metadata discovery in Cleura Cloud uses an HTTP data source that booting servers connect to. Sometimes this is undesirable or \u2014 for specific server/networking configurations \u2014 unreliable. Under those circumstances, you can use an alternate configuration source. Store metadata on a configuration drive A configuration drive (config drive) is a read-only virtual drive that is attached to a server during boot. The server can then mount the drive and read files from it. Configuration drives are used as a data source for cloud-init . Enable the configuration drive on server creation ( openstack CLI) To enable the configuration drive, you need to pass the parameter --use-config-drive to the openstack server create command. In the following example, replace the image, flavor, keypair, and network reference, as well as the server name, to match your desired configuration. openstack server create \\ --use-config-drive \\ --image \"Ubuntu 20.04 Focal Fossa\" \\ --flavor b.1c2gb \\ --keypair mykey --nic net-id = 3a747038-ee59-404c-973d-5f795e8ebb73 \\ myserver Once the server launches, you can monitor its configuration process by monitoring the server console log: openstack console log show myserver","title":"Launching a server with a configuration drive"},{"location":"howto/openstack/nova/config-drive/#launching-a-server-with-a-configuration-drive","text":"","title":"Launching a server with a configuration drive"},{"location":"howto/openstack/nova/config-drive/#background-openstack-metadata-discovery","text":"OpenStack Compute uses metadata to inject custom configurations to servers on boot. You can add custom scripts, install packages, and add SSH keys to the servers using metadata. By default, metadata discovery in Cleura Cloud uses an HTTP data source that booting servers connect to. Sometimes this is undesirable or \u2014 for specific server/networking configurations \u2014 unreliable. Under those circumstances, you can use an alternate configuration source.","title":"Background: OpenStack metadata discovery"},{"location":"howto/openstack/nova/config-drive/#store-metadata-on-a-configuration-drive","text":"A configuration drive (config drive) is a read-only virtual drive that is attached to a server during boot. The server can then mount the drive and read files from it. Configuration drives are used as a data source for cloud-init .","title":"Store metadata on a configuration drive"},{"location":"howto/openstack/nova/config-drive/#enable-the-configuration-drive-on-server-creation-openstack-cli","text":"To enable the configuration drive, you need to pass the parameter --use-config-drive to the openstack server create command. In the following example, replace the image, flavor, keypair, and network reference, as well as the server name, to match your desired configuration. openstack server create \\ --use-config-drive \\ --image \"Ubuntu 20.04 Focal Fossa\" \\ --flavor b.1c2gb \\ --keypair mykey --nic net-id = 3a747038-ee59-404c-973d-5f795e8ebb73 \\ myserver Once the server launches, you can monitor its configuration process by monitoring the server console log: openstack console log show myserver","title":"Enable the configuration drive on server creation (openstack\u00a0CLI)"},{"location":"howto/openstack/nova/new-server/","text":"Creating new servers Once you have an account in Cleura Cloud , you can create virtual machines \u2014 henceforth simply servers \u2014 using either the Cleura Cloud Management Panel or the OpenStack CLI. Let us demonstrate the creation of a new server, following both approaches. Prerequisites You need to have at least one network in the region you are interested in. Additionally, if you prefer to work with the OpenStack CLI, then make sure to properly enable it first . Creating a server To create a server from the Cleura Cloud Management Panel, fire up your favorite web browser, navigate to the Cleura Cloud page, and log into your Cleura account. On the other hand, if you prefer to work with the OpenStack CLI, please do not forget to source the RC file first . Cleura Cloud Management Panel OpenStack CLI On the top right-hand side of the Cleura Cloud Management Panel, click the Create button. A new pane titled Create will slide into view from the right-hand side of the browser window. You will notice several rounded boxes on that pane, each for defining, configuring, and instantiating a different Cleura Cloud object. Go ahead and click the Server box. A new pane titled Create a Server will slide over. At the top, type in a name for the new server and select one of the available regions. In the Boot source section below, click the dropdown menu on the left and make sure you select Boot from image , so you can choose one of the readily available OS images to boot the new server off of. To pick one of the images, click on the dropdown menu on the right. For this how-to guide, we have chosen ubuntu in general and Ubuntu 22.04 Jammy Jellyfish 20220810 in particular. Next, make sure Boot Target is set to Volume (Recommended) . Regarding the server\u2019s CPU core count and amount of memory, set the Flavor accordingly. The default flavor specifies a server with 1 CPU core and 1GB of RAM. You can start with that or select a different configuration by clicking the dropdown menu at the right of Flavor . Please note that, depending on the chosen flavor, the estimated monthly cost of the server changes. (While a server is shut off you are still getting charged for it, but less so.) At any time, this estimated cost is displayed in the green rectangular area at the top. Something else that affects the cost is the size of the root device. Take a look at the Size parameter below, and notice the default (in gibibytes ). You may leave the root device size unchanged, or modify it to be lower or higher than the default. When, at a later time, you decide to delete the server, you can do so but keep its boot volume (you may want, for example, to attach the exact same volume to a new server). Just leave the Delete on termination option disabled if you want this kind of flexibility. On the other hand, if you want your root volume to be automatically deleted when the server terminates, enable Delete on termination . Use this option with caution. Finally, you may enable Disaster recovery for the server. If you do, then daily server snapshots will be created, and you will have the option for easy and fast rollups to previous snapshots. Please be aware that enabling this option increases the server\u2019s monthly estimated cost (again, it is displayed in the green rectangular area at the top). Regarding networking, select one of the available networks to attach the new server to. If you want the server accessible from the Internet, do not forget to enable the I want an external IP for my server option. In the section below, set Security Groups to default . If you already have one or more public keys in your Cleura Cloud account, you can now select a key to be included in the ~/.ssh/authorized_keys file of the server\u2019s default user. That way, you can securely log into the remote user\u2019s account without typing a password. If there are no public keys to choose from, activate the Password login enabled option and set a password for a specific user (with a username you define). A configuration script is automatically prepared based on the choices you have already made. That script runs during system boot and performs housekeeping tasks like user account creation, enabling acceptable authentication methods, and configuring remote package repositories. Click on Advanced Options to see the default script. It is now time to create your Cleura Cloud server; click the green Create button, and the new server will be readily available in a few seconds. An openstack command for creating a server may look like this: openstack server create \\ --flavor $FLAVOR_NAME \\ --image $IMAGE_NAME \\ --boot-from-volume $VOL_SIZE \\ --network $NETWORK_NAME \\ --security-group $SEC_GROUP_NAME \\ --key-name $KEY_NAME \\ --wait \\ $SERVER_NAME Each variable represents a piece of information we have to look for or, in the cases of KEY_NAME and SERVER_NAME , arbitrarily define. Let us begin with the flavors ( FLAVOR_NAME ), which describe combinations of CPU core count and memory size. Each server has a distinct flavor, and to see all available flavors type: openstack flavor list You will get a pretty long list of flavors. For our demonstration, we suggest you go with b.1c1gb . A server with this particular flavor will have one CPU core and one gibibyte of RAM. Go ahead and set FLAVOR_NAME accordingly: FLAVOR_NAME = \"b.1c1gb\" Your server should have an image to boot off of ( IMAGE_NAME ). For a list of all available images in Cleura Cloud, type: openstack image list This time you get a shorter list, but you can still filter for images with the OS you prefer. For example, filter for Ubuntu: openstack image list --tag \"os:ubuntu\" Continue with the Ubuntu 22.04 Jammy Jellyfish 20220810 image: IMAGE_NAME = \"Ubuntu 22.04 Jammy Jellyfish 20220810\" Before you go on, decide on the capacity (in gibibytes) of the server\u2019s boot volume ( VOL_SIZE ). We suggest you start with 20 gibibytes: VOL_SIZE = \"20\" You need at least one network in the region you\u2019re about to create your new server ( NETWORK_NAME ). To get the names of all available (internal) networks, type: openstack network list --internal -c Name +----------------+ | Name | +----------------+ | nordostbahnhof | +----------------+ Set the NETWORK_NAME variable accordingly: NETWORK_NAME = \"nordostbahnhof\" Regarding the security group ( SEC_GROUP_NAME ), unless you have already created one yourself, you will find only one per region: openstack security group list -c Name -c Description +---------+------------------------+ | Name | Description | +---------+------------------------+ | default | Default security group | +---------+------------------------+ Go ahead and set SEC_GROUP_NAME : SEC_GROUP_NAME = \"default\" You most likely want a server you can remotely connect to via SSH without typing a password. Upload one of our public keys to your Cleura Cloud account: openstack keypair create --public-key ~/.ssh/id_ed25519.pub bahnhof In the example above, we uploaded the public key ~/.ssh/id_ed25519.pub to our Cleura Cloud account and named it bahnhof . Follow our example and do not forget to set the KEY_NAME : KEY_NAME = \"bahnhof\" By the way, check all uploaded public keys\u2026 openstack keypair list \u2026and get more information regarding the one you just uploaded: openstack keypair show bahnhof You are almost ready to create your new server. Decide on a name\u2026 SERVER_NAME = \"zug\" # just an example \u2026and then go ahead and create it: openstack server create \\ --flavor b.1c1gb \\ --image $IMAGE_NAME \\ --boot-from-volume 20 \\ --network nordostbahnhof \\ --security-group default \\ --key-name bahnhof \\ --wait \\ zug (For clarity\u2019s sake, and with the exception of IMAGE_NAME , we used the actual values and not the variables we so meticulously set.) The --wait parameter is optional. Whenever you choose to use it, you get back control of your terminal only after the server is readily available in Cleura Cloud. To connect to your server remotely, you need to create a floating IP for the external network in the Cleura Cloud, and then assign this IP to your server. First, create the floating IP: openstack floating ip create ext-net See all floating IPs\u2026 openstack floating ip list \u2026and assign the one you just created to your server: openstack server add floating ip zug 198 .51.100.12 The username of the default user account in the Ubuntu image is ubuntu , so now you can connect to your remote server via SSH without typing a password: ssh ubuntu@198.51.100.12 Viewing information about the newly created server Cleura Cloud Management Panel OpenStack CLI From the Cleura Cloud Management Panel you may, at any time, see all servers and get detailed information regarding each one of them; expand the left-hand side vertical pane, click Compute , then Servers , and, in the central pane, select the region you want. To see all available servers in the region, type: openstack server list You can always get specific information on a particular server: openstack server show zug Connecting to the server console Cleura Cloud Management Panel OpenStack CLI While viewing information regarding your server, you may get its public IP address (e.g., from the Addresses tab) and connect to it remotely. Alternatively, you may launch a web console and log in; click on the three-dot icon on the right of the server header, and from the pop-up menu that appears select Remote Console . A new window pops up, and that\u2019s your web console to your Cleura Cloud server. Please note that this window cannot be resized but can be opened on a new browser window or tab. You may have access to the web console of your server, and you need the corresponding URL for it: openstack console url show zug Usage of the web console is discouraged, though. Instead, securely connect to your server via SSH.","title":"Creating new servers"},{"location":"howto/openstack/nova/new-server/#creating-new-servers","text":"Once you have an account in Cleura Cloud , you can create virtual machines \u2014 henceforth simply servers \u2014 using either the Cleura Cloud Management Panel or the OpenStack CLI. Let us demonstrate the creation of a new server, following both approaches.","title":"Creating new servers"},{"location":"howto/openstack/nova/new-server/#prerequisites","text":"You need to have at least one network in the region you are interested in. Additionally, if you prefer to work with the OpenStack CLI, then make sure to properly enable it first .","title":"Prerequisites"},{"location":"howto/openstack/nova/new-server/#creating-a-server","text":"To create a server from the Cleura Cloud Management Panel, fire up your favorite web browser, navigate to the Cleura Cloud page, and log into your Cleura account. On the other hand, if you prefer to work with the OpenStack CLI, please do not forget to source the RC file first . Cleura Cloud Management Panel OpenStack CLI On the top right-hand side of the Cleura Cloud Management Panel, click the Create button. A new pane titled Create will slide into view from the right-hand side of the browser window. You will notice several rounded boxes on that pane, each for defining, configuring, and instantiating a different Cleura Cloud object. Go ahead and click the Server box. A new pane titled Create a Server will slide over. At the top, type in a name for the new server and select one of the available regions. In the Boot source section below, click the dropdown menu on the left and make sure you select Boot from image , so you can choose one of the readily available OS images to boot the new server off of. To pick one of the images, click on the dropdown menu on the right. For this how-to guide, we have chosen ubuntu in general and Ubuntu 22.04 Jammy Jellyfish 20220810 in particular. Next, make sure Boot Target is set to Volume (Recommended) . Regarding the server\u2019s CPU core count and amount of memory, set the Flavor accordingly. The default flavor specifies a server with 1 CPU core and 1GB of RAM. You can start with that or select a different configuration by clicking the dropdown menu at the right of Flavor . Please note that, depending on the chosen flavor, the estimated monthly cost of the server changes. (While a server is shut off you are still getting charged for it, but less so.) At any time, this estimated cost is displayed in the green rectangular area at the top. Something else that affects the cost is the size of the root device. Take a look at the Size parameter below, and notice the default (in gibibytes ). You may leave the root device size unchanged, or modify it to be lower or higher than the default. When, at a later time, you decide to delete the server, you can do so but keep its boot volume (you may want, for example, to attach the exact same volume to a new server). Just leave the Delete on termination option disabled if you want this kind of flexibility. On the other hand, if you want your root volume to be automatically deleted when the server terminates, enable Delete on termination . Use this option with caution. Finally, you may enable Disaster recovery for the server. If you do, then daily server snapshots will be created, and you will have the option for easy and fast rollups to previous snapshots. Please be aware that enabling this option increases the server\u2019s monthly estimated cost (again, it is displayed in the green rectangular area at the top). Regarding networking, select one of the available networks to attach the new server to. If you want the server accessible from the Internet, do not forget to enable the I want an external IP for my server option. In the section below, set Security Groups to default . If you already have one or more public keys in your Cleura Cloud account, you can now select a key to be included in the ~/.ssh/authorized_keys file of the server\u2019s default user. That way, you can securely log into the remote user\u2019s account without typing a password. If there are no public keys to choose from, activate the Password login enabled option and set a password for a specific user (with a username you define). A configuration script is automatically prepared based on the choices you have already made. That script runs during system boot and performs housekeeping tasks like user account creation, enabling acceptable authentication methods, and configuring remote package repositories. Click on Advanced Options to see the default script. It is now time to create your Cleura Cloud server; click the green Create button, and the new server will be readily available in a few seconds. An openstack command for creating a server may look like this: openstack server create \\ --flavor $FLAVOR_NAME \\ --image $IMAGE_NAME \\ --boot-from-volume $VOL_SIZE \\ --network $NETWORK_NAME \\ --security-group $SEC_GROUP_NAME \\ --key-name $KEY_NAME \\ --wait \\ $SERVER_NAME Each variable represents a piece of information we have to look for or, in the cases of KEY_NAME and SERVER_NAME , arbitrarily define. Let us begin with the flavors ( FLAVOR_NAME ), which describe combinations of CPU core count and memory size. Each server has a distinct flavor, and to see all available flavors type: openstack flavor list You will get a pretty long list of flavors. For our demonstration, we suggest you go with b.1c1gb . A server with this particular flavor will have one CPU core and one gibibyte of RAM. Go ahead and set FLAVOR_NAME accordingly: FLAVOR_NAME = \"b.1c1gb\" Your server should have an image to boot off of ( IMAGE_NAME ). For a list of all available images in Cleura Cloud, type: openstack image list This time you get a shorter list, but you can still filter for images with the OS you prefer. For example, filter for Ubuntu: openstack image list --tag \"os:ubuntu\" Continue with the Ubuntu 22.04 Jammy Jellyfish 20220810 image: IMAGE_NAME = \"Ubuntu 22.04 Jammy Jellyfish 20220810\" Before you go on, decide on the capacity (in gibibytes) of the server\u2019s boot volume ( VOL_SIZE ). We suggest you start with 20 gibibytes: VOL_SIZE = \"20\" You need at least one network in the region you\u2019re about to create your new server ( NETWORK_NAME ). To get the names of all available (internal) networks, type: openstack network list --internal -c Name +----------------+ | Name | +----------------+ | nordostbahnhof | +----------------+ Set the NETWORK_NAME variable accordingly: NETWORK_NAME = \"nordostbahnhof\" Regarding the security group ( SEC_GROUP_NAME ), unless you have already created one yourself, you will find only one per region: openstack security group list -c Name -c Description +---------+------------------------+ | Name | Description | +---------+------------------------+ | default | Default security group | +---------+------------------------+ Go ahead and set SEC_GROUP_NAME : SEC_GROUP_NAME = \"default\" You most likely want a server you can remotely connect to via SSH without typing a password. Upload one of our public keys to your Cleura Cloud account: openstack keypair create --public-key ~/.ssh/id_ed25519.pub bahnhof In the example above, we uploaded the public key ~/.ssh/id_ed25519.pub to our Cleura Cloud account and named it bahnhof . Follow our example and do not forget to set the KEY_NAME : KEY_NAME = \"bahnhof\" By the way, check all uploaded public keys\u2026 openstack keypair list \u2026and get more information regarding the one you just uploaded: openstack keypair show bahnhof You are almost ready to create your new server. Decide on a name\u2026 SERVER_NAME = \"zug\" # just an example \u2026and then go ahead and create it: openstack server create \\ --flavor b.1c1gb \\ --image $IMAGE_NAME \\ --boot-from-volume 20 \\ --network nordostbahnhof \\ --security-group default \\ --key-name bahnhof \\ --wait \\ zug (For clarity\u2019s sake, and with the exception of IMAGE_NAME , we used the actual values and not the variables we so meticulously set.) The --wait parameter is optional. Whenever you choose to use it, you get back control of your terminal only after the server is readily available in Cleura Cloud. To connect to your server remotely, you need to create a floating IP for the external network in the Cleura Cloud, and then assign this IP to your server. First, create the floating IP: openstack floating ip create ext-net See all floating IPs\u2026 openstack floating ip list \u2026and assign the one you just created to your server: openstack server add floating ip zug 198 .51.100.12 The username of the default user account in the Ubuntu image is ubuntu , so now you can connect to your remote server via SSH without typing a password: ssh ubuntu@198.51.100.12","title":"Creating a server"},{"location":"howto/openstack/nova/new-server/#viewing-information-about-the-newly-created-server","text":"Cleura Cloud Management Panel OpenStack CLI From the Cleura Cloud Management Panel you may, at any time, see all servers and get detailed information regarding each one of them; expand the left-hand side vertical pane, click Compute , then Servers , and, in the central pane, select the region you want. To see all available servers in the region, type: openstack server list You can always get specific information on a particular server: openstack server show zug","title":"Viewing information about the newly created server"},{"location":"howto/openstack/nova/new-server/#connecting-to-the-server-console","text":"Cleura Cloud Management Panel OpenStack CLI While viewing information regarding your server, you may get its public IP address (e.g., from the Addresses tab) and connect to it remotely. Alternatively, you may launch a web console and log in; click on the three-dot icon on the right of the server header, and from the pop-up menu that appears select Remote Console . A new window pops up, and that\u2019s your web console to your Cleura Cloud server. Please note that this window cannot be resized but can be opened on a new browser window or tab. You may have access to the web console of your server, and you need the corresponding URL for it: openstack console url show zug Usage of the web console is discouraged, though. Instead, securely connect to your server via SSH.","title":"Connecting to the server console"},{"location":"howto/openstack/octavia/tls-lb/","text":"HTTPS-terminating load balancers In Cleura\u2019s load balancing service, OpenStack Octavia , you can configure load balancers so that they manage HTTPS termination. That is to say that the load balancer encrypts and decrypts HTTPS traffic, and forwards HTTP to and from a backend web server. To do so, the load balancer must have access to encryption credentials (such as certificates and private keys), which it stores in Barbican. PKCS #12 Certificate Bundles The PKCS #12 archive format includes SSL certificates, certificate chains, and private keys all in one bundle. Most certificate providers give you the option of downloading certificate credentials using the PKCS #12 format. In case your certificate provider has made your certificate chain and key available separately, using the PEM format, you can easily convert it to PKCS #12 using the following openssl command: openssl pkcs12 -export -inkey key.pem -in fullchain.pem -out bundle.p12 When prompted for an export password, use a blank one. Creating Barbican secrets from PKCS #12 bundles To create a secret from a stored PKCS #12 bundle, you need pass in the contents of the bundle, pre-encoded with Base64 , as the secret\u2019s payload. openstack secret store \\ --name = 'tls_secret1' \\ -t 'application/octet-stream' \\ -e 'base64' \\ --payload = \" $( base64 < server.p12 ) \" +---------------+---------------------------------------------------------------------------------+ | Field | Value | +---------------+---------------------------------------------------------------------------------+ | Secret href | https://kna1.citycloud.com:9311/v1/secrets/69bd82f5-60c9-4764-99ec-7a3dff05d2aa | | Name | tls_secret1 | | Created | None | | Status | None | | Content types | {'default': 'application/octet-stream'} | | Algorithm | aes | | Bit length | 256 | | Secret type | opaque | | Mode | cbc | | Expiration | None | +---------------+---------------------------------------------------------------------------------+ Creating HTTPS-enabled load balancer listeners Once you have created your secret containing your certificate data, you can create a load balancer listener with the following properties: It uses the TERMINATED_HTTPS protocol, It sets its \u201cdefault TLS container\u201d to the Barbican secret containing the PKCS #12 bundle, It listens on the standard HTTPS port, 443. You create such a listener with the following command: openstack loadbalancer listener create \\ --protocol-port 443 \\ --protocol TERMINATED_HTTPS \\ --name listener1 \\ --default-tls-container-ref = https://kna1.citycloud.com:9311/v1/secrets/dacfbec1-fbed-403f-a4dc-303e28942dae \\ <loadbalancer-name-or-id> +-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | admin_state_up | True | | connection_limit | -1 | | created_at | 2021-01-20T11:51:46 | | default_pool_id | None | | default_tls_container_ref | https://kna1.citycloud.com:9311/v1/secrets/dacfbec1-fbed-403f-a4dc-303e28942dae | | description | | | id | 4ec6b23d-d08a-4de0-9e12-54ac690ee1ec | | insert_headers | None | | l7policies | | | loadbalancers | 2c2a0760-c3a8-48d2-bdd0-288c3d33a43f | | name | listener1 | | operating_status | OFFLINE | | project_id | 4a9484063d4c40d29301ad745c0e2c69 | | protocol | TERMINATED_HTTPS | | protocol_port | 443 | | provisioning_status | PENDING_CREATE | | sni_container_refs | [] | | timeout_client_data | 50000 | | timeout_member_connect | 5000 | | timeout_member_data | 50000 | | timeout_tcp_inspect | 0 | | updated_at | None | | client_ca_tls_container_ref | None | | client_authentication | NONE | | client_crl_container_ref | None | | allowed_cidrs | None | | tls_ciphers | TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:TLS_AES_128_GCM_SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256 | | tls_versions | | +-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Updating the TLS certificate for a HTTPS listener When the certificate associated with a TERMINATED_HTTPS listener is about to expire, you will need to replace it. You can do this online, with no user-noticeable interruption to your service. Create a new PKCS#12 bundle from the updated key, certificate, and CA certificate. Create a new Barbican secret from the bundle. List the listener(s) associated with your load balancer: openstack loadbalancer listener list \\ --loadbalancer <loadbalancer-name-or-id> For all listeners using the TERMINATED_HTTPS protocol, run the following command: openstack loadbalancer listener set \\ --default-tls-container-ref = https://kna1.citycloud.com:9311/v1/secrets/e2d8acc1-c6b9-4c01-9373-cc167b075c25 \\ <listener-name-or-id> Once all your load balancer listeners have completed the update, you may proceed to delete the old, now-unused secret: openstack secret delete \\ https://kna1.citycloud.com:9311/v1/secrets/dacfbec1-fbed-403f-a4dc-303e28942dae","title":"HTTPS-terminating load balancers"},{"location":"howto/openstack/octavia/tls-lb/#https-terminating-load-balancers","text":"In Cleura\u2019s load balancing service, OpenStack Octavia , you can configure load balancers so that they manage HTTPS termination. That is to say that the load balancer encrypts and decrypts HTTPS traffic, and forwards HTTP to and from a backend web server. To do so, the load balancer must have access to encryption credentials (such as certificates and private keys), which it stores in Barbican.","title":"HTTPS-terminating load balancers"},{"location":"howto/openstack/octavia/tls-lb/#pkcs-12-certificate-bundles","text":"The PKCS #12 archive format includes SSL certificates, certificate chains, and private keys all in one bundle. Most certificate providers give you the option of downloading certificate credentials using the PKCS #12 format. In case your certificate provider has made your certificate chain and key available separately, using the PEM format, you can easily convert it to PKCS #12 using the following openssl command: openssl pkcs12 -export -inkey key.pem -in fullchain.pem -out bundle.p12 When prompted for an export password, use a blank one.","title":"PKCS #12 Certificate Bundles"},{"location":"howto/openstack/octavia/tls-lb/#creating-barbican-secrets-from-pkcs-12-bundles","text":"To create a secret from a stored PKCS #12 bundle, you need pass in the contents of the bundle, pre-encoded with Base64 , as the secret\u2019s payload. openstack secret store \\ --name = 'tls_secret1' \\ -t 'application/octet-stream' \\ -e 'base64' \\ --payload = \" $( base64 < server.p12 ) \" +---------------+---------------------------------------------------------------------------------+ | Field | Value | +---------------+---------------------------------------------------------------------------------+ | Secret href | https://kna1.citycloud.com:9311/v1/secrets/69bd82f5-60c9-4764-99ec-7a3dff05d2aa | | Name | tls_secret1 | | Created | None | | Status | None | | Content types | {'default': 'application/octet-stream'} | | Algorithm | aes | | Bit length | 256 | | Secret type | opaque | | Mode | cbc | | Expiration | None | +---------------+---------------------------------------------------------------------------------+","title":"Creating Barbican secrets from PKCS #12 bundles"},{"location":"howto/openstack/octavia/tls-lb/#creating-https-enabled-load-balancer-listeners","text":"Once you have created your secret containing your certificate data, you can create a load balancer listener with the following properties: It uses the TERMINATED_HTTPS protocol, It sets its \u201cdefault TLS container\u201d to the Barbican secret containing the PKCS #12 bundle, It listens on the standard HTTPS port, 443. You create such a listener with the following command: openstack loadbalancer listener create \\ --protocol-port 443 \\ --protocol TERMINATED_HTTPS \\ --name listener1 \\ --default-tls-container-ref = https://kna1.citycloud.com:9311/v1/secrets/dacfbec1-fbed-403f-a4dc-303e28942dae \\ <loadbalancer-name-or-id> +-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | admin_state_up | True | | connection_limit | -1 | | created_at | 2021-01-20T11:51:46 | | default_pool_id | None | | default_tls_container_ref | https://kna1.citycloud.com:9311/v1/secrets/dacfbec1-fbed-403f-a4dc-303e28942dae | | description | | | id | 4ec6b23d-d08a-4de0-9e12-54ac690ee1ec | | insert_headers | None | | l7policies | | | loadbalancers | 2c2a0760-c3a8-48d2-bdd0-288c3d33a43f | | name | listener1 | | operating_status | OFFLINE | | project_id | 4a9484063d4c40d29301ad745c0e2c69 | | protocol | TERMINATED_HTTPS | | protocol_port | 443 | | provisioning_status | PENDING_CREATE | | sni_container_refs | [] | | timeout_client_data | 50000 | | timeout_member_connect | 5000 | | timeout_member_data | 50000 | | timeout_tcp_inspect | 0 | | updated_at | None | | client_ca_tls_container_ref | None | | client_authentication | NONE | | client_crl_container_ref | None | | allowed_cidrs | None | | tls_ciphers | TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:TLS_AES_128_GCM_SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256 | | tls_versions | | +-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"Creating HTTPS-enabled load balancer listeners"},{"location":"howto/openstack/octavia/tls-lb/#updating-the-tls-certificate-for-a-https-listener","text":"When the certificate associated with a TERMINATED_HTTPS listener is about to expire, you will need to replace it. You can do this online, with no user-noticeable interruption to your service. Create a new PKCS#12 bundle from the updated key, certificate, and CA certificate. Create a new Barbican secret from the bundle. List the listener(s) associated with your load balancer: openstack loadbalancer listener list \\ --loadbalancer <loadbalancer-name-or-id> For all listeners using the TERMINATED_HTTPS protocol, run the following command: openstack loadbalancer listener set \\ --default-tls-container-ref = https://kna1.citycloud.com:9311/v1/secrets/e2d8acc1-c6b9-4c01-9373-cc167b075c25 \\ <listener-name-or-id> Once all your load balancer listeners have completed the update, you may proceed to delete the old, now-unused secret: openstack secret delete \\ https://kna1.citycloud.com:9311/v1/secrets/dacfbec1-fbed-403f-a4dc-303e28942dae","title":"Updating the TLS certificate for a HTTPS listener"},{"location":"reference/","text":"Reference This is our reference section. It serves to provide general reference information about Cleura services. Feature and service availability Our Feature Support Matrix lists cloud features and their availability across Cleura regions. The Flavors reference explains our naming convention for pre-defined CPU/RAM/disk configurations (\u201cflavors\u201d) for server instances, and their availability across Cleura regions. Our API Service Version Matrix lists the open source software versions running in our Cleura regions.","title":"Reference"},{"location":"reference/#reference","text":"This is our reference section. It serves to provide general reference information about Cleura services.","title":"Reference"},{"location":"reference/#feature-and-service-availability","text":"Our Feature Support Matrix lists cloud features and their availability across Cleura regions. The Flavors reference explains our naming convention for pre-defined CPU/RAM/disk configurations (\u201cflavors\u201d) for server instances, and their availability across Cleura regions. Our API Service Version Matrix lists the open source software versions running in our Cleura regions.","title":"Feature and service availability"},{"location":"reference/features/","text":"Feature support matrix Services in Cleura Cloud constantly evolve, and we gradually add features to regions as they become available and mature. This page lists the cloud features available in each Cleura Cloud region. Public Cloud : features supported in our Cleura Public Cloud regions. Compliant Cloud : features supported in our Cleura Compliant Cloud regions.","title":"Feature support matrix"},{"location":"reference/features/#feature-support-matrix","text":"Services in Cleura Cloud constantly evolve, and we gradually add features to regions as they become available and mature. This page lists the cloud features available in each Cleura Cloud region. Public Cloud : features supported in our Cleura Public Cloud regions. Compliant Cloud : features supported in our Cleura Compliant Cloud regions.","title":"Feature support matrix"},{"location":"reference/features/compliant/","text":"Compliant Cloud Feature is in production and fully supported Feature is in deployment, not yet supported Feature is deprecated, being phased out Feature is not available Virtualization Sto1HS Sto2HS Dedicated CPU Virtual GPU Block storage Sto1HS Sto2HS Highly available storage High-performance local storage Volume encryption Object storage Sto1HS Sto2HS S3 API S3 SSE-C Swift API Networking (Layer 2/3) Sto1HS Sto2HS IPv4 (with NAT) IPv6 VPN (IPsec with PSK) Load Balancers Sto1HS Sto2HS Transport layer (TCP/UDP) Application layer (HTTP) Application layer ( HTTPS, with secrets management for TLS certificates )","title":"Compliant Cloud"},{"location":"reference/features/compliant/#compliant-cloud","text":"Feature is in production and fully supported Feature is in deployment, not yet supported Feature is deprecated, being phased out Feature is not available","title":"Compliant Cloud"},{"location":"reference/features/compliant/#virtualization","text":"Sto1HS Sto2HS Dedicated CPU Virtual GPU","title":"Virtualization"},{"location":"reference/features/compliant/#block-storage","text":"Sto1HS Sto2HS Highly available storage High-performance local storage Volume encryption","title":"Block storage"},{"location":"reference/features/compliant/#object-storage","text":"Sto1HS Sto2HS S3 API S3 SSE-C Swift API","title":"Object storage"},{"location":"reference/features/compliant/#networking-layer-23","text":"Sto1HS Sto2HS IPv4 (with NAT) IPv6 VPN (IPsec with PSK)","title":"Networking (Layer 2/3)"},{"location":"reference/features/compliant/#load-balancers","text":"Sto1HS Sto2HS Transport layer (TCP/UDP) Application layer (HTTP) Application layer ( HTTPS, with secrets management for TLS certificates )","title":"Load Balancers"},{"location":"reference/features/public/","text":"Public Cloud Feature is in production and fully supported Feature is in deployment, not yet supported Feature is deprecated, being phased out Feature is not available Virtualization Kna1 Sto2 Fra1 Dx1 Tky1 Dedicated CPU Virtual GPU Block storage Kna1 Sto2 Fra1 Dx1 Tky1 Highly available storage High-performance local storage Volume encryption Object storage Kna1 Sto2 Fra1 Dx1 Tky1 S3 API S3 SSE-C Swift API Networking (Layer 2/3) Kna1 Sto2 Fra1 Dx1 Tky1 IPv4 (with NAT) IPv6 VPN (IPsec with PSK) Load Balancers Kna1 Sto2 Fra1 Dx1 Tky1 Transport layer (TCP/UDP) Application layer (HTTP) Application layer ( HTTPS, with secrets management for TLS certificates ) Kubernetes management Kna1 Sto2 Fra1 Dx1 Tky1 OpenStack Magnum Gardener","title":"Public Cloud"},{"location":"reference/features/public/#public-cloud","text":"Feature is in production and fully supported Feature is in deployment, not yet supported Feature is deprecated, being phased out Feature is not available","title":"Public Cloud"},{"location":"reference/features/public/#virtualization","text":"Kna1 Sto2 Fra1 Dx1 Tky1 Dedicated CPU Virtual GPU","title":"Virtualization"},{"location":"reference/features/public/#block-storage","text":"Kna1 Sto2 Fra1 Dx1 Tky1 Highly available storage High-performance local storage Volume encryption","title":"Block storage"},{"location":"reference/features/public/#object-storage","text":"Kna1 Sto2 Fra1 Dx1 Tky1 S3 API S3 SSE-C Swift API","title":"Object storage"},{"location":"reference/features/public/#networking-layer-23","text":"Kna1 Sto2 Fra1 Dx1 Tky1 IPv4 (with NAT) IPv6 VPN (IPsec with PSK)","title":"Networking (Layer 2/3)"},{"location":"reference/features/public/#load-balancers","text":"Kna1 Sto2 Fra1 Dx1 Tky1 Transport layer (TCP/UDP) Application layer (HTTP) Application layer ( HTTPS, with secrets management for TLS certificates )","title":"Load Balancers"},{"location":"reference/features/public/#kubernetes-management","text":"Kna1 Sto2 Fra1 Dx1 Tky1 OpenStack Magnum Gardener","title":"Kubernetes management"},{"location":"reference/flavors/","text":"Flavors Any server instance running in Cleura Cloud has a flavor , which defines the number of virtual CPU cores, the amount of virtual RAM, and other performance-related factors. Naming convention Flavor names in Cleura follow a convention, which can be summarized as X.YcZgb : X stands for a lowercase letter identifying the compute tier , with b representing the general-purpose tier. It is always followed by a full-stop ( . ). Y stands for the number of virtual CPU cores. This number is always followed by the letter c . Z stands for the allocated amount of virtual RAM, in gibibytes . This number is always followed by the string gb . For example, the flavor named b.4c32gb would be used for a general-purpose compute instance with 4 cores and 32 GiB RAM. Compute tiers Cleura Cloud defines the following compute tiers: b : General purpose. This is the default compute tier. Instances launched with matching flavors use highly available network-attached storage. This makes them flexible to migrate within the Cleura Cloud infrastructure, without interruption. Some limitations apply to instances with attached encrypted volumes . s : High-performance local storage. Instances launched with matching flavors use local, directly-attached storage. This generally provides higher throughput and lower latency for I/O intensive applications, but instances launched with these flavors must configure their own high availability and data replication. c : Dedicated CPU. Instances launched with matching flavors are guaranteed to run on compute hardware where CPU cores are allocated to instances on a one-to-one basis and one virtual core maps directly to a physical CPU core. g : Virtual GPU. Instances launched with matching flavors have access to a GPU . Some tiers are only available in select Cleura Cloud regions. For details on tier availability, see the Feature support matrix . The general-purpose tier is always available to all Cleura Cloud customers. For access to other tiers, contact our Service Center .","title":"Flavors"},{"location":"reference/flavors/#flavors","text":"Any server instance running in Cleura Cloud has a flavor , which defines the number of virtual CPU cores, the amount of virtual RAM, and other performance-related factors.","title":"Flavors"},{"location":"reference/flavors/#naming-convention","text":"Flavor names in Cleura follow a convention, which can be summarized as X.YcZgb : X stands for a lowercase letter identifying the compute tier , with b representing the general-purpose tier. It is always followed by a full-stop ( . ). Y stands for the number of virtual CPU cores. This number is always followed by the letter c . Z stands for the allocated amount of virtual RAM, in gibibytes . This number is always followed by the string gb . For example, the flavor named b.4c32gb would be used for a general-purpose compute instance with 4 cores and 32 GiB RAM.","title":"Naming convention"},{"location":"reference/flavors/#compute-tiers","text":"Cleura Cloud defines the following compute tiers: b : General purpose. This is the default compute tier. Instances launched with matching flavors use highly available network-attached storage. This makes them flexible to migrate within the Cleura Cloud infrastructure, without interruption. Some limitations apply to instances with attached encrypted volumes . s : High-performance local storage. Instances launched with matching flavors use local, directly-attached storage. This generally provides higher throughput and lower latency for I/O intensive applications, but instances launched with these flavors must configure their own high availability and data replication. c : Dedicated CPU. Instances launched with matching flavors are guaranteed to run on compute hardware where CPU cores are allocated to instances on a one-to-one basis and one virtual core maps directly to a physical CPU core. g : Virtual GPU. Instances launched with matching flavors have access to a GPU . Some tiers are only available in select Cleura Cloud regions. For details on tier availability, see the Feature support matrix . The general-purpose tier is always available to all Cleura Cloud customers. For access to other tiers, contact our Service Center .","title":"Compute tiers"},{"location":"reference/versions/","text":"Service version matrix Services in Cleura Cloud are updated on a regular basis and on a rolling schedule. This section lists the cloud API service versions available in each Cleura Cloud region. Public Cloud : versions running in our Cleura Public Cloud regions. Compliant Cloud : versions running in our Cleura Compliant Cloud regions. OpenStack Services OpenStack releases are named, in alphabetical order, and occur on a six-month release schedule. In Cleura Public Cloud we upgrade OpenStack releases annually; this means that we deploy every other OpenStack release and skip the intervening one. Cleura Cloud currently runs OpenStack Xena . Ceph Services Ceph major releases are also named, in alphabetical order, and occur on a roughly annual schedule. Cleura Cloud currently runs Ceph Pacific .","title":"Service version matrix"},{"location":"reference/versions/#service-version-matrix","text":"Services in Cleura Cloud are updated on a regular basis and on a rolling schedule. This section lists the cloud API service versions available in each Cleura Cloud region. Public Cloud : versions running in our Cleura Public Cloud regions. Compliant Cloud : versions running in our Cleura Compliant Cloud regions.","title":"Service version matrix"},{"location":"reference/versions/#openstack-services","text":"OpenStack releases are named, in alphabetical order, and occur on a six-month release schedule. In Cleura Public Cloud we upgrade OpenStack releases annually; this means that we deploy every other OpenStack release and skip the intervening one. Cleura Cloud currently runs OpenStack Xena .","title":"OpenStack Services"},{"location":"reference/versions/#ceph-services","text":"Ceph major releases are also named, in alphabetical order, and occur on a roughly annual schedule. Cleura Cloud currently runs Ceph Pacific .","title":"Ceph Services"},{"location":"reference/versions/compliant/","text":"Compliant Cloud OpenStack Services Sto1HS Sto2HS Barbican (secret storage) Xena Xena Cinder (block storage) Xena Xena Glance (image management) Xena Xena Heat (orchestration) Xena Xena Keystone (identity management) Xena Xena Magnum (container management) Xena Xena Neutron (networking) Xena Xena Nova (server virtualization) Xena Xena Octavia (load balancing) Xena Xena Ceph Services Sto1HS Sto2HS Block storage (for OpenStack) Pacific Pacific Object storage (Swift API) Pacific Pacific Object storage (S3 API) Pacific Pacific","title":"Compliant Cloud"},{"location":"reference/versions/compliant/#compliant-cloud","text":"","title":"Compliant Cloud"},{"location":"reference/versions/compliant/#openstack-services","text":"Sto1HS Sto2HS Barbican (secret storage) Xena Xena Cinder (block storage) Xena Xena Glance (image management) Xena Xena Heat (orchestration) Xena Xena Keystone (identity management) Xena Xena Magnum (container management) Xena Xena Neutron (networking) Xena Xena Nova (server virtualization) Xena Xena Octavia (load balancing) Xena Xena","title":"OpenStack Services"},{"location":"reference/versions/compliant/#ceph-services","text":"Sto1HS Sto2HS Block storage (for OpenStack) Pacific Pacific Object storage (Swift API) Pacific Pacific Object storage (S3 API) Pacific Pacific","title":"Ceph Services"},{"location":"reference/versions/public/","text":"Public Cloud OpenStack Services Kna1 Sto2 Fra1 Dx1 Tky1 Barbican (secret storage) Xena Xena Xena Xena Xena Cinder (block storage) Xena Xena Xena Xena Xena Glance (image management) Xena Xena Xena Xena Xena Heat (orchestration) Xena Xena Xena Xena Xena Keystone (identity management) Xena Xena Xena Xena Xena Magnum (container management) Xena Xena Xena Xena Xena Neutron (networking) Xena Xena Xena Xena Xena Nova (server virtualization) Xena Xena Xena Xena Xena Octavia (load balancing) Xena Xena Xena Xena Xena Ceph Services Kna1 Sto2 Fra1 Dx1 Tky1 Block storage (for OpenStack) Pacific Pacific Pacific Pacific Pacific Object storage (Swift API) Pacific Pacific Pacific Object storage (S3 API) Pacific Pacific Pacific","title":"Public Cloud"},{"location":"reference/versions/public/#public-cloud","text":"","title":"Public Cloud"},{"location":"reference/versions/public/#openstack-services","text":"Kna1 Sto2 Fra1 Dx1 Tky1 Barbican (secret storage) Xena Xena Xena Xena Xena Cinder (block storage) Xena Xena Xena Xena Xena Glance (image management) Xena Xena Xena Xena Xena Heat (orchestration) Xena Xena Xena Xena Xena Keystone (identity management) Xena Xena Xena Xena Xena Magnum (container management) Xena Xena Xena Xena Xena Neutron (networking) Xena Xena Xena Xena Xena Nova (server virtualization) Xena Xena Xena Xena Xena Octavia (load balancing) Xena Xena Xena Xena Xena","title":"OpenStack Services"},{"location":"reference/versions/public/#ceph-services","text":"Kna1 Sto2 Fra1 Dx1 Tky1 Block storage (for OpenStack) Pacific Pacific Pacific Pacific Pacific Object storage (Swift API) Pacific Pacific Pacific Object storage (S3 API) Pacific Pacific Pacific","title":"Ceph Services"},{"location":"tutorials/","text":"About our tutorials Our tutorials are gentle introductions to Cleura. They generally require very little prior knowledge and assume only that you have access to a browser, or to a terminal. We group these into two major categories: Browser-based tutorials get you started on Cleura using the Cleura Cloud Management Panel . Terminal-based tutorials introduce you to command-line interfaces (CLIs) and their interaction with Cleura\u2019s web-based Application Programming Interfaces (APIs). This section does not include detailed walkthroughs of specific technical tasks. For those, please see our How-To Guides section. If you find our tutorials helpful, you might also be interested in our self-paced online training courses, available from our course booking site .","title":"About our tutorials"},{"location":"tutorials/#about-our-tutorials","text":"Our tutorials are gentle introductions to Cleura. They generally require very little prior knowledge and assume only that you have access to a browser, or to a terminal. We group these into two major categories: Browser-based tutorials get you started on Cleura using the Cleura Cloud Management Panel . Terminal-based tutorials introduce you to command-line interfaces (CLIs) and their interaction with Cleura\u2019s web-based Application Programming Interfaces (APIs). This section does not include detailed walkthroughs of specific technical tasks. For those, please see our How-To Guides section. If you find our tutorials helpful, you might also be interested in our self-paced online training courses, available from our course booking site .","title":"About our tutorials"}]}